{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":192929082,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:33:45.015060Z","iopub.execute_input":"2024-09-20T03:33:45.015866Z","iopub.status.idle":"2024-09-20T03:33:46.116415Z","shell.execute_reply.started":"2024-09-20T03:33:45.015828Z","shell.execute_reply":"2024-09-20T03:33:46.115273Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Fri Sep 20 03:33:45 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**En este notebook y tutorial, realizaremos un fine-tune [Llama-8k](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) modelo relativamente pequeño de 7 mil millones de parametros - que ha 'demostrado un rendimiento casi de última generación entre los modelos con menos de 13 mil millones de parámetros' - *en tus propios datos!!***\n\n**Aqui usaremos [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), una técnica de fine-tunning altamente eficiente que consiste en cuantizar un LLM preentrenado a solo 4 bits y agregar pequeños 'Adaptadores de Bajo Rango'. Este enfoque único permite realizar el fine-tunning de LLMs utilizando solo una GPU. Esta técnica está respaldada por el/la... [PEFT library](https://huggingface.co/docs/peft/index).**","metadata":{}},{"cell_type":"markdown","source":"# Tabla de Contenido","metadata":{}},{"cell_type":"markdown","source":"- [1- Instalar librerias requeridas](#1)\n- [ 2 - Cargar dataset](#2)\n- [ 3 - Crear configuración de bitsandbytes](#3)\n- [ 4 - Cargar Modelo Base](#4)\n- [ 5 - Tokenizar](#5)\n- [ 6 - Testear el modelo con Zero Shot Inferencing](#6)\n- [ 7 - Pre-procesando el dataset](#7)\n- [ 8 - Configurar el modelo PEFT/LoRA para realizar Fine-Tuning](#8)\n- [ 9 - Entrenar Adaptador PEFT](#9)\n- [ 10 - Evaluar el Modelo Qualitativamente (Evaluacion Humana)](#10)\n- [ 11 - Evaluar el Modelo Quantitaviamente (con Metrica ROUGE)](#11)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n#### 1. Instalar librerias requeridas","metadata":{}},{"cell_type":"code","source":"%%time\n!pip install -U transformers\n!pip install -U bitsandbytes\n!pip install -U peft\n!pip install -U accelerate\n!pip install -U datasets\n!pip install -U scipy\n!pip install -U einops\n!pip install -U evaluate\n#!pip install -U trl\n!pip install -U rouge_score\n!pip install -U torch","metadata":{"_uuid":"c34dd264-0a8e-4fae-8f88-83c906c39c3c","_cell_guid":"59a8cbf9-4057-4756-a512-100f15222529","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-20T03:33:50.303725Z","iopub.execute_input":"2024-09-20T03:33:50.304105Z","iopub.status.idle":"2024-09-20T03:38:33.254318Z","shell.execute_reply.started":"2024-09-20T03:33:50.304071Z","shell.execute_reply":"2024-09-20T03:38:33.253224Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed transformers-4.44.2\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\nSuccessfully installed accelerate-0.34.2\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting datasets\n  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\nSuccessfully installed datasets-3.0.0\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nCollecting scipy\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\nDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.4\n    Uninstalling scipy-1.11.4:\n      Successfully uninstalled scipy-1.11.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.14.0\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1a3322597523c8f360d0bc6c7ce7f6b01bb7ff542968c5d96d3fa86effb4a2ef\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting torch\n  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\nCPU times: user 3.72 s, sys: 857 ms, total: 4.58 s\nWall time: 4min 42s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport os\nimport gc\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport multiprocessing\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom huggingface_hub import interpreter_login\nfrom pynvml import *\nfrom functools import partial\nfrom transformers import set_seed\nfrom datasets import load_dataset, DatasetDict\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n\n#interpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:39:05.493320Z","iopub.execute_input":"2024-09-20T03:39:05.494198Z","iopub.status.idle":"2024-09-20T03:39:24.483077Z","shell.execute_reply.started":"2024-09-20T03:39:05.494161Z","shell.execute_reply":"2024-09-20T03:39:24.482206Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-09-20 03:39:12.198581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-20 03:39:12.198706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-20 03:39:12.349043: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 10.3 s, sys: 1.41 s, total: 11.7 s\nWall time: 19 s\n","output_type":"stream"}]},{"cell_type":"code","source":"DATASET_FOLDER = os.path.join(\"/kaggle/input\", \"drugs-load-dataset\")\nDATASET_PATH = os.path.join(DATASET_FOLDER, \"dataset/drugs_data.parquet\")\n\nif not (os.path.exists(DATASET_PATH)):\n    print('Dataset no existe!!')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:39:30.972888Z","iopub.execute_input":"2024-09-20T03:39:30.973839Z","iopub.status.idle":"2024-09-20T03:39:30.995675Z","shell.execute_reply.started":"2024-09-20T03:39:30.973804Z","shell.execute_reply":"2024-09-20T03:39:30.994989Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n#### 2. Cargar el dataset","metadata":{}},{"cell_type":"code","source":"%%time\n#Cargar tu dataset\ndataset = load_dataset('parquet', data_files=DATASET_PATH)\n\n# Dividir en 70% train y 30% (test + validation)\ntrain_test_valid = dataset['train'].train_test_split(test_size=0.3, seed=42)\n\n# Dividir el 30% restante en 15% test y 15% validation\ntest_valid = train_test_valid['test'].train_test_split(test_size=0.5, seed=42)\n\n# Reunir los conjuntos en un DatasetDict\ndataset = DatasetDict({\n    'train': train_test_valid['train'],\n    'test': test_valid['test'],\n    'validation': test_valid['train']\n})\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:39:35.000020Z","iopub.execute_input":"2024-09-20T03:39:35.000465Z","iopub.status.idle":"2024-09-20T03:39:52.243672Z","shell.execute_reply.started":"2024-09-20T03:39:35.000430Z","shell.execute_reply":"2024-09-20T03:39:52.242644Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bf7edf2d2a44a9b5afd6462eabda94"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 15 s, sys: 5.05 s, total: 20.1 s\nWall time: 17.2 s\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 160299\n    })\n    test: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 34350\n    })\n    validation: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 34350\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Funcion para imprimir la utilización de la memoria de la GPU\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\n# Función para reemplazar NaN con cadena vacía\ndef replace_nan_with_empty_string(example):\n    for key, value in example.items():\n        if value is None or pd.isna(value) or (value == 'nan'):\n            example[key] = ''\n    return example\n\n\ndef create_prompt_formats(sample):\n    '''\n    \n    '''\n    #===========================================================================================\n    try:\n        # Construir las partes iniciales\n        instruct_key = '### Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\"'\n        context_key = '### Context: You are a pharmaceutical chemist specialized in the in-depth understanding of drug descriptions. Your task is to generate a professional and accurate response based on the information provided. If a specific field lacks information, state \"No specific information available\" instead of providing unconfirmed details.'\n        input_key = f\"### Input: Provide a detailed description of the medication {sample.get('generic_name', '')} using the available data.\"\n        end_key = \"### End\"\n\n        # Lista de campos a procesar\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                drugs.append(f'<{field}> {label_name}: {field_value} </{field}>')\n                if question_text:\n                    questions.append(f'<question> {question_text}</question><answer> {field_value}</answer>')\n\n        # Construir las partes finales\n        output_key = f\"### Output: {sample.get('description', '')}\"\n        if drugs:\n            output_key += \"\\n\" + \"\\n\".join(drugs)\n\n        question_key = '### Questions: ' + (\"\\n\".join(questions) if questions else \"\")\n\n        # Construir el texto final\n        parts = [instruct_key, context_key, input_key, output_key, question_key, end_key]\n        sample[\"text\"] = \"\\n\\n\".join(parts)\n\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    return sample\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )\n\n    \ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        _preprocessing_function = partial(preprocess_batch,\n                                          max_length = max_length,\n                                          tokenizer = tokenizer\n                                         )\n\n        dataset = dataset.map(_preprocessing_function, \n                              remove_columns=[col for col in dataset.column_names if col != \"text\"],\n                              #num_proc=num_proc\n                             )\n\n        # Filtrar las muestras que tienen input_ids que exceden la longitud máxima (max_length).\n        dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef print_number_of_trainable_model_parameters(model):\n    try:\n        trainable_model_params = 0\n        all_model_params = 0\n        for _, param in model.named_parameters():\n            all_model_params += param.numel()\n            if param.requires_grad:\n                trainable_model_params += param.numel()\n        return f\"all model parameters: {all_model_params}\\ntrainable model parameters: {trainable_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n    except Exception as ex:\n        print(f'Ocurrió un error inesperado al imprimir los parametros del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:39:56.420887Z","iopub.execute_input":"2024-09-20T03:39:56.421225Z","iopub.status.idle":"2024-09-20T03:39:56.442475Z","shell.execute_reply.started":"2024-09-20T03:39:56.421186Z","shell.execute_reply":"2024-09-20T03:39:56.441623Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ModelAnalizer:\n    '''\n    '''\n    \n    def __init__(self, model_name_or_path):\n        self.model_name_or_path = model_name_or_path\n        self.model = None\n        self.tokenizer = None\n        self._load_qtz_config()\n    \n    \n    def _load_qtz_config(self):\n        try:\n            compute_dtype = getattr(torch, \"float16\")\n            self.bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                                 bnb_4bit_quant_type='nf4',\n                                                 bnb_4bit_compute_dtype=compute_dtype,\n                                                 bnb_4bit_use_double_quant=True,\n                                                )\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar quantization-config [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n        \n    def _load_model(self):\n        try:\n            device_map = {\"\": 0}\n            self.model = AutoModelForCausalLM.from_pretrained(self.model_name_or_path, \n                                                              device_map=device_map,\n                                                              quantization_config=self.bnb_config,\n                                                              trust_remote_code=True,\n                                                              token=\"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\"\n                                                              )\n            # Carga el tokenizador\n            self._tokenizer()\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n            \n    \n    def _tokenizer(self):\n        # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n        try:\n            print(f'self.model_name_or_path : {self.model_name_or_path}')\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, \n                                                          trust_remote_code=True, \n                                                          add_bos_token=True,\n                                                          use_fast=False, \n                                                          add_eos_token=True, \n                                                          padding_side=\"left\",\n                                                          token=\"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\"\n                                                         )\n            if not(self.tokenizer):\n                raise Exception(f'No se ha definido el atributo self.tokenizer')\n            \n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el tokenizador [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    \n    \n    def gen(self, prompt, maxlen=512, sample=True):\n        try:\n            '''\n            eval_tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path,\n                                                           trust_remote_code=True,\n                                                           add_bos_token=True,\n                                                           use_fast=False\n                                                          )\n            eval_tokenizer.pad_token = eval_tokenizer.eos_token\n            \n            toks = eval_tokenizer(p, return_tensors=\"pt\")\n            '''\n            \n            toks = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n            res = self.model.generate(**toks.to(\"cuda\"), \n                                      max_new_tokens=maxlen,\n                                      do_sample=sample,\n                                      num_return_sequences=1,\n                                      temperature=0.7,\n                                      num_beams=1,\n                                      top_p=0.95\n                                     ).to('cpu')\n            return self.tokenizer.batch_decode(res, skip_special_tokens=True)\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al procesar la inferencia en el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n    \n    def get_max_length(self):\n        try:\n            max_length = None\n            for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n                max_length = getattr(self.model.config, length_setting, None)\n                if max_length:\n                    print(f\"Found max length: {max_length}\")\n                    break\n            if not max_length:\n                max_length = 1024\n                print(f\"Using default max length: {max_length}\")\n            return max_length\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al obtener tamaño del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:40:00.755604Z","iopub.execute_input":"2024-09-20T03:40:00.756506Z","iopub.status.idle":"2024-09-20T03:40:00.773499Z","shell.execute_reply.started":"2024-09-20T03:40:00.756473Z","shell.execute_reply":"2024-09-20T03:40:00.772460Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dataset['train'][120]","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:40:05.322788Z","iopub.execute_input":"2024-09-20T03:40:05.323152Z","iopub.status.idle":"2024-09-20T03:40:05.337628Z","shell.execute_reply.started":"2024-09-20T03:40:05.323122Z","shell.execute_reply":"2024-09-20T03:40:05.336713Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'abuse': '',\n 'abuse_table': '',\n 'active_ingredient': 'Active ingredients Aspirin 500 mg (NSAID*) Caffeine 60 mg *Nonsteroidal anti-inflammatory drug',\n 'active_ingredient_table': '',\n 'adverse_reactions': '',\n 'adverse_reactions_table': '',\n 'alarms': '',\n 'ask_doctor_or_pharmacist': 'Ask a doctor or phamacist before use if you are taking a prescription drug for diabetes gout arthritis',\n 'ask_doctor_or_pharmacist_table': '',\n 'ask_doctor_table': '',\n 'brand_name': 'Blowfish',\n 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility': '',\n 'clinical_pharmacology_table': '',\n 'clinical_studies_table': '',\n 'components_table': '',\n 'contraindications': '',\n 'controlled_substance': '',\n 'dependence': '',\n 'dependence_table': '',\n 'description': '',\n 'description_table': '',\n 'do_not_use_table': '',\n 'dosage_and_administration': 'Directions upon waking, fully dissolve 2 tablets in 16 oz. of water and drink do not exceed recommended dosage Adults and children 12 years and over (up to 60 years of age) 2 tablets every 6 hours, as needed, or as directed by a doctor. Do not exceed 8 tablets in 24 hours Adults 60 years and over 2 tablets every 6 hours, as needed, or as directed by a doctor. Do not exceed 4 tablets in 24 hours Children under 12 years Do not use',\n 'dosage_and_administration_table': 'Adults and children 12 years and over (up to 60 years of age) 2 tablets every 6 hours, as needed, or as directed by a doctor. Do not exceed 8 tablets in 24 hoursAdults 60 years and over2 tablets every 6 hours, as needed, or as directed by a doctor. Do not exceed 4 tablets in 24 hoursChildren under 12 yearsDo not use',\n 'drug_abuse_and_dependence': '',\n 'drug_abuse_and_dependence_table': '',\n 'drug_and_or_laboratory_test_interactions_table': '',\n 'drug_interactions_table': '',\n 'effective_time': '20231229',\n 'general_precautions': '',\n 'general_precautions_table': '',\n 'generic_name': 'ASPIRIN, CAFFEINE',\n 'geriatric_use': '',\n 'how_supplied_table': '',\n 'id': '0daa059c-59cd-268d-e063-6294a90a0c61',\n 'inactive_ingredient': 'Inactive ingredients acesulfame potassium, anhydrous citric acid, aspartame, docusate sodium, flavors, mannitol, povidone, sodium benzoate, sodium bicarbonate',\n 'indications_and_usage': 'Uses for the temporary relief of minor aches and pains associated with a hangover helps restore mental alertness or wakefulness when experiencing fatigue or drowsiness associated with a hangover also for the temporary relief of headaches or body aches and pains alone for the temporary relief of headache pain also temporarily relieves minor aches and pains due to: backache muscle aches arthritis menstrual cramps hangover for the temporary relief of muscle aches and pain helps restore mental alertness when experiencing fatigue',\n 'indications_and_usage_table': '',\n 'information_for_patients_table': '',\n 'instructions_for_use': '',\n 'labor_and_delivery': '',\n 'labor_and_delivery_table': '',\n 'laboratory_tests': '',\n 'laboratory_tests_table': '',\n 'last_updated': '2024-08-16',\n 'mechanism_of_action_table': '',\n 'microbiology_table': '',\n 'nonteratogenic_effects': '',\n 'nursing_mothers': '',\n 'nursing_mothers_table': '',\n 'overdosage': '',\n 'overdosage_table': '',\n 'package_label_principal_display_panel': 'Blowfish for Hangovers Blowfish for Hangovers Alertness Aid - Caffeine Pain Reliever - Aspirin (NSAID) 12 effervescent tablets Lemon Flavor 12_Count_Box_Outside_Revised_12_27.jpg IMAGE NAME IMAGE DELETE IMAGE REFERENCED 12_Count_Box_Outside_Revised_12_27.jpg [Delete Image] Yes product--12Tablet.jpg 3D Image of Package Full Package Blowfish for Headaches 20 effervescent tablets Lemon Flavor Alertness Aid - Caffeine Pain Reliever - Aspirin (NSAID) Headache PDP Headache Drug Facts Blowfish Sport Blowfish Sport Caffeinated Pain Relief 20 effervescent tablets Lemon Flavor Alertness Aid - Caffeine Pain Reliever - Aspirin (NSAID) Sport Front Panel Sport Drug Facts Blowfish for Headaches 20 effervescent tablets Lemon Flavor Alertness Aid - Caffeine Pain Reliever - Aspirin (NSAID) Headache PDP Headache Drug Facts Blowfish Sport Blowfish Sport Caffeinated Pain Relief 20 effervescent tablets Lemon Flavor Alertness Aid - Caffeine Pain Reliever - Aspirin (NSAID) Sport Front Panel Sport Drug Facts',\n 'patient_medication_information': '',\n 'pediatric_use_table': '',\n 'pharmacodynamics_table': '',\n 'pharmacogenomics_table': '',\n 'pharmacokinetics_table': '',\n 'precautions': '',\n 'precautions_table': '',\n 'pregnancy': '',\n 'pregnancy_or_breast_feeding': 'If pregnant or breastfeeding ask a health professional before use. It is especially important not to use aspiring during the last 3 months of pregnancy unless definitely directed to do so by a doctor because it may cause problems in the unborn child or complications during delivery.',\n 'pregnancy_or_breast_feeding_table': '',\n 'pregnancy_table': '',\n 'product_type': 'HUMAN OTC DRUG',\n 'purpose_table': '',\n 'questions_table': '',\n 'recent_major_changes': '',\n 'recent_major_changes_table': '',\n 'risks': '',\n 'risks_table': '',\n 'spl_medguide': '',\n 'spl_patient_package_insert': '',\n 'statement_of_identity': '',\n 'stop_use_table': '',\n 'storage_and_handling': 'Other information each tablet contains: sodium 406 mg phenylketonurics: contains phenylalanine 12.6 mg per tablet store at room temperature (59–86°F). Protect from excessive heat.',\n 'storage_and_handling_table': '',\n 'substance_name': 'ASPIRIN CAFFEINE',\n 'summary_of_safety_and_effectiveness': '',\n 'teratogenic_effects': '',\n 'use_in_specific_populations': '',\n 'use_in_specific_populations_table': '',\n 'user_safety_warnings': '',\n 'warnings': 'Warnings Reye’s syndrome: Children and teenagers who have or are recovering from chicken pox or flu-like symptoms should not use this product. When using this product. If changes in behavior with nausea and vomiting occur, consult a doctor because these symptoms could be an early sign of Reyes syndrome, a rare but serious illness. Allergy alert: Aspirin may cause a severe allergic reaction which may include: hives facial swelling asthma (wheezing) shock Stomach bleeding warning: This product contains a nonsteroidal anti-inflammatory drug (NSAID) which may cause severe stomach bleeding. The chance is higher if you are age 60 or older have had stomach ulcers or bleeding problems take a blood thinning (anticoagulant) or steroid drug take other drugs containing prescription or non-prescription NSAIDs (aspirin, ibuprofen, naproxen, or others) have 3 or more alcoholic drinks every day while using this product take more or for a longer time than directed',\n 'warnings_and_cautions_table': '',\n 'when_using': 'Caffeine warning the recommended dose of this product contains about as much caffeine as a cup of coffee. Limit the use of caffeine-containing medications, foods, or beverages because too much caffeine may cause nervousness, irritability, irritability, sleeplessness, and occasionally rapid heart beat. for occasional use only. Do not use for more than 2 days for a hangover unless directed by a doctor. Not intended for use as a substitute for sleep. If fatigue or drowsiness persists or continues to occur, consult a doctor.',\n 'when_using_table': ''}"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Imprime el consumo de GPU antes de cargar el modelo pre-entrenado","metadata":{}},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:40:08.787989Z","iopub.execute_input":"2024-09-20T03:40:08.788359Z","iopub.status.idle":"2024-09-20T03:40:08.815917Z","shell.execute_reply.started":"2024-09-20T03:40:08.788329Z","shell.execute_reply":"2024-09-20T03:40:08.815031Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"GPU memory occupied: 265 MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n#model_name='meta-llama/Meta-Llama-3-8B'\nmodel_name = 'meta-llama/Llama-2-7b-hf'\ntry:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:40:14.189467Z","iopub.execute_input":"2024-09-20T03:40:14.189826Z","iopub.status.idle":"2024-09-20T03:41:29.882419Z","shell.execute_reply.started":"2024-09-20T03:40:14.189797Z","shell.execute_reply":"2024-09-20T03:41:29.881512Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2790a56860423a918f19822eb4e67e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138f3b952e1e479ab74fd9c257bcddf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0087fd2fa6244e259018cd9e96ca8c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61dc8ead1e754b1e9f0c84ec969aa59f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7779a0d009146ea9b7a24b6574db4d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"406f76b10b554e35b4ec46e6c45ea194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c9aed06d7e434eb8b05d78be413bea"}},"metadata":{}},{"name":"stdout","text":"self.model_name_or_path : meta-llama/Llama-2-7b-hf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd59ea821aa14fb9b7db1e126b71459b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42de863872084f218cbbd6a76060ce09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58cb49b457e435ca42ad307e4bbc1ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7cb9c7072e944ea9e6fd35a152d262f"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 21.7 s, sys: 20.9 s, total: 42.6 s\nWall time: 1min 15s\n","output_type":"stream"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:39:06.838786Z","iopub.execute_input":"2024-09-20T05:39:06.839426Z","iopub.status.idle":"2024-09-20T05:39:06.845137Z","shell.execute_reply.started":"2024-09-20T05:39:06.839393Z","shell.execute_reply":"2024-09-20T05:39:06.844270Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"GPU memory occupied: 4843 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 6. Prueba el modelo con inferencia Zero Shot","metadata":{}},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 120\nset_seed(seed)\nmax_tokens = 100\n\ntry:\n    prompt = dataset['train'][index]\n\n\n    # Instrucción: Resume la siguiente conversación\n    formatted_prompt = f'Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\\n Provide a detailed description of the medication {prompt[\"generic_name\"]} using the available data.\\n Output:\\n'\n    res = llm.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n    output = res[0].split('Output:\\n')[1]\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Model Generation - Zero Shot:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:42:55.346875Z","iopub.execute_input":"2024-09-20T03:42:55.347244Z","iopub.status.idle":"2024-09-20T03:43:03.002749Z","shell.execute_reply.started":"2024-09-20T03:42:55.347195Z","shell.execute_reply":"2024-09-20T03:43:03.001740Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nInput Prompt:\nInstruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\n Provide a detailed description of the medication ASPIRIN, CAFFEINE using the available data.\n Output:\n\n---------------------------------------------------------------------------------------------------\nModel Generation - Zero Shot:\n nobody is coming to save us\nnobody is coming to save us, and nobody is coming to save us.\nWe are going to have to do it ourselves.\nWe are going to have to find the resources to do it ourselves.\nWe are going to have to find the courage to do it ourselves.\nWe are going to have to find the strength to do it ourselves.\nWe are going to have to find the love to do it ourselves.\nWe are going\nCPU times: user 7.65 s, sys: 0 ns, total: 7.65 s\nWall time: 7.65 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 7. Pre-procesando el dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntry:\n    max_length = llm.get_max_length()\n    \n    train_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                       max_length=max_length,\n                                       seed=seed,\n                                       dataset=dataset['train']\n                                      )\n    \n    eval_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                      max_length=max_length,\n                                      seed=seed,\n                                      dataset=dataset['validation']\n                                     )\nexcept Exception as ex:\n    print(f\"Error [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:44:18.191741Z","iopub.execute_input":"2024-09-20T03:44:18.192602Z","iopub.status.idle":"2024-09-20T05:35:09.957018Z","shell.execute_reply.started":"2024-09-20T03:44:18.192559Z","shell.execute_reply":"2024-09-20T05:35:09.955996Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Found max length: 4096\nPreprocessing dataset...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/160299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"112028de6a544f4c8486b1dc6afd1ea8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/160299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f607a6610406434bb714640c9dde1d72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/160299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958d548a3312422e9b3f37d4c8db9804"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/34350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae322c1dd39c4fc8b8abba46c360afa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/34350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e80d15838e14b5d86075ed202e28ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/34350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54897f17d5f54eb288c1a9f8d0c09b4a"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 1h 49min 38s, sys: 1min 2s, total: 1h 50min 41s\nWall time: 1h 50min 51s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:39:20.430612Z","iopub.execute_input":"2024-09-20T05:39:20.430968Z","iopub.status.idle":"2024-09-20T05:39:20.436325Z","shell.execute_reply.started":"2024-09-20T05:39:20.430942Z","shell.execute_reply":"2024-09-20T05:39:20.435473Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (112701, 3)\nValidation: (24177, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 112701\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 8. Configura el modelo PEFT/LoRA para el Fine-Tuning\nAhora, vamos a realizar un ajuste fino eficiente en parámetros (PEFT). PEFT es una forma de ajuste fino por instrucciones que es mucho más eficiente que el ajuste fino completo. PEFT es un término genérico que incluye Adaptación de Bajo Rango (LoRA) y ajuste por indicaciones (¡que NO ES LO MISMO que la ingeniería de prompts!). En la mayoría de los casos, cuando alguien menciona PEFT, generalmente se refieren a LoRA. LoRA, en esencia, permite un ajuste fino eficiente del modelo utilizando menos recursos computacionales, a menudo realizable con solo una GPU. Después del ajuste fino con LoRA para una tarea o caso de uso específico, el resultado es un LLM original sin cambios y la aparición de un \"adaptador LoRA\" considerablemente más pequeño, que a menudo representa un porcentaje de un solo dígito del tamaño del LLM original (en MBs en lugar de GBs).\n\nDurante la inferencia, el adaptador LoRA debe combinarse con su LLM original. La ventaja radica en la capacidad de muchos adaptadores LoRA para reutilizar el LLM original, reduciendo así los requisitos generales de memoria cuando se manejan múltiples tareas y casos de uso.\n\nNota el hiperparámetro de rango (r), que define el rango/dimensión del adaptador a ser entrenado. r es el rango de la matriz de bajo rango utilizada en los adaptadores, lo que controla el número de parámetros entrenados. Un rango mayor permitirá mayor expresividad, pero hay una compensación en términos de cómputo.\n\nalpha es el factor de escalado para los pesos aprendidos. La matriz de pesos se escala por alpha/r, y por lo tanto, un valor más alto de alpha asigna más peso a las activaciones de LoRA.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(llm.model))","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:39:41.807378Z","iopub.execute_input":"2024-09-20T05:39:41.808264Z","iopub.status.idle":"2024-09-20T05:39:41.815903Z","shell.execute_reply.started":"2024-09-20T05:39:41.808202Z","shell.execute_reply":"2024-09-20T05:39:41.814984Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"all model parameters: 3500412928\ntrainable model parameters: 262410240\npercentage of trainable model parameters: 7.50%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(llm.model)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:39:49.999192Z","iopub.execute_input":"2024-09-20T05:39:50.000039Z","iopub.status.idle":"2024-09-20T05:39:50.007481Z","shell.execute_reply.started":"2024-09-20T05:39:50.000008Z","shell.execute_reply":"2024-09-20T05:39:50.006550Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_config = LoraConfig(r=64, #32\n                         lora_alpha=16, #32,\n                         target_modules=['q_proj','k_proj','v_proj','o_proj'], #dense\n                         bias=\"none\",\n                         lora_dropout=0.1, #0.05,  # Conventional\n                         task_type=\"CAUSAL_LM\",\n                        )\n\n# 1 - Habilitando el registro de puntos de control de gradiente para reducir el uso de memoria \n# durante el fine-tuning\nllm.model.gradient_checkpointing_enable()\n\n# 2 - Utilizando el método prepare_model_for_kbit_training de PEFT.\nllm.model = prepare_model_for_kbit_training(llm.model)\n\npeft_model = get_peft_model(llm.model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:39:59.731949Z","iopub.execute_input":"2024-09-20T05:39:59.732342Z","iopub.status.idle":"2024-09-20T05:40:00.813654Z","shell.execute_reply.started":"2024-09-20T05:39:59.732309Z","shell.execute_reply":"2024-09-20T05:40:00.812868Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Una vez que todo esté configurado y el modelo base esté preparado, podemos utilizar la función auxiliar print_trainable_parameters() para ver cuántos parámetros entrenables hay en el modelo.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:40:11.534082Z","iopub.execute_input":"2024-09-20T05:40:11.534675Z","iopub.status.idle":"2024-09-20T05:40:11.546411Z","shell.execute_reply.started":"2024-09-20T05:40:11.534644Z","shell.execute_reply":"2024-09-20T05:40:11.545440Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"all model parameters: 3567521792\ntrainable model parameters: 67108864\npercentage of trainable model parameters: 1.88%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Observa cómo se ve diferente el modelo ahora, con los adaptadores LoRA añadidos:\nprint(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:40:21.563967Z","iopub.execute_input":"2024-09-20T05:40:21.564340Z","iopub.status.idle":"2024-09-20T05:40:21.580824Z","shell.execute_reply.started":"2024-09-20T05:40:21.564309Z","shell.execute_reply":"2024-09-20T05:40:21.579985Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 9. Entrenando el Adaptador PEFT\n\nDefine los argumentos de entrenamiento y crea una instancia de Trainer.","metadata":{}},{"cell_type":"code","source":"from torch import amp\n\noutput_dir = './drugs-final-checkpoint'\n\n'''\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n'''\n\n# Cambiar torch.cuda.amp.GradScaler a torch.amp.GradScaler\n#scaler = amp.GradScaler('cuda')\n\npeft_training_args = TrainingArguments(\n    do_eval=True,\n    eval_strategy=\"steps\",\n    fp16=True,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    learning_rate=2.0e-05,\n    logging_steps=25,\n    log_level=\"info\",\n    logging_strategy=\"steps\",\n    lr_scheduler_type=\"cosine\",\n    max_steps=1000,\n    #num_train_epochs=1,\n    output_dir = output_dir,\n    overwrite_output_dir = True,\n    per_device_eval_batch_size=1,\n    per_device_train_batch_size=1,\n    report_to=\"none\",\n    save_strategy=\"steps\",\n    eval_steps=25,\n    group_by_length=True,\n    logging_dir=\"./logs\",\n    optim=\"paged_adamw_8bit\",\n    save_steps=25,\n    warmup_steps=50,\n    save_total_limit=None,\n    seed=42\n)\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(model=peft_model,\n                                    train_dataset=train_dataset,\n                                    eval_dataset=eval_dataset,\n                                    args=peft_training_args,\n                                    data_collator=transformers.DataCollatorForLanguageModeling(llm.tokenizer, mlm=False),\n                                    )","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:41:10.187326Z","iopub.execute_input":"2024-09-20T05:41:10.187685Z","iopub.status.idle":"2024-09-20T05:41:10.233948Z","shell.execute_reply.started":"2024-09-20T05:41:10.187656Z","shell.execute_reply":"2024-09-20T05:41:10.233077Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_training_args.device\n#print(f\"GPUs disponibles: {torch.cuda.device_count()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:41:19.201910Z","iopub.execute_input":"2024-09-20T05:41:19.202654Z","iopub.status.idle":"2024-09-20T05:41:19.208180Z","shell.execute_reply.started":"2024-09-20T05:41:19.202619Z","shell.execute_reply":"2024-09-20T05:41:19.207186Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T05:41:50.729184Z","iopub.execute_input":"2024-09-20T05:41:50.730021Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 112,701\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Training with DataParallel so batch size has been adjusted to: 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1,000\n  Number of trainable parameters = 67,108,864\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  26/1000 22:47 < 15:24:49, 0.02 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='8164' max='12089' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 8164/12089 9:23:01 < 4:30:43, 0.24 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 24177\n  Batch size = 2\n","output_type":"stream"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Liberar memoria para la fusión de pesos\ndel llm.model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-09-20T03:26:09.222867Z","iopub.execute_input":"2024-09-20T03:26:09.223327Z","iopub.status.idle":"2024-09-20T03:26:09.278374Z","shell.execute_reply.started":"2024-09-20T03:26:09.223289Z","shell.execute_reply":"2024-09-20T03:26:09.277139Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Liberar memoria para la fusión de pesos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m peft_trainer\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","\u001b[0;31mAttributeError\u001b[0m: model"],"ename":"AttributeError","evalue":"model","output_type":"error"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cualitativamente (Evaluación Humana)","metadata":{}},{"cell_type":"code","source":"try:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nft_model = PeftModel.from_pretrained(llm.model, \n                                     \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",\n                                     torch_dtype=torch.float16,\n                                     is_trainable=False\n                                    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 120\nset_seed(seed)\nmax_tokens = 512\n\ntry:\n    prompt = dataset['train'][index]\n\n    # Instrucción: Resume la siguiente conversación\n    formatted_prompt = f'Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\\n Provide a detailed description of the medication {prompt[\"generic_name\"]} using the available data.\\n Output:\\n'\n    res = ft_model.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n    output = res[0].split('Output:\\n')[1]\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Peft Model Generation:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cuantitativamente (con la Metrica ROUGE)","metadata":{}},{"cell_type":"code","source":"def data_process(dataset):\n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n\n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n\n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n\n        dataset = dataset.map(create_prompt_formats_v1,\n                              num_proc=num_proc\n                             )#, batched=True)\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    train_dataset=data_process(dataset['train'])\n    eval_dataset=data_process(dataset['validation'])\n\n    print(f\"Shapes of the datasets:\")\n    print(f\"Training: {train_dataset.shape}\")\n    print(f\"Validation: {eval_dataset.shape}\")\n    \nexcept Exception as ex:\n    print(f'Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_dataset[120])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset[120]","metadata":{},"execution_count":null,"outputs":[]}]}