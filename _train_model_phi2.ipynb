{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9501867,"sourceType":"datasetVersion","datasetId":5782792}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**En este notebook y tutorial, realizaremos un fine-tune [Llama-8k](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) modelo relativamente pequeño de 7 mil millones de parametros - que ha 'demostrado un rendimiento casi de última generación entre los modelos con menos de 13 mil millones de parámetros' - *en tus propios datos!!***\n\n**Aqui usaremos [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), una técnica de fine-tunning altamente eficiente que consiste en cuantizar un LLM preentrenado a solo 4 bits y agregar pequeños 'Adaptadores de Bajo Rango'. Este enfoque único permite realizar el fine-tunning de LLMs utilizando solo una GPU. Esta técnica está respaldada por el/la... [PEFT library](https://huggingface.co/docs/peft/index).**","metadata":{}},{"cell_type":"markdown","source":"# Tabla de Contenido","metadata":{}},{"cell_type":"markdown","source":"- [1- Instalar librerias requeridas](#1)\n- [ 2 - Cargar dataset](#2)\n- [ 3 - Crear configuración de bitsandbytes](#3)\n- [ 4 - Cargar Modelo Base](#4)\n- [ 5 - Tokenizar](#5)\n- [ 6 - Testear el modelo con Zero Shot Inferencing](#6)\n- [ 7 - Pre-procesando el dataset](#7)\n- [ 8 - Configurar el modelo PEFT/LoRA para realizar Fine-Tuning](#8)\n- [ 9 - Entrenar Adaptador PEFT](#9)\n- [ 10 - Evaluar el Modelo Qualitativamente (Evaluacion Humana)](#10)\n- [ 11 - Evaluar el Modelo Quantitaviamente (con Metrica ROUGE)](#11)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n#### 1. Instalar librerias requeridas","metadata":{}},{"cell_type":"code","source":"%%time\n!pip install -U transformers\n!pip install -U bitsandbytes\n!pip install -U peft\n!pip install -U accelerate\n!pip install -U datasets\n!pip install -U scipy\n!pip install -U einops\n!pip install -U evaluate\n!pip install -U trl\n!pip install -U rouge_score\n!pip install -U torch","metadata":{"_uuid":"c34dd264-0a8e-4fae-8f88-83c906c39c3c","_cell_guid":"59a8cbf9-4057-4756-a512-100f15222529","execution":{"iopub.status.busy":"2024-09-30T19:28:18.992341Z","iopub.execute_input":"2024-09-30T19:28:18.992693Z","iopub.status.idle":"2024-09-30T19:33:06.799189Z","shell.execute_reply.started":"2024-09-30T19:28:18.992663Z","shell.execute_reply":"2024-09-30T19:33:06.798071Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed tokenizers-0.20.0 transformers-4.45.1\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nCollecting peft\n  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\nSuccessfully installed accelerate-0.34.2\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting datasets\n  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\nSuccessfully installed datasets-3.0.1\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nCollecting scipy\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\nDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.4\n    Uninstalling scipy-1.11.4:\n      Successfully uninstalled scipy-1.11.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.14.0\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting trl\n  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2)\nRequirement already satisfied: transformers>=4.40.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.45.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.5.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.4.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (4.66.4)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.40.0->trl) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, docstring-parser, tyro, trl\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed docstring-parser-0.16 shtab-1.7.1 trl-0.11.1 tyro-0.8.11\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9c017c18ee6c9c3c04d2e93564dbcdc355a6f57f573aa8bd555a1a5bf865e594\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting torch\n  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\nCPU times: user 3.78 s, sys: 876 ms, total: 4.65 s\nWall time: 4min 47s\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:33:53.350705Z","iopub.execute_input":"2024-09-30T19:33:53.351113Z","iopub.status.idle":"2024-09-30T19:33:55.169351Z","shell.execute_reply.started":"2024-09-30T19:33:53.351077Z","shell.execute_reply":"2024-09-30T19:33:55.168229Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.4.1+cu121\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport os\nimport shutil\nimport zipfile\nimport gc\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport multiprocessing\nimport psutil\nimport requests\nimport tarfile\nimport json\nimport evaluate\nimport datetime, os\n\nfrom trl import SFTTrainer\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig,\n    EarlyStoppingCallback\n)\nfrom transformers.integrations import TensorBoardCallback\nfrom tqdm import tqdm\nfrom huggingface_hub import interpreter_login\nfrom pynvml import *\nfrom functools import partial\nfrom transformers import set_seed\nfrom datasets import load_dataset, DatasetDict, load_from_disk\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom IPython.display import FileLink\nfrom urllib.request import urlopen\nfrom io import BytesIO\nfrom subprocess import Popen\nfrom os import chmod\nfrom os.path import isfile\n\n#interpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:33:58.669666Z","iopub.execute_input":"2024-09-30T19:33:58.670096Z","iopub.status.idle":"2024-09-30T19:34:15.185724Z","shell.execute_reply.started":"2024-09-30T19:33:58.670069Z","shell.execute_reply":"2024-09-30T19:34:15.184784Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2024-09-30 19:34:03.582467: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-30 19:34:03.582560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-30 19:34:03.715598: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 9.36 s, sys: 1.22 s, total: 10.6 s\nWall time: 16.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"# Habilitar los permisos necesarios para acceder a google-drive\nimport os\n\nPROJECT_NAME = 'drugs-generative'\n\ntry:\n    from google.colab import drive\n    ROOT = '/content/drive/'\n    drive.mount(ROOT, force_remount=True)\n    IN_COLAB = True\n    BASE_FOLDER = ROOT + 'MyDrive/' + PROJECT_NAME\n    DATASET_FOLDER = BASE_FOLDER\nexcept:\n    #ROOT = '/kaggle/input/drugs-data'\n    ROOT = '/kaggle'\n    IN_COLAB = False\n    BASE_FOLDER = os.path.join(\"/kaggle/working\", PROJECT_NAME)\n    DATASET_FOLDER = os.path.join(\"/kaggle/input\", PROJECT_NAME)\n    TOKENIZER_FOLDER = os.path.join(\"/kaggle/input\", 'drugs-tokenizer')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:37:32.245412Z","iopub.execute_input":"2024-09-30T19:37:32.246272Z","iopub.status.idle":"2024-09-30T19:37:32.253223Z","shell.execute_reply.started":"2024-09-30T19:37:32.246236Z","shell.execute_reply":"2024-09-30T19:37:32.252235Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nUSE_ALL_DATASET = False\nNUMBER_ELEMENT = 10000\nSAVE_TOKENIZER = True\nLOAD_TOKENIZER = False\nPROCESS_SAMPLE = False\nNGROK_TOKEN = '2mfZzvcUfXHZqEB2Cc3REgZQ3eG_8a2WJJCc9vp9UpVV3AFVT'\nHUGGING_TOKEN = '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'\n\n#model_name='meta-llama/Meta-Llama-3-8B'\nmodel_name = 'meta-llama/Llama-2-7b-hf'\nname_zip_tokenizer = 'tokenizer.zip'\nlog_name_directory = 'logs'\nngrok_url = 'https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz'\ntrain_dataset = None\neval_dataset = None\nseed = 42\n\nDATASET_PATH = os.path.join(DATASET_FOLDER, \"drugs_data.parquet\")\nTOKENIZER_PATH = os.path.join(TOKENIZER_FOLDER, \"tokenizer.zip\")\nLOG_TRAIN_PATH = os.path.join(BASE_FOLDER, log_name_directory)\n\n# Valida directorio principal del proyecto\nif not(os.path.exists(BASE_FOLDER)):\n    !mkdir -p {BASE_FOLDER}\n    print('Directorio proyecto creado exitosamente!!')\n\n    \n# Valida directorio en donde se almacenan los logs del entrenamiento\nif not(os.path.exists(LOG_TRAIN_PATH)):\n    !mkdir -p {LOG_TRAIN_PATH}\n    print('Directorio para almacenar logs creado exitosamente!!')    \n    \n# Valida descarga dataset del Proyecto\nif not (os.path.exists(DATASET_PATH)):\n    print('Dataset no existe!!')\n\n\n################################################################################\n# Model parameters\npadding_side = \"right\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = LOG_TRAIN_PATH\n\n# Number of training epochs\nnum_train_epochs = 10\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 1\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 1\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 100\n\n# Log every X updates steps\nlogging_steps = 25\n\n\neval_strategy = 'steps'\n\n\neval_steps = 25\n\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\n# device_map = {\"\": 0}\ndevice_map = \"auto\"\n    \n################################################################################\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        bf16 = True\n        print(\"=\" * 80)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:11:32.179024Z","iopub.execute_input":"2024-09-30T20:11:32.179366Z","iopub.status.idle":"2024-09-30T20:11:32.206676Z","shell.execute_reply.started":"2024-09-30T20:11:32.179340Z","shell.execute_reply":"2024-09-30T20:11:32.205761Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n#### 2. Definición de Funciones ","metadata":{}},{"cell_type":"code","source":"# Funcion para imprimir la utilización de la memoria de la GPU\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\n# Función para reemplazar NaN con cadena vacía\ndef replace_nan_with_empty_string(example):\n    for key, value in example.items():\n        if value is None or pd.isna(value) or (value == 'nan'):\n            example[key] = ''\n    return example\n\n\ndef create_prompt_formats_llama3(sample):\n    '''\n    \n    '''\n    #===========================================================================================\n    try:\n        # Construir las partes iniciales\n        instruct_key = '### Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\"'\n        context_key = '### Context: You are a pharmaceutical chemist specialized in the in-depth understanding of drug descriptions. Your task is to generate a professional and accurate response based on the information provided. If a specific field lacks information, state \"No specific information available\" instead of providing unconfirmed details.'\n        input_key = f\"### Input: Provide a detailed description of the medication {sample.get('generic_name', '')} using the available data.\"\n        end_key = \"### End\"\n\n        # Lista de campos a procesar\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                drugs.append(f'<{field}> {label_name}: {field_value} </{field}>')\n                if question_text:\n                    questions.append(f'<question> {question_text}</question><answer> {field_value}</answer>')\n\n        # Construir las partes finales\n        output_key = f\"### Output: {sample.get('description', '')}\"\n        if drugs:\n            output_key += \"\\n\" + \"\\n\".join(drugs)\n\n        question_key = '### Questions: ' + (\"\\n\".join(questions) if questions else \"\")\n\n        # Construir el texto final\n        parts = [instruct_key, context_key, input_key, output_key, question_key, end_key]\n        sample[\"text\"] = \"\\n\\n\".join(parts)\n\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    return sample\n\ndef create_prompt_formats_llama2(sample):\n    '''\n\n    '''\n    #===========================================================================================\n    try:\n        # Lista de campos a procesar: campo en el dataset, nombre a mostrar, pregunta asociada\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos y construir las secciones de descripción y preguntas/respuestas\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                # Añadir al bloque de descripción del medicamento en formato simple\n                drugs.append(f'{label_name}: {field_value}')\n                # Si hay una pregunta asociada al campo, añadirla también\n                if question_text:\n                    questions.append(f'Question: {question_text}\\nAnswer: {field_value}')\n\n        # Mensaje del sistema con el prompt mejorado\n        system_message = f\"\"\"You are a helpful Medical Assistant. Your task is to generate descriptions of medications or respond to questions related to them, depending on the user's request.\n\n        If the user requests a **medication description**, follow this structure:\n        - Brand Name: [Brand Name]\n        - Generic Name: [Generic Name]\n        - Active Ingredient: [Active ingredients]\n        - Indications: [Uses]\n        - Dosage: [Recommended dosage]\n        - Side Effects: [Common side effects]\n        - Contraindications: [When the medication should not be used]\n        - Interactions: [Drugs or substances that interact with this medication]\n\n        If the user asks a **direct question about the medication**, answer based on the provided information and the medication's context.\n\n        Please use only the available information in the provided context.\n\n        Guidelines:\n        - Maintain a professional, precise, and concise tone in all responses.\n        - Do not fabricate information. If a field lacks data, state \"No specific information available.\"\n        - Ensure the information is understandable for both healthcare professionals and patients.\n        \"\"\"\n\n        # Construir la descripción y preguntas de manera directa sin etiquetas adicionales\n        description = \"\\n\".join(drugs)\n\n        # Agregar las preguntas si hay\n        if questions:\n            questions_block = \"\\n\\n\".join(questions)\n        else:\n            questions_block = \"No additional questions provided.\"\n\n        # Crear el prompt completo combinando todo dentro de [INST]\n        sample['text'] = f\"\"\"<s>[INST] <<SYS>>\n        {system_message}\n        <</SYS>>\n        {description}\n\n        {questions_block}\n\n        [/INST]</s>\"\"\"\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return sample\n    \n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )\n\n    \ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        create_prompt_formats = None\n        if(model_name == 'meta-llama/Meta-Llama-3-8B'):\n            print(\"Create prompt llama3...\")\n            create_prompt_formats = create_prompt_formats_llama3\n        elif(model_name == 'meta-llama/Llama-2-7b-hf'):\n            print(\"Create prompt llama2...\")\n            create_prompt_formats = create_prompt_formats_llama2\n        \n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        _preprocessing_function = partial(preprocess_batch,\n                                          max_length = max_length,\n                                          tokenizer = tokenizer\n                                         )\n\n        dataset = dataset.map(_preprocessing_function, \n                              remove_columns=[col for col in dataset.column_names if col != \"text\"],\n                              #num_proc=num_proc\n                             )\n\n        # Filtrar las muestras que tienen input_ids que exceden la longitud máxima (max_length).\n        dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef preprocess_dataset_sample(seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        create_prompt_formats = None\n        if(model_name == 'meta-llama/Meta-Llama-3-8B'):\n            print(\"Create prompt llama3...\")\n            create_prompt_formats = create_prompt_formats_llama3\n        elif(model_name == 'meta-llama/Llama-2-7b-hf'):\n            print(\"Create prompt llama2...\")\n            create_prompt_formats = create_prompt_formats_llama2\n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef print_number_of_trainable_model_parameters(model):\n    try:\n        trainable_model_params = 0\n        all_model_params = 0\n        for _, param in model.named_parameters():\n            all_model_params += param.numel()\n            if param.requires_grad:\n                trainable_model_params += param.numel()\n        return f\"all model parameters: {all_model_params}\\ntrainable model parameters: {trainable_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n    except Exception as ex:\n        print(f'Ocurrió un error inesperado al imprimir los parametros del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n\ndef launch_tensorboard():\n    tb_process, ngrok_process = None, None\n    \n    # Launch TensorBoard\n    if not is_process_running('tensorboard'):\n        tb_command = f'tensorboard --logdir {LOG_TRAIN_PATH}/runs/ --host 0.0.0.0 --port 6006'\n        tb_process = run_cmd_async_unsafe(tb_command)\n    \n    # Install ngrok\n    if not isfile('./ngrok'):\n        #ngrok_url = 'https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\n        print('Inicia descarga de ngrok....')\n        download_and_extract(ngrok_url)\n        chmod('./ngrok', 0o755)\n        \n        #Registra token de autorizacion\n        tb_command = f'./ngrok config add-authtoken {NGROK_TOKEN}'\n        tb_process = run_cmd_async_unsafe(tb_command)\n\n    # Create ngrok tunnel and print its public URL\n    if not is_process_running('ngrok'):\n        ngrok_process = run_cmd_async_unsafe('./ngrok http 6006')\n        time.sleep(1) # Waiting for ngrok to start the tunnel\n    \n    ngrok_api_res = urlopen('http://127.0.0.1:4040/api/tunnels', timeout=10)\n    ngrok_api_res = json.load(ngrok_api_res)\n    assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n    tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n    print(f'TensorBoard URL: {tb_public_url}')\n\n    return tb_process, ngrok_process\n\n\ndef download_and_extract(url, extract_to='.'):\n    try:\n        # Descargar el archivo\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Lanza una excepción si la respuesta tiene un error\n\n        # Detectar el tipo de archivo a partir de la URL\n        if url.endswith('.zip'):\n            # Si es un archivo ZIP, utilizar ZipFile\n            with ZipFile(BytesIO(response.content)) as zip_file:\n                zip_file.extractall(path=extract_to)\n                print(f'Archivo ZIP extraído en: {os.path.abspath(extract_to)}')\n\n        elif url.endswith('.tgz') or url.endswith('.tar.gz'):\n            # Si es un archivo .tgz o .tar.gz, utilizar tarfile\n            with tarfile.open(fileobj=BytesIO(response.content), mode='r:gz') as tar_file:\n                tar_file.extractall(path=extract_to)\n                print(f'Archivo TGZ extraído en: {os.path.abspath(extract_to)}')\n\n        else:\n            print(\"Formato de archivo no soportado.\")\n            return\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error en la descarga: {e}\")\n    except Exception as e:\n        print(f\"Ocurrió un error inesperado: {e}, {e.__traceback__.tb_lineno}\")\n\n\ndef run_cmd_async_unsafe(cmd):\n    return Popen(cmd, shell=True)\n\n\ndef is_process_running(process_name):\n    running_process_names = (proc.name() for proc in psutil.process_iter())\n    return process_name in running_process_names\n        \n\ndef compute_perplexity(eval_pred):\n    logits, labels = eval_pred\n    loss = eval_pred.loss\n    perplexity = math.exp(loss)\n    return {\"perplexity\": perplexity}\n\n\ndef compute_rouge(eval_pred):\n    rouge_metric = evaluate.load_metric(\"rouge\")\n    predictions, labels = eval_pred\n    predictions = [pred.strip() for pred in predictions]\n    references = [label.strip() for label in labels]\n    rouge_scores = rouge_metric.compute(predictions=predictions, references=references)\n    return {\n        \"rouge1\": rouge_scores[\"rouge1\"].mid.fmeasure,\n        \"rouge2\": rouge_scores[\"rouge2\"].mid.fmeasure,\n        \"rougeL\": rouge_scores[\"rougeL\"].mid.fmeasure\n    }\n\n\ndef compute_meteor(eval_pred):\n    meteor_metric = evaluate.load_metric(\"meteor\")\n    predictions, labels = eval_pred\n    predictions = [pred.strip() for pred in predictions]\n    references = [label.strip() for label in labels]\n    meteor_score = meteor_metric.compute(predictions=predictions, references=references)\n    return {\"meteor\": meteor_score[\"meteor\"]}\n\n\ndef compute_exact_match(eval_pred):\n    predictions, labels = eval_pred\n    exact_matches = sum([1 if pred == ref else 0 for pred, ref in zip(predictions, labels)]) / len(labels)\n    return {\"exact_match\": exact_matches}\n\n\n# Definir las métricas de evaluación\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = [logit.argmax(dim=-1) for logit in logits]\n\n    # Calcular todas las métricas\n    perplexity = compute_perplexity((logits, labels))[\"perplexity\"]\n    rouge = compute_rouge((predictions, labels))\n    meteor = compute_meteor((predictions, labels))[\"meteor\"]\n    exact_match = compute_exact_match((predictions, labels))[\"exact_match\"]\n\n    # Guardar métricas en un archivo CSV\n    with open('metrics.csv', mode='a') as metrics_file:\n        metrics_writer = csv.writer(metrics_file)\n        metrics_writer.writerow([perplexity, rouge[\"rouge1\"], rouge[\"rouge2\"], rouge[\"rougeL\"], meteor, exact_match])\n\n    # Retornar todas las métricas\n    return {\n        \"perplexity\": perplexity,\n        \"rouge1\": rouge[\"rouge1\"],\n        \"rouge2\": rouge[\"rouge2\"],\n        \"rougeL\": rouge[\"rougeL\"],\n        \"meteor\": meteor,\n        \"exact_match\": exact_match\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:51:56.054932Z","iopub.execute_input":"2024-09-30T19:51:56.055332Z","iopub.status.idle":"2024-09-30T19:51:56.107101Z","shell.execute_reply.started":"2024-09-30T19:51:56.055292Z","shell.execute_reply":"2024-09-30T19:51:56.106036Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ModelAnalizer:\n    '''\n    '''\n    \n    def __init__(self, model_name_or_path):\n        self.model_name_or_path = model_name_or_path\n        self.model = None\n        self.tokenizer = None\n        self._load_qtz_config()\n    \n    \n    def _load_qtz_config(self):\n        try:\n            #compute_dtype = getattr(torch, \"float16\")\n            \n            # Load tokenizer and model with QLoRA configuration\n            self.bnb_config = BitsAndBytesConfig(load_in_4bit=use_4bit, #True,\n                                                 bnb_4bit_quant_type=bnb_4bit_quant_type, #'nf4',\n                                                 bnb_4bit_compute_dtype=compute_dtype,\n                                                 bnb_4bit_use_double_quant=use_nested_quant #False,\n                                                )\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar quantization-config [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n        \n    def _load_model(self):\n        try:\n            #device_map = {\"\": 0}\n            self.model = AutoModelForCausalLM.from_pretrained(self.model_name_or_path, \n                                                              device_map=device_map,\n                                                              quantization_config=self.bnb_config,\n                                                              trust_remote_code=True,\n                                                              token=HUGGING_TOKEN\n                                                              )\n            self.model.config.use_cache = False\n            self.model.config.pretraining_tp = 1\n            # Carga el tokenizador\n            self._tokenizer()\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n            \n    \n    def _tokenizer(self):\n        # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n        try:\n            print(f'self.model_name_or_path : {self.model_name_or_path}')\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, \n                                                          trust_remote_code=True, \n                                                          add_bos_token=True,\n                                                          use_fast=False, \n                                                          add_eos_token=True, \n                                                          padding_side=padding_side, #\"left\",\n                                                          token=HUGGING_TOKEN\n                                                         )\n            if not(self.tokenizer):\n                raise Exception(f'No se ha definido el atributo self.tokenizer')\n            \n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el tokenizador [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    \n    \n    def gen(self, prompt, maxlen=512, sample=True):\n        try:\n            '''\n            eval_tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path,\n                                                           trust_remote_code=True,\n                                                           add_bos_token=True,\n                                                           use_fast=False\n                                                          )\n            eval_tokenizer.pad_token = eval_tokenizer.eos_token\n            \n            toks = eval_tokenizer(p, return_tensors=\"pt\")\n            '''\n            \n            toks = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n            res = self.model.generate(**toks.to(\"cuda\"), \n                                      max_new_tokens=maxlen,\n                                      do_sample=sample,\n                                      num_return_sequences=1,\n                                      temperature=0.7,\n                                      num_beams=1,\n                                      top_p=0.95\n                                     ).to('cpu')\n            return self.tokenizer.batch_decode(res, skip_special_tokens=True)\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al procesar la inferencia en el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n    \n    def get_max_length(self):\n        try:\n            max_length = None\n            for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n                max_length = getattr(self.model.config, length_setting, None)\n                if max_length:\n                    print(f\"Found max length: {max_length}\")\n                    break\n            if not max_length:\n                max_length = 1024\n                print(f\"Using default max length: {max_length}\")\n            return max_length\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al obtener tamaño del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:52:03.977264Z","iopub.execute_input":"2024-09-30T19:52:03.977678Z","iopub.status.idle":"2024-09-30T19:52:03.996715Z","shell.execute_reply.started":"2024-09-30T19:52:03.977644Z","shell.execute_reply":"2024-09-30T19:52:03.995681Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<a name='3'></a>\n#### 3. Cargar el dataset","metadata":{}},{"cell_type":"code","source":"%%time\n#Cargar tu dataset\ndataset = load_dataset('parquet', data_files=DATASET_PATH)\n\n\n# Tomar una muestra aleatoria de x cantidad de registros de forma aleatoria)\nif not(USE_ALL_DATASET):\n    sampled_dataset = dataset['train'].shuffle(seed=42).select(range(NUMBER_ELEMENT))\nelse:\n    sampled_dataset = dataset['train']\n\n# Dividir en 70% train y 30% (test + validation)\ntrain_test_valid = sampled_dataset.train_test_split(test_size=0.3, seed=42)\n\n# Dividir el 30% restante en 15% test y 15% validation\ntest_valid = train_test_valid['test'].train_test_split(test_size=0.5, seed=42)\n\n# Reunir los conjuntos en un DatasetDict\ndataset = DatasetDict({\n    'train': train_test_valid['train'],\n    'test': test_valid['test'],\n    'validation': test_valid['train']\n})\n\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:12:40.991624Z","iopub.execute_input":"2024-09-30T20:12:40.992783Z","iopub.status.idle":"2024-09-30T20:12:41.411261Z","shell.execute_reply.started":"2024-09-30T20:12:40.992747Z","shell.execute_reply":"2024-09-30T20:12:41.410319Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"CPU times: user 191 ms, sys: 40.3 ms, total: 231 ms\nWall time: 409 ms\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 7000\n    })\n    test: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 1500\n    })\n    validation: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][26]","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:12:50.211912Z","iopub.execute_input":"2024-09-30T20:12:50.212849Z","iopub.status.idle":"2024-09-30T20:12:50.228115Z","shell.execute_reply.started":"2024-09-30T20:12:50.212806Z","shell.execute_reply":"2024-09-30T20:12:50.227031Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'abuse': '',\n 'abuse_table': '',\n 'active_ingredient': 'Active ingredient Benzalkonium Chloride 0.13%',\n 'active_ingredient_table': '',\n 'adverse_reactions': '',\n 'adverse_reactions_table': '',\n 'alarms': '',\n 'ask_doctor_or_pharmacist': '',\n 'ask_doctor_or_pharmacist_table': '',\n 'ask_doctor_table': '',\n 'brand_name': '',\n 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility': '',\n 'clinical_pharmacology_table': '',\n 'clinical_studies_table': '',\n 'components_table': '',\n 'contraindications': '',\n 'controlled_substance': '',\n 'dependence': '',\n 'dependence_table': '',\n 'description': '',\n 'description_table': '',\n 'do_not_use_table': '',\n 'dosage_and_administration': 'Directions Pump onto hands as needed. Rub briskly together until dry. Pump onto wounds 3 times a day after cleaning. Allow foam to dissipate. Wipe excess with sterile gauze. May be bandaged once dry.',\n 'dosage_and_administration_table': '',\n 'drug_abuse_and_dependence': '',\n 'drug_abuse_and_dependence_table': '',\n 'drug_and_or_laboratory_test_interactions_table': '',\n 'drug_interactions_table': '',\n 'effective_time': '20220204',\n 'general_precautions': '',\n 'general_precautions_table': '',\n 'generic_name': '',\n 'geriatric_use': '',\n 'how_supplied_table': '',\n 'id': '721e265d-c9c0-4912-b29e-d23d69b9cb7c',\n 'inactive_ingredient': 'Inactive ingredients ionized water, aloe vera, carbamide',\n 'indications_and_usage': 'Uses For hand sanitizing to decrease bacteria on the skin. Recommended for repeated use. For wound sanitizing to help prevent bacterial contamination in cuts, scrapes, burns, lacerations and skin infections.',\n 'indications_and_usage_table': '',\n 'information_for_patients_table': '',\n 'instructions_for_use': '',\n 'labor_and_delivery': '',\n 'labor_and_delivery_table': '',\n 'laboratory_tests': '',\n 'laboratory_tests_table': '',\n 'last_updated': '2024-09-21',\n 'mechanism_of_action_table': '',\n 'microbiology_table': '',\n 'nonteratogenic_effects': '',\n 'nursing_mothers': '',\n 'nursing_mothers_table': '',\n 'overdosage': '',\n 'overdosage_table': '',\n 'package_label_principal_display_panel': 'PRINCIPAL DISPLAY PANEL - 59 mL Bottle Label EVOWELL™ WELLNESS EVOLVED Hand Sanitizer with Aloe Vera EXTENDED PROTECTION* Eliminates 99.99% of Germs alcohol-free triclosan-free & fragrance-free Non-Irritating • Non-Stinging Benzalkonium Chloride 0.13% Antimicrobial & Antiseptic 2.0 FL OZ (59 mL) PRINCIPAL DISPLAY PANEL - 59 mL Bottle Label',\n 'patient_medication_information': '',\n 'pediatric_use_table': '',\n 'pharmacodynamics_table': '',\n 'pharmacogenomics_table': '',\n 'pharmacokinetics_table': '',\n 'precautions': '',\n 'precautions_table': '',\n 'pregnancy': '',\n 'pregnancy_or_breast_feeding': '',\n 'pregnancy_or_breast_feeding_table': '',\n 'pregnancy_table': '',\n 'product_type': '',\n 'purpose_table': '',\n 'questions_table': '',\n 'recent_major_changes': '',\n 'recent_major_changes_table': '',\n 'risks': '',\n 'risks_table': '',\n 'spl_medguide': '',\n 'spl_patient_package_insert': '',\n 'statement_of_identity': '',\n 'stop_use_table': '',\n 'storage_and_handling': '',\n 'storage_and_handling_table': '',\n 'substance_name': '',\n 'summary_of_safety_and_effectiveness': '',\n 'teratogenic_effects': '',\n 'use_in_specific_populations': '',\n 'use_in_specific_populations_table': '',\n 'user_safety_warnings': '',\n 'warnings': 'Warnings For external use only. When using this product avoid contact with eyes. In case of eye contact, flush eyes with water. Discontinue use if irritation or redness develops. If condition persists for more than 72 hours consult a doctor. Keep out of reach of children. If swallowed, get medical help or contact a Poison Control Center right away.',\n 'warnings_and_cautions_table': '',\n 'when_using': 'When using this product avoid contact with eyes. In case of eye contact, flush eyes with water.',\n 'when_using_table': ''}"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Imprime el consumo de GPU antes de cargar el modelo pre-entrenado","metadata":{}},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:02:05.019863Z","iopub.execute_input":"2024-09-30T20:02:05.020249Z","iopub.status.idle":"2024-09-30T20:02:05.025161Z","shell.execute_reply.started":"2024-09-30T20:02:05.020218Z","shell.execute_reply":"2024-09-30T20:02:05.024253Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"GPU memory occupied: 2105 MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntry:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T19:53:35.718488Z","iopub.execute_input":"2024-09-30T19:53:35.719349Z","iopub.status.idle":"2024-09-30T19:54:45.133287Z","shell.execute_reply.started":"2024-09-30T19:53:35.719315Z","shell.execute_reply":"2024-09-30T19:54:45.132036Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e707ca0500384a2da576a33e9d7c0960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a48b53b8af7445f496f2896cb321ceba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c982df5ee0b142c4923cdaab4798a213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03180c9e33cc4cdbb06aee3282cbf2d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac1f93dec454454882274a647f8193b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6051585c7a4ecaaa947782a505920b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8f12d0c8342429cb3ecdae076e42c47"}},"metadata":{}},{"name":"stdout","text":"self.model_name_or_path : meta-llama/Llama-2-7b-hf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a90a2781c483461fa38b9100f4d18894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f17ae7c97d5c4d82b94cc3537127a2f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9254002ec44504973fce68bf1fe997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494baa5c817342649eeca9e2e4b2ff27"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 23.2 s, sys: 21.2 s, total: 44.4 s\nWall time: 1min 9s\n","output_type":"stream"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:03:25.946412Z","iopub.execute_input":"2024-09-30T20:03:25.946790Z","iopub.status.idle":"2024-09-30T20:03:25.952222Z","shell.execute_reply.started":"2024-09-30T20:03:25.946761Z","shell.execute_reply":"2024-09-30T20:03:25.950998Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"GPU memory occupied: 2373 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 6. Prueba el modelo con inferencia Zero Shot","metadata":{}},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 26\nset_seed(seed)\nmax_tokens = 500\n\ntry:\n    prompt = dataset['train'][index]\n    \n    prompt_description = \"\"\"Generate a detailed description for the following medication: [Name, Composition, Indications, Dosage, Side Effects, Contraindications, Interactions]\n    Medication: Ibuprofen\n    \"\"\"\n\n    # Pregunta al modelo o instrucciones para preguntas y respuestas\n    prompt_question = \"Using the context of the medication Ibuprofen, answer the following question: What are the common side effects of Ibuprofen?\"\n    \n    # Mensaje del sistema\n    system_message = \"\"\"Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\n    You must follow the following structure for descriptions:\n    - Name: [Medication Name]\n    - Composition: [Active ingredients]\n    - Indications: [Uses]\n    - Dosage: [Recommended dosage]\n    - Side Effects: [Common side effects]\n    - Contraindications: [When the medication should not be used]\n    - Interactions: [Drugs or substances that interact with this medication]\n    Please use only the information available in the context provided.\"\"\"\n\n    # Template del prompt completo con instancias y sistema\n    formatted_prompt = f'''<s>[INST] <<SYS>>\n    {system_message}\n    <</SYS>>\n    {prompt_description} [/INST]</s>'''\n\n    # Template para preguntas y respuestas\n    question_template = f'''<s>[INST] <<SYS>>\n    {system_message}\n    <</SYS>>\n    {prompt_question} [/INST]</s>'''\n    \n    output = llm.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Model Generation - Zero Shot:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:02:16.116637Z","iopub.execute_input":"2024-09-30T20:02:16.117051Z","iopub.status.idle":"2024-09-30T20:02:51.170743Z","shell.execute_reply.started":"2024-09-30T20:02:16.117015Z","shell.execute_reply":"2024-09-30T20:02:51.169625Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nInput Prompt:\n<s>[INST] <<SYS>>\n    Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\n    You must follow the following structure for descriptions:\n    - Name: [Medication Name]\n    - Composition: [Active ingredients]\n    - Indications: [Uses]\n    - Dosage: [Recommended dosage]\n    - Side Effects: [Common side effects]\n    - Contraindications: [When the medication should not be used]\n    - Interactions: [Drugs or substances that interact with this medication]\n    Please use only the information available in the context provided.\n    <</SYS>>\n    Generate a detailed description for the following medication: [Name, Composition, Indications, Dosage, Side Effects, Contraindications, Interactions]\n    Medication: Ibuprofen\n     [/INST]</s>\n---------------------------------------------------------------------------------------------------\nModel Generation - Zero Shot:\n['[INST] <<SYS>>\\n    Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\\n    You must follow the following structure for descriptions:\\n    - Name: [Medication Name]\\n    - Composition: [Active ingredients]\\n    - Indications: [Uses]\\n    - Dosage: [Recommended dosage]\\n    - Side Effects: [Common side effects]\\n    - Contraindications: [When the medication should not be used]\\n    - Interactions: [Drugs or substances that interact with this medication]\\n    Please use only the information available in the context provided.\\n    <</SYS>>\\n    Generate a detailed description for the following medication: [Name, Composition, Indications, Dosage, Side Effects, Contraindications, Interactions]\\n    Medication: Ibuprofen\\n     [/INST] nobody is allowed to die!\\nnobody is allowed to die!nobody is allowed to die!nobody is allowed to die!\\nnobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!nobody is allowed to die!\\nHey there! I\\'m a photographer and filmmaker based in Philadelphia. I\\'m also a mother, a wife, a teacher, and a cat lover. I\\'m here to create and tell stories, and I hope you\\'ll come along for the ride.\\nI\\'m passionate about social justice, and I use my work to advocate for the people and causes I believe in. I\\'ve worked with many great organizations, including The Philadelphia Foundation, The Mural Arts Program, and The Philadelphia Gay Men\\'s Chorus.\\nI shoot on both digital and film, and I love working with both mediums. I shoot portraits, events, documentaries, and music videos. I\\'m also available to teach classes on filmmaking, photography, and editing.\\nI\\'ve had the pleasure of working with many talented musicians, artists, and activists. I\\'m always looking for new collaborations and new friends.\\nI\\'m also a teacher, and I\\'ve been fortunate to teach for The Philadelphia Foundation and The Philadelphia Gay Men\\'s Chorus. If you\\'re looking for someone to teach a photography class, filmmaking class, or editing class, I\\'m your gal.\\nIf you have any questions, please feel free to contact me. I\\'m always happy to chat about projects and collaborations.\\nContact me to discuss your project. I\\'d love to hear from you!\\nHi, my name is Tracy. I\\'m a photographer, filmmaker, and teacher based in Philadelphia. I\\'m also a wife, a mother, and a cat lover. I\\'m here to create and tell stories, and I hope you\\'ll come along for the ride.\\nI\\'m passionate about social justice, and I use my work to advocate']\nCPU times: user 33.6 s, sys: 301 ms, total: 33.9 s\nWall time: 35 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 7. Pre-procesando el dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntry:\n    \n    path_tokenizer = os.path.join(BASE_FOLDER, \"dataset\")\n    train_dataset_path = os.path.join(path_tokenizer, \"train_dataset\")\n    eval_dataset_path = os.path.join(path_tokenizer, \"eval_dataset\")\n    \n    if(False):\n        if not(os.path.exists(path_tokenizer)):\n            #!mkdir -p {tokenizer_path_folder}\n            os.makedirs(path_tokenizer)\n            print('Directorio para almacenar dataset creado exitosamente!')\n\n        if (os.path.exists(TOKENIZER_PATH)):\n            with zipfile.ZipFile(TOKENIZER_PATH, 'r') as zip_ref:\n                zip_ref.extractall(path_tokenizer)\n            print(f\"Tokenizador cargado desde {TOKENIZER_PATH}\")\n        else:\n            raise FileNotFoundError(f\"El tokenizador no existe en la ruta {TOKENIZER_PATH}\")\n\n    train_dataset = load_from_disk(train_dataset_path)\n    eval_dataset = load_from_disk(eval_dataset_path)\n     \n    \nexcept Exception as ex:\n    print(f\"Error [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n\nFileLink(f'./{PROJECT_NAME}/dataset/{name_zip_tokenizer}')","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    \n    path_tokenizer = os.path.join(BASE_FOLDER, \"dataset\")\n    path_save_tokenizer = os.path.join(path_tokenizer, name_zip_tokenizer)\n    #path_load_tokenizer = os.path.join(DATASET_FOLDER, name_zip_tokenizer)\n    \n    train_dataset_path = os.path.join(path_tokenizer, \"train_dataset\")\n    eval_dataset_path = os.path.join(path_tokenizer, \"eval_dataset\")\n    \n    if not(os.path.exists(path_tokenizer)):\n        #!mkdir -p {tokenizer_path_folder}\n        os.makedirs(path_tokenizer)\n        print('Directorio para almacenar dataset creado exitosamente!')\n    \n     \n    if (LOAD_TOKENIZER):\n        if (os.path.exists(TOKENIZER_PATH)):\n            with zipfile.ZipFile(TOKENIZER_PATH, 'r') as zip_ref:\n                zip_ref.extractall(path_tokenizer)\n            print(f\"Tokenizador cargado desde {TOKENIZER_PATH}\")\n        else:\n            raise FileNotFoundError(f\"El tokenizador no existe en la ruta {TOKENIZER_PATH}\")\n        \n        train_dataset = load_dataset(train_dataset_path)\n        eval_dataset = load_dataset(eval_dataset_path)\n        \n    else:\n        if not(PROCESS_SAMPLE):\n            max_length = llm.get_max_length()\n            print('Inicia pre-procesamiento')\n            train_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                               max_length=max_length,\n                                               seed=seed,\n                                               dataset=dataset['train']\n                                              )\n\n            eval_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                              max_length=max_length,\n                                              seed=seed,\n                                              dataset=dataset['validation']\n                                             )\n        else:\n            train_dataset = preprocess_dataset_sample(seed=seed,\n                                                      dataset=dataset['train']\n                                                     )\n\n            eval_dataset = preprocess_dataset_sample(seed=seed,\n                                                     dataset=dataset['validation']\n                                                     )\n            \n        \n        \n        if(SAVE_TOKENIZER):\n            # save in disk\n            train_dataset.save_to_disk(train_dataset_path)\n            eval_dataset.save_to_disk(eval_dataset_path)\n            \n            file_path = None\n            \n            if (os.path.exists(path_save_tokenizer)):\n                os.remove(path_save_tokenizer)  # Eliminar el archivo existente\n                print(f'Archivo zip existente, eliminado desde la ruta : {path_tokenizer}')\n                \n            with zipfile.ZipFile(path_save_tokenizer, 'w') as zipf:\n                for folder, subfolders, files in os.walk(path_tokenizer):\n                    for file in files:\n                        file_path = os.path.join(folder, file)\n                        zipf.write(file_path, os.path.relpath(file_path, path_tokenizer))\n            \n            if(os.path.exists(path_save_tokenizer)):\n                print('Process file zip tokenizer...')\nexcept Exception as ex:\n    print(f\"Error [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n\nFileLink(f'./{PROJECT_NAME}/dataset/{name_zip_tokenizer}')","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:12:58.610124Z","iopub.execute_input":"2024-09-30T20:12:58.610897Z","iopub.status.idle":"2024-09-30T20:17:57.230520Z","shell.execute_reply.started":"2024-09-30T20:12:58.610865Z","shell.execute_reply":"2024-09-30T20:17:57.229488Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Found max length: 4096\nInicia pre-procesamiento\nPreprocessing dataset...\nCreate prompt llama2...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe068b2663842a58e69f08a5d9d7df6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f967b247e54fe1909719fbe3c25a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/7000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742aa13752404fb89614f23754bfe60a"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\nCreate prompt llama2...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884f9688593d43e5b213b026f575e7f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6060c4245ede401e93f1d7fc35c67a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcc398e91e246f6a0fdd46d93bafd99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/5039 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1b6f4b807541ae8b26b5b24cffdd6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1051 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e9540d076e4429a80561c3e312d64d"}},"metadata":{}},{"name":"stdout","text":"Archivo zip existente, eliminado desde la ruta : /kaggle/working/drugs-generative/dataset\nProcess file zip tokenizer...\nCPU times: user 4min 49s, sys: 3.45 s, total: 4min 53s\nWall time: 4min 58s\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/drugs-generative/dataset/tokenizer.zip","text/html":"<a href='./drugs-generative/dataset/tokenizer.zip' target='_blank'>./drugs-generative/dataset/tokenizer.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:18:32.460577Z","iopub.execute_input":"2024-09-30T20:18:32.461236Z","iopub.status.idle":"2024-09-30T20:18:32.466554Z","shell.execute_reply.started":"2024-09-30T20:18:32.461202Z","shell.execute_reply":"2024-09-30T20:18:32.465567Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (5039, 3)\nValidation: (1051, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 5039\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 8. Configura el modelo PEFT/LoRA para el Fine-Tuning\nAhora, vamos a realizar un ajuste fino eficiente en parámetros (PEFT). PEFT es una forma de ajuste fino por instrucciones que es mucho más eficiente que el ajuste fino completo. PEFT es un término genérico que incluye Adaptación de Bajo Rango (LoRA) y ajuste por indicaciones (¡que NO ES LO MISMO que la ingeniería de prompts!). En la mayoría de los casos, cuando alguien menciona PEFT, generalmente se refieren a LoRA. LoRA, en esencia, permite un ajuste fino eficiente del modelo utilizando menos recursos computacionales, a menudo realizable con solo una GPU. Después del ajuste fino con LoRA para una tarea o caso de uso específico, el resultado es un LLM original sin cambios y la aparición de un \"adaptador LoRA\" considerablemente más pequeño, que a menudo representa un porcentaje de un solo dígito del tamaño del LLM original (en MBs en lugar de GBs).\n\nDurante la inferencia, el adaptador LoRA debe combinarse con su LLM original. La ventaja radica en la capacidad de muchos adaptadores LoRA para reutilizar el LLM original, reduciendo así los requisitos generales de memoria cuando se manejan múltiples tareas y casos de uso.\n\nNota el hiperparámetro de rango (r), que define el rango/dimensión del adaptador a ser entrenado. r es el rango de la matriz de bajo rango utilizada en los adaptadores, lo que controla el número de parámetros entrenados. Un rango mayor permitirá mayor expresividad, pero hay una compensación en términos de cómputo.\n\nalpha es el factor de escalado para los pesos aprendidos. La matriz de pesos se escala por alpha/r, y por lo tanto, un valor más alto de alpha asigna más peso a las activaciones de LoRA.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(llm.model))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:05:29.538380Z","iopub.execute_input":"2024-09-30T20:05:29.538742Z","iopub.status.idle":"2024-09-30T20:05:29.546507Z","shell.execute_reply.started":"2024-09-30T20:05:29.538712Z","shell.execute_reply":"2024-09-30T20:05:29.545392Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"all model parameters: 3500412928\ntrainable model parameters: 262410240\npercentage of trainable model parameters: 7.50%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(llm.model)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:05:31.694336Z","iopub.execute_input":"2024-09-30T20:05:31.694723Z","iopub.status.idle":"2024-09-30T20:05:31.703384Z","shell.execute_reply.started":"2024-09-30T20:05:31.694691Z","shell.execute_reply":"2024-09-30T20:05:31.702408Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_config = LoraConfig(lora_alpha=lora_alpha, #16,\n                         lora_dropout=lora_dropout, #0.1,\n                         r=lora_r, #64,\n                         bias=\"none\",\n                         task_type=\"CAUSAL_LM\",\n                         target_modules=['q_proj','k_proj','v_proj','o_proj'], #dense\n                        )\n\n\n# 2 - Utilizando el método prepare_model_for_kbit_training de PEFT.\nllm.model = prepare_model_for_kbit_training(llm.model)\npeft_model = get_peft_model(llm.model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:05:35.852086Z","iopub.execute_input":"2024-09-30T20:05:35.852493Z","iopub.status.idle":"2024-09-30T20:05:37.436528Z","shell.execute_reply.started":"2024-09-30T20:05:35.852459Z","shell.execute_reply":"2024-09-30T20:05:37.435675Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Una vez que todo esté configurado y el modelo base esté preparado, podemos utilizar la función auxiliar print_trainable_parameters() para ver cuántos parámetros entrenables hay en el modelo.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:05:40.364267Z","iopub.execute_input":"2024-09-30T20:05:40.364639Z","iopub.status.idle":"2024-09-30T20:05:40.377831Z","shell.execute_reply.started":"2024-09-30T20:05:40.364611Z","shell.execute_reply":"2024-09-30T20:05:40.376954Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"all model parameters: 3567521792\ntrainable model parameters: 67108864\npercentage of trainable model parameters: 1.88%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Observa cómo se ve diferente el modelo ahora, con los adaptadores LoRA añadidos:\nprint(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:05:43.923784Z","iopub.execute_input":"2024-09-30T20:05:43.924208Z","iopub.status.idle":"2024-09-30T20:05:43.942710Z","shell.execute_reply.started":"2024-09-30T20:05:43.924173Z","shell.execute_reply":"2024-09-30T20:05:43.941645Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 9. Entrenando el Adaptador PEFT\n\nDefine los argumentos de entrenamiento y crea una instancia de Trainer.","metadata":{}},{"cell_type":"code","source":"tensorboard_callback = TensorBoardCallback()\ntb_process, ngrok_process = launch_tensorboard()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:21:16.620415Z","iopub.execute_input":"2024-09-30T20:21:16.620960Z","iopub.status.idle":"2024-09-30T20:21:16.629679Z","shell.execute_reply.started":"2024-09-30T20:21:16.620929Z","shell.execute_reply":"2024-09-30T20:21:16.628795Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"TensorBoard URL: https://4027-34-168-146-28.ngrok-free.app\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import amp\n\n'''\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n'''\n'''\npeft_training_args = TrainingArguments(\n    output_dir = LOG_TRAIN_PATH,\n    do_eval=True,\n    eval_strategy=\"steps\",\n    fp16=False,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    learning_rate=2.0e-04,\n    logging_steps=25,\n    log_level=\"info\",\n    logging_strategy=\"steps\",\n    lr_scheduler_type=\"cosine\",\n    max_steps=1000,\n    #num_train_epochs=1,\n    overwrite_output_dir = True,\n    per_device_eval_batch_size=1,\n    per_device_train_batch_size=1,\n    save_strategy=\"steps\",\n    eval_steps=25,\n    group_by_length=True,\n    logging_dir=LOG_PATH,\n    optim=\"paged_adamw_8bit\",\n    save_steps=25,\n    warmup_steps=50,\n    save_total_limit=None,\n    seed=42,\n    report_to=\"tensorboard\",\n)\n\npeft_trainer = transformers.Trainer(model=peft_model,\n                                    train_dataset=train_dataset,\n                                    eval_dataset=eval_dataset,\n                                    args=peft_training_args,\n                                    data_collator=transformers.DataCollatorForLanguageModeling(llm.tokenizer, mlm=False),\n                                    )\n'''\n\nllm.model.train()\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    #group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    seed=42,\n    eval_strategy=eval_strategy,\n    eval_steps=eval_steps,\n    load_best_model_at_end=True,\n    report_to=\"tensorboard\"\n)\n\n# Configurar early stopping\nearly_stopping = EarlyStoppingCallback(\n    early_stopping_patience=3,  # Número de evaluaciones sin mejora antes de detener el entrenamiento\n    early_stopping_threshold=0.001  # Mejora mínima requerida para continuar el entrenamiento\n)\n\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=llm.model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    #dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=llm.tokenizer,\n    args=training_arguments,\n    #compute_metrics=compute_metrics,\n    callbacks=[early_stopping],\n    packing=packing,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:21:26.202317Z","iopub.execute_input":"2024-09-30T20:21:26.202961Z","iopub.status.idle":"2024-09-30T20:21:27.462823Z","shell.execute_reply.started":"2024-09-30T20:21:26.202928Z","shell.execute_reply":"2024-09-30T20:21:27.461902Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#training_args.device\ntorch.cuda.empty_cache()\nprint(f\"GPUs disponibles: {torch.cuda.device_count()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:21:36.869661Z","iopub.execute_input":"2024-09-30T20:21:36.870528Z","iopub.status.idle":"2024-09-30T20:21:36.875604Z","shell.execute_reply.started":"2024-09-30T20:21:36.870492Z","shell.execute_reply":"2024-09-30T20:21:36.874537Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"GPUs disponibles: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-30T20:21:39.362419Z","iopub.execute_input":"2024-09-30T20:21:39.362785Z","iopub.status.idle":"2024-10-01T01:43:54.136256Z","shell.execute_reply.started":"2024-09-30T20:21:39.362756Z","shell.execute_reply":"2024-10-01T01:43:54.133719Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='326' max='50390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  326/50390 5:16:48 < 815:53:51, 0.02 it/s, Epoch 0.06/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>0.711500</td>\n      <td>0.350900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.350200</td>\n      <td>0.315240</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.334500</td>\n      <td>0.302872</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.292200</td>\n      <td>0.296332</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.263500</td>\n      <td>0.288717</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.276200</td>\n      <td>0.282655</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.271400</td>\n      <td>0.280138</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.236700</td>\n      <td>0.275439</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.325100</td>\n      <td>0.276044</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.245500</td>\n      <td>0.271233</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.281800</td>\n      <td>0.268630</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.282500</td>\n      <td>0.266571</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='230' max='1051' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 230/1051 05:04 < 18:11, 0.75 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66fb2129-61409eee50b70b8855c1a6b0;fd008a1e-0dc7-49a8-a6f2-348da3e53006)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66fb3992-73e8f3a300f46bad518d74b2;5349a4ef-9c0d-46e4-8f8a-02f870473e45)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66fb521f-64d01d6250e6cdf11b0a4e5d;79dc785c-bd3a-4a4e-bf3f-4c3bce2c01e5)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:156\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2467\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2915\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2913\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2918\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2872\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2872\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2875\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:4051\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4048\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4050\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4051\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4052\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4053\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:559\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    561\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# This call is inside the try-block since is_npu_available is not supported by torch.compile.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:816\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:816\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Liberar memoria para la fusión de pesos\n\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cualitativamente (Evaluación Humana)","metadata":{}},{"cell_type":"code","source":"try:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nft_model = PeftModel.from_pretrained(llm.model, \n                                     \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",\n                                     torch_dtype=torch.float16,\n                                     is_trainable=False\n                                    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 120\nset_seed(seed)\nmax_tokens = 512\n\ntry:\n    prompt = dataset['train'][index]\n\n    # Instrucción: Resume la siguiente conversación\n    formatted_prompt = f'Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\\n Provide a detailed description of the medication {prompt[\"generic_name\"]} using the available data.\\n Output:\\n'\n    res = ft_model.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n    output = res[0].split('Output:\\n')[1]\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Peft Model Generation:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cuantitativamente (con la Metrica ROUGE)","metadata":{}},{"cell_type":"code","source":"def data_process(dataset):\n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n\n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n\n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n\n        dataset = dataset.map(create_prompt_formats_v1,\n                              num_proc=num_proc\n                             )#, batched=True)\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    train_dataset=data_process(dataset['train'])\n    eval_dataset=data_process(dataset['validation'])\n\n    print(f\"Shapes of the datasets:\")\n    print(f\"Training: {train_dataset.shape}\")\n    print(f\"Validation: {eval_dataset.shape}\")\n    \nexcept Exception as ex:\n    print(f'Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_dataset[120])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset[120]","metadata":{},"execution_count":null,"outputs":[]}]}