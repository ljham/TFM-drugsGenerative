{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9501867,"sourceType":"datasetVersion","datasetId":5782792}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**En este notebook y tutorial, realizaremos un fine-tune [Llama-8k](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) modelo relativamente pequeño de 7 mil millones de parametros - que ha 'demostrado un rendimiento casi de última generación entre los modelos con menos de 13 mil millones de parámetros' - *en tus propios datos!!***\n\n**Aqui usaremos [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), una técnica de fine-tunning altamente eficiente que consiste en cuantizar un LLM preentrenado a solo 4 bits y agregar pequeños 'Adaptadores de Bajo Rango'. Este enfoque único permite realizar el fine-tunning de LLMs utilizando solo una GPU. Esta técnica está respaldada por el/la... [PEFT library](https://huggingface.co/docs/peft/index).**","metadata":{}},{"cell_type":"markdown","source":"# Tabla de Contenido","metadata":{}},{"cell_type":"markdown","source":"- [1- Instalar librerias requeridas](#1)\n- [ 2 - Cargar dataset](#2)\n- [ 3 - Crear configuración de bitsandbytes](#3)\n- [ 4 - Cargar Modelo Base](#4)\n- [ 5 - Tokenizar](#5)\n- [ 6 - Testear el modelo con Zero Shot Inferencing](#6)\n- [ 7 - Pre-procesando el dataset](#7)\n- [ 8 - Configurar el modelo PEFT/LoRA para realizar Fine-Tuning](#8)\n- [ 9 - Entrenar Adaptador PEFT](#9)\n- [ 10 - Evaluar el Modelo Qualitativamente (Evaluacion Humana)](#10)\n- [ 11 - Evaluar el Modelo Quantitaviamente (con Metrica ROUGE)](#11)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n#### 1. Instalar librerias requeridas","metadata":{}},{"cell_type":"code","source":"%%time\n!pip install -U transformers\n!pip install -U bitsandbytes\n!pip install -U peft\n!pip install -U accelerate\n!pip install -U datasets\n!pip install -U scipy\n!pip install -U einops\n!pip install -U evaluate\n!pip install -U trl\n!pip install -U rouge_score\n!pip install -U torch","metadata":{"_uuid":"c34dd264-0a8e-4fae-8f88-83c906c39c3c","_cell_guid":"59a8cbf9-4057-4756-a512-100f15222529","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport os\nimport shutil\nimport zipfile\nimport gc\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport multiprocessing\nimport psutil\nimport requests\nimport tarfile\nimport json\nimport evaluate\nimport datetime, os\n\nfrom trl import SFTTrainer\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom transformers.integrations import TensorBoardCallback\nfrom tqdm import tqdm\nfrom huggingface_hub import interpreter_login\nfrom pynvml import *\nfrom functools import partial\nfrom transformers import set_seed\nfrom datasets import load_dataset, DatasetDict\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom IPython.display import FileLink\nfrom urllib.request import urlopen\nfrom io import BytesIO\nfrom subprocess import Popen\nfrom os import chmod\nfrom os.path import isfile\n\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:53:27.419033Z","iopub.execute_input":"2024-09-29T13:53:27.419656Z","iopub.status.idle":"2024-09-29T13:53:47.965002Z","shell.execute_reply.started":"2024-09-29T13:53:27.419620Z","shell.execute_reply":"2024-09-29T13:53:47.964050Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2024-09-29 13:53:32.497450: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-29 13:53:32.497513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-29 13:53:32.499006: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n    Setting a new token will erase the existing one.\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ·····································\nAdd token as git credential? (Y/n)  n\n"},{"name":"stdout","text":"Token is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nCPU times: user 8.04 s, sys: 803 ms, total: 8.84 s\nWall time: 20.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"# Habilitar los permisos necesarios para acceder a google-drive\nimport os\n\nPROJECT_NAME = 'drugs-generative'\n\ntry:\n    from google.colab import drive\n    ROOT = '/content/drive/'\n    drive.mount(ROOT, force_remount=True)\n    IN_COLAB = True\n    BASE_FOLDER = ROOT + 'MyDrive/' + PROJECT_NAME\n    DATASET_FOLDER = BASE_FOLDER\nexcept:\n    #ROOT = '/kaggle/input/drugs-data'\n    ROOT = '/kaggle'\n    IN_COLAB = False\n    BASE_FOLDER = os.path.join(\"/kaggle/working\", PROJECT_NAME)\n    DATASET_FOLDER = os.path.join(\"/kaggle/input\", PROJECT_NAME)\n    TOKENIZER_FOLDER = os.path.join(\"/kaggle/input\", 'drugs-tokenizer')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:53:51.389637Z","iopub.execute_input":"2024-09-29T13:53:51.390674Z","iopub.status.idle":"2024-09-29T13:53:51.396873Z","shell.execute_reply.started":"2024-09-29T13:53:51.390640Z","shell.execute_reply":"2024-09-29T13:53:51.395940Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\nUSE_ALL_DATASET = False\nNUMBER_ELEMENT = 100\nSAVE_TOKENIZER = True\nLOAD_TOKENIZER = False\nPROCESS_SAMPLE = False\nNGROK_TOKEN = '2mfZzvcUfXHZqEB2Cc3REgZQ3eG_8a2WJJCc9vp9UpVV3AFVT'\nHUGGING_TOKEN = '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'\n\n#model_name='meta-llama/Meta-Llama-3-8B'\nmodel_name = 'meta-llama/Llama-2-7b-hf'\nname_zip_tokenizer = 'tokenizer.zip'\nlog_name_directory = 'logs'\nngrok_url = 'https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz'\ntrain_dataset = None\neval_dataset = None\nseed = 42\n\nDATASET_PATH = os.path.join(DATASET_FOLDER, \"drugs_data.parquet\")\nTOKENIZER_PATH = os.path.join(TOKENIZER_FOLDER, \"tokenizer.zip\")\nLOG_TRAIN_PATH = os.path.join(BASE_FOLDER, log_name_directory)\n\n# Valida directorio principal del proyecto\nif not(os.path.exists(BASE_FOLDER)):\n    !mkdir -p {BASE_FOLDER}\n    print('Directorio proyecto creado exitosamente!!')\n\n    \n# Valida directorio en donde se almacenan los logs del entrenamiento\nif not(os.path.exists(LOG_TRAIN_PATH)):\n    !mkdir -p {LOG_TRAIN_PATH}\n    print('Directorio para almacenar logs creado exitosamente!!')    \n    \n# Valida descarga dataset del Proyecto\nif not (os.path.exists(DATASET_PATH)):\n    print('Dataset no existe!!')\n\n\n################################################################################\n# Model parameters\npadding_side = \"right\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = LOG_TRAIN_PATH\n\n# Number of training epochs\nnum_train_epochs = 35\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 100\n\n# Log every X updates steps\nlogging_steps = 25\n\nevaluation_strategy = \"steps\"\n\neval_steps = 100\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\n# device_map = {\"\": 0}\ndevice_map = \"auto\"\n    \n################################################################################\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        bf16 = True\n        print(\"=\" * 80)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:07.268301Z","iopub.execute_input":"2024-09-29T13:54:07.269015Z","iopub.status.idle":"2024-09-29T13:54:07.390079Z","shell.execute_reply.started":"2024-09-29T13:54:07.268978Z","shell.execute_reply":"2024-09-29T13:54:07.389252Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n#### 2. Definición de Funciones ","metadata":{}},{"cell_type":"code","source":"# Funcion para imprimir la utilización de la memoria de la GPU\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\n# Función para reemplazar NaN con cadena vacía\ndef replace_nan_with_empty_string(example):\n    for key, value in example.items():\n        if value is None or pd.isna(value) or (value == 'nan'):\n            example[key] = ''\n    return example\n\n\ndef create_prompt_formats_llama3(sample):\n    '''\n    \n    '''\n    #===========================================================================================\n    try:\n        # Construir las partes iniciales\n        instruct_key = '### Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\"'\n        context_key = '### Context: You are a pharmaceutical chemist specialized in the in-depth understanding of drug descriptions. Your task is to generate a professional and accurate response based on the information provided. If a specific field lacks information, state \"No specific information available\" instead of providing unconfirmed details.'\n        input_key = f\"### Input: Provide a detailed description of the medication {sample.get('generic_name', '')} using the available data.\"\n        end_key = \"### End\"\n\n        # Lista de campos a procesar\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                drugs.append(f'<{field}> {label_name}: {field_value} </{field}>')\n                if question_text:\n                    questions.append(f'<question> {question_text}</question><answer> {field_value}</answer>')\n\n        # Construir las partes finales\n        output_key = f\"### Output: {sample.get('description', '')}\"\n        if drugs:\n            output_key += \"\\n\" + \"\\n\".join(drugs)\n\n        question_key = '### Questions: ' + (\"\\n\".join(questions) if questions else \"\")\n\n        # Construir el texto final\n        parts = [instruct_key, context_key, input_key, output_key, question_key, end_key]\n        sample[\"text\"] = \"\\n\\n\".join(parts)\n\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    return sample\n\ndef create_prompt_formats_llama2(sample):\n    '''\n\n    '''\n    #===========================================================================================\n    try:\n        # Lista de campos a procesar: campo en el dataset, nombre a mostrar, pregunta asociada\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos y construir las secciones de descripción y preguntas/respuestas\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                # Añadir al bloque de descripción del medicamento en formato simple\n                drugs.append(f'{label_name}: {field_value}')\n                # Si hay una pregunta asociada al campo, añadirla también\n                if question_text:\n                    questions.append(f'Question: {question_text}\\nAnswer: {field_value}')\n\n        # Mensaje del sistema con el prompt mejorado\n        system_message = f\"\"\"You are a helpful Medical Assistant. Your task is to generate descriptions of medications or respond to questions related to them, depending on the user's request.\n\n        If the user requests a **medication description**, follow this structure:\n        - Brand Name: [Brand Name]\n        - Generic Name: [Generic Name]\n        - Active Ingredient: [Active ingredients]\n        - Indications: [Uses]\n        - Dosage: [Recommended dosage]\n        - Side Effects: [Common side effects]\n        - Contraindications: [When the medication should not be used]\n        - Interactions: [Drugs or substances that interact with this medication]\n\n        If the user asks a **direct question about the medication**, answer based on the provided information and the medication's context.\n\n        Please use only the available information in the provided context.\n\n        Guidelines:\n        - Maintain a professional, precise, and concise tone in all responses.\n        - Do not fabricate information. If a field lacks data, state \"No specific information available.\"\n        - Ensure the information is understandable for both healthcare professionals and patients.\n        \"\"\"\n\n        # Construir la descripción y preguntas de manera directa sin etiquetas adicionales\n        description = \"\\n\".join(drugs)\n\n        # Agregar las preguntas si hay\n        if questions:\n            questions_block = \"\\n\\n\".join(questions)\n        else:\n            questions_block = \"No additional questions provided.\"\n\n        # Crear el prompt completo combinando todo dentro de [INST]\n        sample['text'] = f\"\"\"<s>[INST] <<SYS>>\n        {system_message}\n        <</SYS>>\n        {description}\n\n        {questions_block}\n\n        [/INST]</s>\"\"\"\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return sample\n    \n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )\n\n    \ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        create_prompt_formats = None\n        if(model_name == 'meta-llama/Meta-Llama-3-8B'):\n            print(\"Create prompt llama3...\")\n            create_prompt_formats = create_prompt_formats_llama3\n        elif(model_name == 'meta-llama/Llama-2-7b-hf'):\n            print(\"Create prompt llama2...\")\n            create_prompt_formats = create_prompt_formats_llama2\n        \n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        _preprocessing_function = partial(preprocess_batch,\n                                          max_length = max_length,\n                                          tokenizer = tokenizer\n                                         )\n\n        dataset = dataset.map(_preprocessing_function, \n                              remove_columns=[col for col in dataset.column_names if col != \"text\"],\n                              #num_proc=num_proc\n                             )\n\n        # Filtrar las muestras que tienen input_ids que exceden la longitud máxima (max_length).\n        dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef preprocess_dataset_sample(seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        create_prompt_formats = None\n        if(model_name == 'meta-llama/Meta-Llama-3-8B'):\n            print(\"Create prompt llama3...\")\n            create_prompt_formats = create_prompt_formats_llama3\n        elif(model_name == 'meta-llama/Llama-2-7b-hf'):\n            print(\"Create prompt llama2...\")\n            create_prompt_formats = create_prompt_formats_llama2\n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef print_number_of_trainable_model_parameters(model):\n    try:\n        trainable_model_params = 0\n        all_model_params = 0\n        for _, param in model.named_parameters():\n            all_model_params += param.numel()\n            if param.requires_grad:\n                trainable_model_params += param.numel()\n        return f\"all model parameters: {all_model_params}\\ntrainable model parameters: {trainable_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n    except Exception as ex:\n        print(f'Ocurrió un error inesperado al imprimir los parametros del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n\ndef launch_tensorboard():\n    tb_process, ngrok_process = None, None\n    \n    # Launch TensorBoard\n    if not is_process_running('tensorboard'):\n        tb_command = f'tensorboard --logdir {LOG_TRAIN_PATH}/runs/ --host 0.0.0.0 --port 6006'\n        tb_process = run_cmd_async_unsafe(tb_command)\n    \n    # Install ngrok\n    if not isfile('./ngrok'):\n        #ngrok_url = 'https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\n        print('Inicia descarga de ngrok....')\n        download_and_extract(ngrok_url)\n        chmod('./ngrok', 0o755)\n        \n        #Registra token de autorizacion\n        tb_command = f'./ngrok config add-authtoken {NGROK_TOKEN}'\n        tb_process = run_cmd_async_unsafe(tb_command)\n\n    # Create ngrok tunnel and print its public URL\n    if not is_process_running('ngrok'):\n        ngrok_process = run_cmd_async_unsafe('./ngrok http 6006')\n        time.sleep(1) # Waiting for ngrok to start the tunnel\n    \n    ngrok_api_res = urlopen('http://127.0.0.1:4040/api/tunnels', timeout=10)\n    ngrok_api_res = json.load(ngrok_api_res)\n    assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n    tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n    print(f'TensorBoard URL: {tb_public_url}')\n\n    return tb_process, ngrok_process\n\n\ndef download_and_extract(url, extract_to='.'):\n    try:\n        # Descargar el archivo\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Lanza una excepción si la respuesta tiene un error\n\n        # Detectar el tipo de archivo a partir de la URL\n        if url.endswith('.zip'):\n            # Si es un archivo ZIP, utilizar ZipFile\n            with ZipFile(BytesIO(response.content)) as zip_file:\n                zip_file.extractall(path=extract_to)\n                print(f'Archivo ZIP extraído en: {os.path.abspath(extract_to)}')\n\n        elif url.endswith('.tgz') or url.endswith('.tar.gz'):\n            # Si es un archivo .tgz o .tar.gz, utilizar tarfile\n            with tarfile.open(fileobj=BytesIO(response.content), mode='r:gz') as tar_file:\n                tar_file.extractall(path=extract_to)\n                print(f'Archivo TGZ extraído en: {os.path.abspath(extract_to)}')\n\n        else:\n            print(\"Formato de archivo no soportado.\")\n            return\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error en la descarga: {e}\")\n    except Exception as e:\n        print(f\"Ocurrió un error inesperado: {e}, {e.__traceback__.tb_lineno}\")\n\n\ndef run_cmd_async_unsafe(cmd):\n    return Popen(cmd, shell=True)\n\n\ndef is_process_running(process_name):\n    running_process_names = (proc.name() for proc in psutil.process_iter())\n    return process_name in running_process_names\n        \n\ndef compute_perplexity(eval_pred):\n    logits, labels = eval_pred\n    loss = eval_pred.loss\n    perplexity = math.exp(loss)\n    return {\"perplexity\": perplexity}\n\n\ndef compute_rouge(eval_pred):\n    rouge_metric = evaluate.load_metric(\"rouge\")\n    predictions, labels = eval_pred\n    predictions = [pred.strip() for pred in predictions]\n    references = [label.strip() for label in labels]\n    rouge_scores = rouge_metric.compute(predictions=predictions, references=references)\n    return {\n        \"rouge1\": rouge_scores[\"rouge1\"].mid.fmeasure,\n        \"rouge2\": rouge_scores[\"rouge2\"].mid.fmeasure,\n        \"rougeL\": rouge_scores[\"rougeL\"].mid.fmeasure\n    }\n\n\ndef compute_meteor(eval_pred):\n    meteor_metric = evaluate.load_metric(\"meteor\")\n    predictions, labels = eval_pred\n    predictions = [pred.strip() for pred in predictions]\n    references = [label.strip() for label in labels]\n    meteor_score = meteor_metric.compute(predictions=predictions, references=references)\n    return {\"meteor\": meteor_score[\"meteor\"]}\n\n\ndef compute_exact_match(eval_pred):\n    predictions, labels = eval_pred\n    exact_matches = sum([1 if pred == ref else 0 for pred, ref in zip(predictions, labels)]) / len(labels)\n    return {\"exact_match\": exact_matches}\n\n\n# Definir las métricas de evaluación\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = [logit.argmax(dim=-1) for logit in logits]\n\n    # Calcular todas las métricas\n    perplexity = compute_perplexity((logits, labels))[\"perplexity\"]\n    rouge = compute_rouge((predictions, labels))\n    meteor = compute_meteor((predictions, labels))[\"meteor\"]\n    exact_match = compute_exact_match((predictions, labels))[\"exact_match\"]\n\n    # Guardar métricas en un archivo CSV\n    with open('metrics.csv', mode='a') as metrics_file:\n        metrics_writer = csv.writer(metrics_file)\n        metrics_writer.writerow([perplexity, rouge[\"rouge1\"], rouge[\"rouge2\"], rouge[\"rougeL\"], meteor, exact_match])\n\n    # Retornar todas las métricas\n    return {\n        \"perplexity\": perplexity,\n        \"rouge1\": rouge[\"rouge1\"],\n        \"rouge2\": rouge[\"rouge2\"],\n        \"rougeL\": rouge[\"rougeL\"],\n        \"meteor\": meteor,\n        \"exact_match\": exact_match\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:18.925289Z","iopub.execute_input":"2024-09-29T13:54:18.925678Z","iopub.status.idle":"2024-09-29T13:54:18.981095Z","shell.execute_reply.started":"2024-09-29T13:54:18.925649Z","shell.execute_reply":"2024-09-29T13:54:18.980075Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class ModelAnalizer:\n    '''\n    '''\n    \n    def __init__(self, model_name_or_path):\n        self.model_name_or_path = model_name_or_path\n        self.model = None\n        self.tokenizer = None\n        self._load_qtz_config()\n    \n    \n    def _load_qtz_config(self):\n        try:\n            #compute_dtype = getattr(torch, \"float16\")\n            \n            # Load tokenizer and model with QLoRA configuration\n            self.bnb_config = BitsAndBytesConfig(load_in_4bit=use_4bit, #True,\n                                                 bnb_4bit_quant_type=bnb_4bit_quant_type, #'nf4',\n                                                 bnb_4bit_compute_dtype=compute_dtype,\n                                                 bnb_4bit_use_double_quant=use_nested_quant #False,\n                                                )\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar quantization-config [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n        \n    def _load_model(self):\n        try:\n            #device_map = {\"\": 0}\n            self.model = AutoModelForCausalLM.from_pretrained(self.model_name_or_path, \n                                                              device_map=device_map,\n                                                              quantization_config=self.bnb_config,\n                                                              trust_remote_code=True,\n                                                              token=HUGGING_TOKEN\n                                                              )\n            self.model.config.use_cache = False\n            self.model.config.pretraining_tp = 1\n            # Carga el tokenizador\n            self._tokenizer()\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n            \n    \n    def _tokenizer(self):\n        # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n        try:\n            print(f'self.model_name_or_path : {self.model_name_or_path}')\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, \n                                                          trust_remote_code=True, \n                                                          add_bos_token=True,\n                                                          use_fast=False, \n                                                          add_eos_token=True, \n                                                          padding_side=padding_side, #\"left\",\n                                                          token=HUGGING_TOKEN\n                                                         )\n            if not(self.tokenizer):\n                raise Exception(f'No se ha definido el atributo self.tokenizer')\n            \n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el tokenizador [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    \n    \n    def gen(self, prompt, maxlen=512, sample=True):\n        try:\n            '''\n            eval_tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path,\n                                                           trust_remote_code=True,\n                                                           add_bos_token=True,\n                                                           use_fast=False\n                                                          )\n            eval_tokenizer.pad_token = eval_tokenizer.eos_token\n            \n            toks = eval_tokenizer(p, return_tensors=\"pt\")\n            '''\n            \n            toks = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n            res = self.model.generate(**toks.to(\"cuda\"), \n                                      max_new_tokens=maxlen,\n                                      do_sample=sample,\n                                      num_return_sequences=1,\n                                      temperature=0.7,\n                                      num_beams=1,\n                                      top_p=0.95\n                                     ).to('cpu')\n            return self.tokenizer.batch_decode(res, skip_special_tokens=True)\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al procesar la inferencia en el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n    \n    def get_max_length(self):\n        try:\n            max_length = None\n            for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n                max_length = getattr(self.model.config, length_setting, None)\n                if max_length:\n                    print(f\"Found max length: {max_length}\")\n                    break\n            if not max_length:\n                max_length = 1024\n                print(f\"Using default max length: {max_length}\")\n            return max_length\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al obtener tamaño del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:25.675220Z","iopub.execute_input":"2024-09-29T13:54:25.675606Z","iopub.status.idle":"2024-09-29T13:54:25.692174Z","shell.execute_reply.started":"2024-09-29T13:54:25.675578Z","shell.execute_reply":"2024-09-29T13:54:25.691238Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<a name='3'></a>\n#### 3. Cargar el dataset","metadata":{}},{"cell_type":"code","source":"%%time\n#Cargar tu dataset\ndataset = load_dataset('parquet', data_files=DATASET_PATH)\n\n\n# Tomar una muestra aleatoria de x cantidad de registros de forma aleatoria)\nif not(USE_ALL_DATASET):\n    sampled_dataset = dataset['train'].shuffle(seed=42).select(range(NUMBER_ELEMENT))\nelse:\n    sampled_dataset = dataset['train']\n\n# Dividir en 70% train y 30% (test + validation)\ntrain_test_valid = sampled_dataset.train_test_split(test_size=0.3, seed=42)\n\n# Dividir el 30% restante en 15% test y 15% validation\ntest_valid = train_test_valid['test'].train_test_split(test_size=0.5, seed=42)\n\n# Reunir los conjuntos en un DatasetDict\ndataset = DatasetDict({\n    'train': train_test_valid['train'],\n    'test': test_valid['test'],\n    'validation': test_valid['train']\n})\n\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:32.036957Z","iopub.execute_input":"2024-09-29T13:54:32.037576Z","iopub.status.idle":"2024-09-29T13:54:32.740717Z","shell.execute_reply.started":"2024-09-29T13:54:32.037543Z","shell.execute_reply":"2024-09-29T13:54:32.739754Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"CPU times: user 212 ms, sys: 51.4 ms, total: 263 ms\nWall time: 693 ms\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 70\n    })\n    test: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 15\n    })\n    validation: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 15\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][26]","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:43.654172Z","iopub.execute_input":"2024-09-29T13:54:43.654560Z","iopub.status.idle":"2024-09-29T13:54:43.666383Z","shell.execute_reply.started":"2024-09-29T13:54:43.654526Z","shell.execute_reply":"2024-09-29T13:54:43.665389Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'abuse': '',\n 'abuse_table': '',\n 'active_ingredient': '',\n 'active_ingredient_table': '',\n 'adverse_reactions': 'ADVERSE REACTIONS Most adverse effects have been mild and transient. The frequency estimates in the following table were derived from controlled studies in hypertensive patients in which adverse reactions were either volunteered by the patient (U.S. studies) or elicited, e.g., by checklist (foreign studies). The reported frequency of elicited adverse effects was higher for both atenolol and placebo-treated patients than when these reactions were volunteered. Where frequency of adverse effects of atenolol and placebo is similar, causal relationship to atenolol is uncertain. Volunteered (US Studies) Total-Volunteered and Elicited (Foreign + U.S. Studies) Atenolol (n = 164) % Placebo (n = 206) % Atenolol (n = 339) % Placebo (n = 407) % CARDIOVASCULAR Bradycardia 3 0 3 0 Cold Extremities 0 0.5 12 5 Postural Hypotension 2 1 4 5 Leg Pain 0 0.5 3 1 CENTRAL NERVOUS SYSTEM/ NEUROMUSCULAR Dizziness 4 1 13 6 Vertigo 2 0.5 2 0.2 Lightheadedness 1 0 3 0.7 Tiredness 0.6 0.5 26 13 Fatigue 3 1 6 5 Lethargy 1 0 3 0.7 Drowsiness 0.6 0 2 0.5 Depression 0.6 0.5 12 9 Dreaming 0 0 3 1 GASTROINTESTINAL Diarrhea 2 0 3 2 Nausea 4 1 3 1 RESPIRATORY (see WARNINGS ) Wheeziness 0 0 3 3 Dyspnea 0.6 1 6 4 Acute Myocardial Infarction In a series of investigations in the treatment of acute myocardial infarction, bradycardia and hypotension occurred more commonly, as expected for any beta-blocker, in atenolol-treated patients than in control patients. However, these usually responded to atropine and/or to withholding further dosage of atenolol. The incidence of heart failure was not increased by atenolol. Inotropic agents were infrequently used. The reported frequency of these and other events occurring during these investigations is given in the following table. In a study of 477 patients, the following adverse events were reported during either intravenous and/or oral atenolol administration: Conventional Therapy Plus Atenolol (n = 244) Conventional Therapy Alone (n = 233) Bradycardia 43 (18%) 24 (10%) Hypotension 60 (25%) 34 (15%) Bronchospasm 3 (1.2%) 2 (0.9%) Heart Failure 46 (19%) 56 (24%) Heart Block 11 (4.5%) 10 (4.3%) BBB + Major Axis Deviation 16 (6.6%) 28 (12%) Supraventricular Tachycardia 28 (11.5%) 45 (19%) Atrial Fibrillation 12 (5%) 29 (11%) Atrial Flutter 4 (1.6%) 7 (3%) Ventricular Tachycardia 39 (16%) 52 (22%) Cardiac Reinfarction 0 (0%) 6 (2.6%) Total Cardiac Arrests 4 (1.6%) 16 (6.9%) Nonfatal Cardiac Arrests 4 (1.6%) 12 (5.1%) Deaths 7 (2.9%) 16 (6.9%) Cardiogenic Shock 1 (0.4%) 4 (1.7%) Development of Ventricular Septal Defect 0 (0%) 2 (0.9%) Development of MitralRegurgitation 0 (0%) 2 (0.9%) Renal Failure 1 (0.4%) 0 (0%) Pulmonary Emboli 3 (1.2%) 0 (0%) In the subsequent International Study of Infarct Survival (ISIS-1) including over 16,000 patients of whom 8,037 were randomized to receive atenolol treatment, the dosage of intravenous and subsequent oral atenolol was either discontinued or reduced for the following reasons: Reasons for Reduced Dosage IV Atenolol Reduced Dose ( first degree) 5 (0.06%) 143 (1.7%) Cardiac Failure 1 (0.01%) 233 (2.9%) Arrhythmias 3 (0.04%) 22 (0.27%) Bronchospasm 1 (0.01%) 50 (0.62%) *Full dosage was 10 mg and some patients received less than 10 mg but more than 5 mg. During postmarketing experience with atenolol, the following have been reported in temporal relationship to the use of the drug: elevated liver enzymes and/or bilirubin, hallucinations, headache, impotence, Peyronie’s disease, postural hypotension which may be associated with syncope, psoriasiform rash or exacerbation of psoriasis, psychoses, purpura, reversible alopecia, thrombocytopenia, visual disturbance, sick sinus syndrome, and dry mouth. Atenolol, like other beta-blockers, has been associated with the development of antinuclear antibodies (ANA), lupus syndrome, and Raynaud’s phenomenon. POTENTIAL ADVERSE EFFECTS In addition, a variety of adverse effects have been reported with other beta-adrenergic blocking agents, and may be considered potential adverse effects of atenolol. Hematologic: Agranulocytosis. Allergic: Fever, combined with aching and sore throat, laryngospasm, and respiratory distress. Central Nervous System: Reversible mental depression progressing to catatonia; an acute reversible syndrome characterized by disorientation of time and place; short-term memory loss; emotional lability with slightly clouded sensorium; and, decreased performance on neuropsychometrics. Gastrointestinal: Mesenteric arterial thrombosis, ischemic colitis. Other: Erythematous rash. Miscellaneous: There have been reports of skin rashes and/or dry eyes associated with the use of beta-adrenergic blocking drugs. The reported incidence is small, and in most cases, the symptoms have cleared when treatment was withdrawn. Discontinuance of the drug should be considered if any such reaction is not otherwise explicable. Patients should be closely monitored following cessation of therapy (see INDICATIONS AND USAGE ). The oculomucocutaneous syndrome associated with the beta-blocker practolol has not been reported with atenolol. Furthermore, a number of patients who had previously demonstrated established practolol reactions were transferred to atenolol therapy with subsequent resolution or quiescence of the reaction. To report side effects, you can call (866) 562-4597 or (800) FDA-1088.',\n 'adverse_reactions_table': '  Volunteered  (US Studies) Total-Volunteered  and Elicited (Foreign + U.S. Studies) Atenolol  (n = 164)  % Placebo  (n = 206)  % Atenolol  (n = 339)  % Placebo  (n = 407)  % CARDIOVASCULAR Bradycardia 3 0 3 0 Cold Extremities 0 0.5 12 5 Postural Hypotension 2 1 4 5 Leg Pain 0 0.5 3 1 CENTRAL NERVOUS SYSTEM/  NEUROMUSCULAR Dizziness 4 1 13 6 Vertigo 2 0.5 2 0.2 Lightheadedness 1 0 3 0.7 Tiredness 0.6 0.5 26 13 Fatigue 3 1 6 5 Lethargy 1 0 3 0.7 Drowsiness 0.6 0 2 0.5 Depression 0.6 0.5 12 9 Dreaming 0 0 3 1 GASTROINTESTINAL Diarrhea 2 0 3 2 Nausea 4 1 3 1 RESPIRATORY (see WARNINGS)          Wheeziness 0 0 3 3 Dyspnea 0.6 1 6 4    Conventional Therapy  Plus Atenolol (n = 244)  Conventional  Therapy Alone (n = 233)  Bradycardia  43  (18%)  24  (10%)  Hypotension  60  (25%)  34  (15%)  Bronchospasm  3  (1.2%)  2  (0.9%)  Heart Failure  46  (19%)  56  (24%)  Heart Block  11  (4.5%)  10  (4.3%)  BBB + Major  Axis Deviation  16  (6.6%)  28  (12%)  Supraventricular Tachycardia  28  (11.5%)  45  (19%)  Atrial Fibrillation  12  (5%)  29  (11%)  Atrial Flutter  4  (1.6%)  7  (3%)  Ventricular Tachycardia  39  (16%)  52  (22%)  Cardiac Reinfarction  0  (0%)  6  (2.6%)  Total Cardiac Arrests  4  (1.6%)  16  (6.9%)  Nonfatal Cardiac Arrests  4  (1.6%)  12  (5.1%)  Deaths  7  (2.9%)  16  (6.9%)  Cardiogenic Shock  1  (0.4%)  4  (1.7%)  Development of Ventricular   Septal Defect  0  (0%)  2  (0.9%)  Development of MitralRegurgitation  0  (0%)  2  (0.9%)  Renal Failure  1  (0.4%)  0  (0%)  Pulmonary Emboli  3  (1.2%)  0  (0%)     Reasons for Reduced Dosage  IV Atenolol  Reduced Dose  (&lt; 5 mg)*  Oral Partial Dose Hypotension/Bradycardia  105 (1.3%)  1168 (14.5%) Cardiogenic Shock  4 (0.04%)  35 (0.44%) Reinfarction  0 (0%)  5 (0.06%) Cardiac Arrest  5 (0.06%)  28 (0.34%) Heart Block (&gt; first degree)  5 (0.06%)  143 (1.7%) Cardiac Failure  1 (0.01%)  233 (2.9%) Arrhythmias  3 (0.04%)  22 (0.27%) Bronchospasm  1 (0.01%)  50 (0.62%) ',\n 'alarms': '',\n 'ask_doctor_or_pharmacist': '',\n 'ask_doctor_or_pharmacist_table': '',\n 'ask_doctor_table': '',\n 'brand_name': '',\n 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility': '',\n 'clinical_pharmacology_table': '',\n 'clinical_studies_table': '',\n 'components_table': '',\n 'contraindications': \"CONTRAINDICATIONS Atenolol is contraindicated in sinus bradycardia, heart block greater than first degree, cardiogenic shock, and overt cardiac failure (see WARNINGS ). Atenolol is contraindicated in those patients with a history of hypersensitivity to the atenolol or any of the drug product's components\",\n 'controlled_substance': '',\n 'dependence': '',\n 'dependence_table': '',\n 'description': 'DESCRIPTION Atenolol, a synthetic, beta 1 -selective (cardioselective) adrenoreceptor blocking agent, may be chemically described as benzeneacetamide, 4 -[2’-hydroxy-3’-[(1- methylethyl) amino] propoxy]-. The molecular and structural formulas are: Atenolol (free base) has a molecular weight of 266. It is a relatively polar hydrophilic compound with a water solubility of 26.5 mg/mL at 37°C and a log partition coefficient (octanol/water) of 0.23. It is freely soluble in 1N HCl (300 mg/mL at 25°C) and less soluble in chloroform (3 mg/mL at 25°C). Atenolol tablets, USP is available as 25, 50 and 100 mg tablets for oral administration. Inactive Ingredients: Magnesium stearate, microcrystalline cellulose, povidone, sodium starch glycolate, hydroxypropyl methylcellulose, titanium dioxide and glycerine Atenolol Structure',\n 'description_table': '',\n 'do_not_use_table': '',\n 'dosage_and_administration': 'DOSAGE & ADMINISTRATION Hypertension The initial dose of atenolol is 50 mg given as one tablet a day either alone or added to diuretic therapy. The full effect of this dose will usually be seen within one to two weeks. If an optimal response is not achieved, the dosage should be increased to atenolol 100 mg given as one tablet a day. Increasing the dosage beyond 100 mg a day is unlikely to produce any further benefit. Atenolol may be used alone or concomitantly with other antihypertensive agents including thiazide-type diuretics, hydralazine, prazosin, and alpha-methyldopa. Angina Pectoris The initial dose of atenolol is 50 mg given as one tablet a day. If an optimal response is not achieved within one week, the dosage should be increased to atenolol 100 mg given as one tablet a day. Some patients may require a dosage of 200 mg once a day for optimal effect. Twenty-four hour control with once daily dosing is achieved by giving doses larger than necessary to achieve an immediate maximum effect. The maximum early effect on exercise tolerance occurs with doses of 50 to 100 mg, but at these doses the effect at 24 hours is attenuated, averaging about 50% to 75% of that observed with once a day oral doses of 200 mg. Acute Myocardial Infarction In patients with definite or suspected acute myocardial infarction, treatment with atenolol I.V. Injection should be initiated as soon as possible after the patient’s arrival in the hospital and after eligibility is established. Such treatment should be initiated in a coronary care or similar unit immediately after the patient’s hemodynamic condition has stabilized. Treatment should begin with the intravenous administration of 5 mg atenolol over 5 minutes followed by another 5 mg intravenous injection 10 minutes later. Atenolol I.V. Injection should be administered under carefully controlled conditions including monitoring of blood pressure, heart rate, and electrocardiogram. Dilutions of atenolol I.V. Injection in Dextrose Injection USP, Sodium Chloride Injection USP, or Sodium Chloride and Dextrose Injection may be used. These admixtures are stable for 48 hours if they are not used immediately. In patients who tolerate the full intravenous dose (10 mg), atenolol tablets 50 mg should be initiated 10 minutes after the last intravenous dose followed by another 50 mg oral dose 12 hours later. Thereafter, atenolol can be given orally either 100 mg once daily or 50 mg twice a day for a further 6-9 days or until discharge from the hospital. If bradycardia or hypotension requiring treatment or any other untoward effects occur, atenolol should be discontinued. (See full prescribing information prior to initiating therapy with atenolol tablets.) Data from other beta-blocker trials suggest that if there is any question concerning the use of IV beta-blocker or clinical estimate that there is a contraindication, the IV beta-blocker may be eliminated and patients fulfilling the safety criteria may be given atenolol tablets 50 mg twice daily or 100 mg once a day for at least seven days (if the IV dosing is excluded). Although the demonstration of efficacy of atenolol is based entirely on data from the first seven postinfarction days, data from other beta-blocker trials suggest that treatment with beta-blockers that are effective in the postinfarction setting may be continued for one to three years if there are no contraindications. Atenolol is an additional treatment to standard coronary care unit therapy. Elderly Patients or Patients with Renal Impairment Atenolol is excreted by the kidneys; consequently dosage should be adjusted in cases of severe impairment of renal function. In general, dose selection for an elderly patient should be cautious, usually starting at the low end of the dosing range, reflecting greater frequency of decreased hepatic, renal, or cardiac function, and of concomitant disease or other drug therapy. Evaluation of patients with hypertension or myocardial infarction should always include assessment of renal function. Atenolol excretion would be expected to decrease with advancing age. No significant accumulation of atenolol occurs until creatinine clearance falls below 35 mL/min/1.73 m2. Accumulation of atenolol and prolongation of its half-life were studied in subjects with creatinine clearance between 5 and 105 mL/min. Peak plasma levels were significantly increased in subjects with creatinine clearances below 30 mL/min. The following maximum oral dosages are recommended for elderly, renally-impaired patients and for patients with renal impairment due to other causes: Creatinine Clearance (mL/min/1.73 m2) Atenolol Elimination Half-Life (h) Maximum Dosage 15-35 16- 27 50 mg daily 15 27 25 mg daily Some renally-impaired or elderly patients being treated for hypertension may require a lower starting dose of atenolol: 25 mg given as one tablet a day. If this 25 mg dose is used, assessment of efficacy must be made carefully. This should include measurement of blood pressure just prior to the next dose (\"trough\" blood pressure) to ensure that the treatment effect is present for a full 24 hours. Although a similar dosage reduction may be considered for elderly and/or renally-impaired patients being treated for indications other than hypertension, data are not available for these patient populations. Patients on hemodialysis should be given 25 mg or 50 mg after each dialysis; this should be done under hospital supervision as marked falls in blood pressure can occur. Cessation of Therapy in Patients with Angina Pectoris If withdrawal of atenolol therapy is planned, it should be achieved gradually and patients should be carefully observed and advised to limit physical activity to a minimum.',\n 'dosage_and_administration_table': 'Creatinine Clearance  (mL/min/1.73 m2) Atenolol Elimination  Half-Life (h)  Maximum Dosage 15-35 16- 27 50 mg daily 15 27 25 mg daily ',\n 'drug_abuse_and_dependence': '',\n 'drug_abuse_and_dependence_table': '',\n 'drug_and_or_laboratory_test_interactions_table': '',\n 'drug_interactions_table': '',\n 'effective_time': '20210302',\n 'general_precautions': '',\n 'general_precautions_table': '',\n 'generic_name': '',\n 'geriatric_use': '',\n 'how_supplied_table': '',\n 'id': '3c9027a9-c83c-4e58-8a9d-77b62d538ffb',\n 'inactive_ingredient': '',\n 'indications_and_usage': \"INDICATIONS & USAGE Atenolol tablets USP are indicated for the treatment of hypertension, to lower blood pressure. Lowering blood pressure lowers the risk of fatal and non-fatal cardiovascular events, primarily strokes and myocardial infarctions. These benefits have been seen in controlled trials of antihypertensive drugs from a wide variety of pharmacologic classes including atenolol. Control of high blood pressure should be part of comprehensive cardiovascular risk management, including, as appropriate, lipid control, diabetes management, antithrombotic therapy, smoking cessation, exercise, and limited sodium intake. Many patients will require more than 1 drug to achieve blood pressure goals. For specific advice on goals and management, see published guidelines, such as those of the National High Blood Pressure Education Program's Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure (JNC). Numerous antihypertensive drugs, from a variety of pharmacologic classes and with different mechanisms of action, have been shown in randomized controlled trials to reduce cardiovascular morbidity and mortality, and it can be concluded that it is blood pressure reduction, and not some other pharmacologic property of the drugs, that is largely responsible for those benefits. The largest and most consistent cardiovascular outcome benefit has been a reduction in the risk of stroke, but reductions in myocardial infarction and cardiovascular mortality also have been seen regularly. Elevated systolic or diastolic pressure causes increased cardiovascular risk, and the absolute risk increase per mmHg is greater at higher blood pressures, so that even modest reductions of severe hypertension can provide substantial benefit. Relative risk reduction from blood pressure reduction is similar across populations with varying absolute risk, so the absolute benefit is greater in patients who are at higher risk independent of their hypertension (for example, patients with diabetes or hyperlipidemia), and such patients would be expected to benefit from more aggressive treatment to a lower blood pressure goal. Some antihypertensive drugs have smaller blood pressure effects (as monotherapy) in black patients, and many antihypertensive drugs have additional approved indications and effects (e.g., on angina, heart failure, or diabetic kidney disease). These considerations may guide selection of therapy. Atenolol tablets USP may be administered with other antihypertensive agents. Angina Pectoris Due to Coronary Atherosclerosis: Atenolol is indicated for the long-term management of patients with angina pectoris. Acute Myocardial Infarction: Atenolol is indicated in the management of hemodynamically stable patients with definite or suspected acute myocardial infarction to reduce cardiovascular mortality. Treatment can be initiated as soon as the patient's clinical condition allows. (See DOSAGE AND ADMNISTRATION , CONTRAINDICATIONS and WARNINGS ). In general, there is no basis for treating patients like those who were excluded from the ISIS-1 trial (blood pressure less than 100 mm Hg systolic, heart rate less than 50 bpm) or have other reasons to avoid beta-blockade. As noted above, some subgroups (e.g., elderly patients with systolic blood pressure below 120 mm Hg) seemed less likely to benefit.\",\n 'indications_and_usage_table': '',\n 'information_for_patients_table': '',\n 'instructions_for_use': '',\n 'labor_and_delivery': '',\n 'labor_and_delivery_table': '',\n 'laboratory_tests': '',\n 'laboratory_tests_table': '',\n 'last_updated': '2024-09-21',\n 'mechanism_of_action_table': '',\n 'microbiology_table': '',\n 'nonteratogenic_effects': '',\n 'nursing_mothers': '',\n 'nursing_mothers_table': '',\n 'overdosage': 'OVERDOSAGE Overdosage with atenolol has been reported with patients surviving acute doses as high as 5 g. One death was reported in a man who may have taken as much as 10 g acutely. The predominant symptoms reported following atenolol overdose are lethargy, disorder of respiratory drive, wheezing, sinus pause and bradycardia. Additionally, common effects associated with overdosage of any beta-adrenergic blocking agent and which might also be expected in atenolol overdose are congestive heart failure, hypotension, bronchospasm and/or hypoglycemia. Treatment of overdose should be directed to the removal of any unabsorbed drug by induced emesis, gastric lavage, or administration of activated charcoal. Atenolol can be removed from the general circulation by hemodialysis. Other treatment modalities should be employed at the physician’s discretion and may include: BRADYCARDIA: Atropine intravenously. If there is no response to vagal blockade, give isoproterenol cautiously. In refractory cases, a transvenous cardiac pacemaker may be indicated. HEART BLOCK (SECOND OR THIRD DEGREE): Isoproterenol or transvenous cardiac pacemaker. CARDIAC FAILURE: Digitalize the patient and administer a diuretic. Glucagon has been reported to be useful. HYPOTENSION: Vasopressors such as dopamine or norepinephrine (levarterenol). Monitor blood pressure continuously. BRONCHOSPASM: A beta2 stimulant such as isoproterenol or terbutaline and/or aminophylline. HYPOGLYCEMIA: Intravenous glucose. Based on the severity of symptoms, management may require intensive support care and facilities for applying cardiac and respiratory support.',\n 'overdosage_table': '',\n 'package_label_principal_display_panel': 'ATENOLOL TABLET Label Image',\n 'patient_medication_information': '',\n 'pediatric_use_table': '',\n 'pharmacodynamics_table': '',\n 'pharmacogenomics_table': '',\n 'pharmacokinetics_table': '',\n 'precautions': 'PRECAUTIONS General Patients already on a beta-blocker must be evaluated carefully before atenolol is administered. Initial and subsequent atenolol dosages can be adjusted downward depending on clinical observations including pulse and blood pressure. Atenolol may aggravate peripheral arterial circulatory disorders. Impaired Renal Function The drug should be used with caution in patients with impaired renal function (see DOSAGE AND ADMINISTRATION ). Drug Interactions Catecholamine-depleting drugs (e.g., reserpine) may have an additive effect when given with beta-blocking agents. Patients treated with atenolol plus a catecholamine depletor should therefore be closely observed for evidence of hypotension and/or marked bradycardia which may produce vertigo, syncope, or postural hypotension. Calcium channel blockers may also have an additive effect when given with atenolol (see WARNINGS ). Disopyramide is a Type I antiarrhythmic drug with potent negative inotropic and chronotropic effects. Disopyramide has been associated with severe bradycardia, asystole and heart failure when administered with beta-blockers. Amiodarone is an antiarrhythmic agent with negative chronotropic properties that may be additive to those seen with beta-blockers. Beta-blockers may exacerbate the rebound hypertension which can follow the withdrawal of clonidine. If the two drugs are coadministered, the beta-blocker should be withdrawn several days before the gradual withdrawal of clonidine. If replacing clonidine by beta-blocker therapy, the introduction of beta-blockers should be delayed for several days after clonidine administration has stopped. Concomitant use of prostaglandin synthase inhibiting drugs, e.g., indomethacin, may decrease the hypotensive effects of beta-blockers. Information on concurrent usage of atenolol and aspirin is limited. Data from several studies, i.e., TIMI-II, ISIS-2, currently do not suggest any clinical interaction between aspirin and beta-blockers in the acute myocardial infarction setting. While taking beta-blockers, patients with a history of anaphylactic reaction to a variety of allergens may have a more severe reaction on repeated challenge, either accidental, diagnostic or therapeutic. Such patients may be unresponsive to the usual doses of epinephrine used to treat the allergic reaction. Both digitalis glycosides and beta-blockers slow atrioventricular conduction and decrease heart rate. Concomitant use can increase the risk of bradycardia. Carcinogenesis, Mutagenesis, Impairment of Fertility Two long-term (maximum dosing duration of 18 or 24 months) rat studies and one long-term (maximum dosing duration of 18 months) mouse study, each employing dose levels as high as 300 mg/kg/day or 150 times the maximum recommended human antihypertensive dose*, did not indicate a carcinogenic potential of atenolol. A third (24 month) rat study, employing doses of 500 and 1,500 mg/kg/day (250 and 750 times the maximum recommended human antihypertensive dose*) resulted in increased incidences of benign adrenal medullary tumors in males and females, mammary fibroadenomas in females, and anterior pituitary adenomas and thyroid parafollicular cell carcinomas in males. No evidence of a mutagenic potential of atenolol was uncovered in the dominant lethal test (mouse), in vivo cytogenetics test (Chinese hamster) or Ames test (S typhimurium). Fertility of male or female rats (evaluated at dose levels as high as 200 mg/kg/day or 100 times the maximum recommended human dose*) was unaffected by atenolol administration. Animal Toxicology Chronic studies employing oral atenolol performed in animals have revealed the occurrence of vacuolation of epithelial cells of Brunner’s glands in the duodenum of both male and female dogs at all tested dose levels of atenolol (starting at 15 mg/kg/day or 7.5 times the maximum recommended human antihypertensive dose*) and increased incidence of atrial degeneration of hearts of male rats at 300 but not 150 mg atenolol/kg/day (150 and 75 times the maximum recommended human antihypertensive dose*, respectively). *Based on the maximum dose of 100 mg/day in a 50 kg patient. Usage in Pregnancy Pregnancy Category D See WARNINGS - Pregnancy and Fetal Injury . Nursing Mothers Atenolol is excreted in human breast milk at a ratio of 1.5 to 6.8 when compared to the concentration in plasma. Caution should be exercised when atenolol is administered to a nursing woman. Clinically significant bradycardia has been reported in breastfed infants. Premature infants, or infants with impaired renal function, may be more likely to develop adverse effects. Neonates born to mothers who are receiving atenolol at parturition or breastfeeding may be at risk for hypoglycemia and bradycardia. Caution should be exercised when atenolol is administered during pregnancy or to a woman who is breastfeeding (see WARNINGS, Pregnancy and Fetal Injury ). Pediatric Use Safety and effectiveness in pediatric patients have not been established. Geriatric Use Hypertension and Angina Pectoris Due to Coronary Atherosclerosis: Clinical studies of atenolol did not include sufficient number of patients aged 65 and over to determine whether they respond differently from younger subjects. Other reported clinical experience has not identified differences in responses between the elderly and younger patients. In general, dose selection for an elderly patient should be cautious, usually starting at the low end of the dosing range, reflecting the greater frequency of decreased hepatic, renal, or cardiac function, and of concomitant disease or other drug therapy. Acute Myocardial Infarction: Of the 8,037 patients with suspected acute myocardial infarction randomized to atenolol in the ISIS-1 trial (see CLINICAL PHARMACOLOGY ), 33% (2,644) were 65 years of age and older. It was not possible to identify significant differences in efficacy and safety between older and younger patients; however, elderly patients with systolic blood pressure < 120 mm Hg seemed less likely to benefit (see INDICATIONS AND USAGE ). In general, dose selection for an elderly patient should be cautious, usually starting at the low end of the dosing range, reflecting greater frequency of decreased hepatic, renal, or cardiac function, and of concomitant disease or other drug therapy. Evaluation of patients with hypertension or myocardial infarction should always include assessment of renal function.',\n 'precautions_table': '',\n 'pregnancy': '',\n 'pregnancy_or_breast_feeding': '',\n 'pregnancy_or_breast_feeding_table': '',\n 'pregnancy_table': '',\n 'product_type': '',\n 'purpose_table': '',\n 'questions_table': '',\n 'recent_major_changes': '',\n 'recent_major_changes_table': '',\n 'risks': '',\n 'risks_table': '',\n 'spl_medguide': '',\n 'spl_patient_package_insert': '',\n 'statement_of_identity': '',\n 'stop_use_table': '',\n 'storage_and_handling': '',\n 'storage_and_handling_table': '',\n 'substance_name': '',\n 'summary_of_safety_and_effectiveness': '',\n 'teratogenic_effects': '',\n 'use_in_specific_populations': '',\n 'use_in_specific_populations_table': '',\n 'user_safety_warnings': '',\n 'warnings': 'WARNINGS Cardiac Failure Sympathetic stimulation is necessary in supporting circulatory function in congestive heart failure, and beta-blockade carries the potential hazard of further depressing myocardial contractility and precipitating more severe failure. In patients with acute myocardial infarction, cardiac failure which is not promptly and effectively controlled by 80 mg of intravenous furosemide or equivalent therapy is a contraindication to beta-blocker treatment. In Patients Without a History of Cardiac Failure Continued depression of the myocardium with beta-blocking agents over a period of time can, in some cases, lead to cardiac failure. At the first sign or symptom of impending cardiac failure, patients should be treated appropriately according to currently recommended guidelines, and the response observed closely. If cardiac failure continues despite adequate treatment, atenolol should be withdrawn (see DOSAGE AND ADMNISTRATION ).',\n 'warnings_and_cautions_table': '',\n 'when_using': '',\n 'when_using_table': ''}"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Imprime el consumo de GPU antes de cargar el modelo pre-entrenado","metadata":{}},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:49.298219Z","iopub.execute_input":"2024-09-29T13:54:49.298902Z","iopub.status.idle":"2024-09-29T13:54:49.303981Z","shell.execute_reply.started":"2024-09-29T13:54:49.298867Z","shell.execute_reply":"2024-09-29T13:54:49.303032Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"GPU memory occupied: 267 MB.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntry:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:54:51.695150Z","iopub.execute_input":"2024-09-29T13:54:51.696041Z","iopub.status.idle":"2024-09-29T13:55:51.202417Z","shell.execute_reply.started":"2024-09-29T13:54:51.696008Z","shell.execute_reply":"2024-09-29T13:55:51.201379Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887088e77a444263895b39e37264218e"}},"metadata":{}},{"name":"stdout","text":"self.model_name_or_path : meta-llama/Llama-2-7b-hf\nCPU times: user 5.85 s, sys: 6.52 s, total: 12.4 s\nWall time: 59.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:14.893065Z","iopub.execute_input":"2024-09-29T14:09:14.893864Z","iopub.status.idle":"2024-09-29T14:09:14.898902Z","shell.execute_reply.started":"2024-09-29T14:09:14.893828Z","shell.execute_reply":"2024-09-29T14:09:14.897979Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"GPU memory occupied: 2105 MB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 6. Prueba el modelo con inferencia Zero Shot","metadata":{}},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 26\nset_seed(seed)\nmax_tokens = 500\n\ntry:\n    prompt = dataset['train'][index]\n    \n    prompt_description = \"\"\"Generate a detailed description for the following medication: [Name, Composition, Indications, Dosage, Side Effects, Contraindications, Interactions]\n    Medication: Ibuprofen\n    \"\"\"\n\n    # Pregunta al modelo o instrucciones para preguntas y respuestas\n    prompt_question = \"Using the context of the medication Ibuprofen, answer the following question: What are the common side effects of Ibuprofen?\"\n    \n    # Mensaje del sistema\n    system_message = \"\"\"Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\n    You must follow the following structure for descriptions:\n    - Name: [Medication Name]\n    - Composition: [Active ingredients]\n    - Indications: [Uses]\n    - Dosage: [Recommended dosage]\n    - Side Effects: [Common side effects]\n    - Contraindications: [When the medication should not be used]\n    - Interactions: [Drugs or substances that interact with this medication]\n    Please use only the information available in the context provided.\"\"\"\n\n    # Template del prompt completo con instancias y sistema\n    formatted_prompt = f'''<s>[INST] <<SYS>>\n    {system_message}\n    <</SYS>>\n    {prompt_description} [/INST]</s>'''\n\n    # Template para preguntas y respuestas\n    question_template = f'''<s>[INST] <<SYS>>\n    {system_message}\n    <</SYS>>\n    {prompt_question} [/INST]</s>'''\n    \n    output = llm.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Model Generation - Zero Shot:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7. Pre-procesando el dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntry:\n    \n    path_tokenizer = os.path.join(BASE_FOLDER, \"dataset\")\n    path_save_tokenizer = os.path.join(path_tokenizer, name_zip_tokenizer)\n    #path_load_tokenizer = os.path.join(DATASET_FOLDER, name_zip_tokenizer)\n    \n    train_dataset_path = os.path.join(path_tokenizer, \"train_dataset\")\n    eval_dataset_path = os.path.join(path_tokenizer, \"eval_dataset\")\n    \n    if not(os.path.exists(path_tokenizer)):\n        #!mkdir -p {tokenizer_path_folder}\n        os.makedirs(path_tokenizer)\n        print('Directorio para almacenar dataset creado exitosamente!')\n    \n     \n    if (LOAD_TOKENIZER):\n        if (os.path.exists(TOKENIZER_PATH)):\n            with zipfile.ZipFile(TOKENIZER_PATH, 'r') as zip_ref:\n                zip_ref.extractall(path_tokenizer)\n            print(f\"Tokenizador cargado desde {TOKENIZER_PATH}\")\n        else:\n            raise FileNotFoundError(f\"El tokenizador no existe en la ruta {TOKENIZER_PATH}\")\n        \n        train_dataset = load_dataset(train_dataset_path)\n        eval_dataset = load_dataset(eval_dataset_path)\n        \n    else:\n        if not(PROCESS_SAMPLE):\n            max_length = llm.get_max_length()\n            print('Inicia pre-procesamiento')\n            train_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                               max_length=max_length,\n                                               seed=seed,\n                                               dataset=dataset['train']\n                                              )\n\n            eval_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                              max_length=max_length,\n                                              seed=seed,\n                                              dataset=dataset['validation']\n                                             )\n        else:\n            train_dataset = preprocess_dataset_sample(seed=seed,\n                                                      dataset=dataset['train']\n                                                     )\n\n            eval_dataset = preprocess_dataset_sample(seed=seed,\n                                                     dataset=dataset['validation']\n                                                     )\n            \n        \n        \n        if(SAVE_TOKENIZER):\n            # save in disk\n            train_dataset.save_to_disk(train_dataset_path)\n            eval_dataset.save_to_disk(eval_dataset_path)\n            \n            file_path = None\n            \n            if (os.path.exists(path_save_tokenizer)):\n                os.remove(path_save_tokenizer)  # Eliminar el archivo existente\n                print(f'Archivo zip existente, eliminado desde la ruta : {path_tokenizer}')\n                \n            with zipfile.ZipFile(path_save_tokenizer, 'w') as zipf:\n                for folder, subfolders, files in os.walk(path_tokenizer):\n                    for file in files:\n                        file_path = os.path.join(folder, file)\n                        zipf.write(file_path, os.path.relpath(file_path, path_tokenizer))\n            \n            if(os.path.exists(path_save_tokenizer)):\n                print('Process file zip tokenizer...')\nexcept Exception as ex:\n    print(f\"Error [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n\nFileLink(f'./{PROJECT_NAME}/dataset/{name_zip_tokenizer}')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:23.383548Z","iopub.execute_input":"2024-09-29T14:09:23.383976Z","iopub.status.idle":"2024-09-29T14:09:23.548448Z","shell.execute_reply.started":"2024-09-29T14:09:23.383946Z","shell.execute_reply":"2024-09-29T14:09:23.547527Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Found max length: 4096\nInicia pre-procesamiento\nPreprocessing dataset...\nCreate prompt llama2...\nNúmero de núcleos de la CPU disponibles: 4\nPreprocessing dataset...\nCreate prompt llama2...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/49 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2bee798feb94065a34dd65b53e47bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4466f67287b14d95accc1d075e434fcc"}},"metadata":{}},{"name":"stdout","text":"Archivo zip existente, eliminado desde la ruta : /kaggle/working/drugs-generative/dataset\nProcess file zip tokenizer...\nCPU times: user 116 ms, sys: 13.5 ms, total: 130 ms\nWall time: 148 ms\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/drugs-generative/dataset/tokenizer.zip","text/html":"<a href='./drugs-generative/dataset/tokenizer.zip' target='_blank'>./drugs-generative/dataset/tokenizer.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:32.146720Z","iopub.execute_input":"2024-09-29T14:09:32.147595Z","iopub.status.idle":"2024-09-29T14:09:32.152580Z","shell.execute_reply.started":"2024-09-29T14:09:32.147562Z","shell.execute_reply":"2024-09-29T14:09:32.151665Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (49, 3)\nValidation: (9, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 49\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 8. Configura el modelo PEFT/LoRA para el Fine-Tuning\nAhora, vamos a realizar un ajuste fino eficiente en parámetros (PEFT). PEFT es una forma de ajuste fino por instrucciones que es mucho más eficiente que el ajuste fino completo. PEFT es un término genérico que incluye Adaptación de Bajo Rango (LoRA) y ajuste por indicaciones (¡que NO ES LO MISMO que la ingeniería de prompts!). En la mayoría de los casos, cuando alguien menciona PEFT, generalmente se refieren a LoRA. LoRA, en esencia, permite un ajuste fino eficiente del modelo utilizando menos recursos computacionales, a menudo realizable con solo una GPU. Después del ajuste fino con LoRA para una tarea o caso de uso específico, el resultado es un LLM original sin cambios y la aparición de un \"adaptador LoRA\" considerablemente más pequeño, que a menudo representa un porcentaje de un solo dígito del tamaño del LLM original (en MBs en lugar de GBs).\n\nDurante la inferencia, el adaptador LoRA debe combinarse con su LLM original. La ventaja radica en la capacidad de muchos adaptadores LoRA para reutilizar el LLM original, reduciendo así los requisitos generales de memoria cuando se manejan múltiples tareas y casos de uso.\n\nNota el hiperparámetro de rango (r), que define el rango/dimensión del adaptador a ser entrenado. r es el rango de la matriz de bajo rango utilizada en los adaptadores, lo que controla el número de parámetros entrenados. Un rango mayor permitirá mayor expresividad, pero hay una compensación en términos de cómputo.\n\nalpha es el factor de escalado para los pesos aprendidos. La matriz de pesos se escala por alpha/r, y por lo tanto, un valor más alto de alpha asigna más peso a las activaciones de LoRA.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(llm.model))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:37.027965Z","iopub.execute_input":"2024-09-29T14:09:37.028635Z","iopub.status.idle":"2024-09-29T14:09:37.036409Z","shell.execute_reply.started":"2024-09-29T14:09:37.028601Z","shell.execute_reply":"2024-09-29T14:09:37.035412Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"all model parameters: 3500412928\ntrainable model parameters: 262410240\npercentage of trainable model parameters: 7.50%\n","output_type":"stream"}]},{"cell_type":"code","source":"print(llm.model)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:39.121833Z","iopub.execute_input":"2024-09-29T14:09:39.122516Z","iopub.status.idle":"2024-09-29T14:09:39.130657Z","shell.execute_reply.started":"2024-09-29T14:09:39.122480Z","shell.execute_reply":"2024-09-29T14:09:39.129689Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"peft_config = LoraConfig(lora_alpha=lora_alpha, #16,\n                         lora_dropout=lora_dropout, #0.1,\n                         r=lora_r, #64,\n                         bias=\"none\",\n                         task_type=\"CAUSAL_LM\",\n                         target_modules=['q_proj','k_proj','v_proj','o_proj'], #dense\n                        )\n\n\n# 2 - Utilizando el método prepare_model_for_kbit_training de PEFT.\nllm.model = prepare_model_for_kbit_training(llm.model)\npeft_model = get_peft_model(llm.model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:45.219494Z","iopub.execute_input":"2024-09-29T14:09:45.219904Z","iopub.status.idle":"2024-09-29T14:09:46.426578Z","shell.execute_reply.started":"2024-09-29T14:09:45.219875Z","shell.execute_reply":"2024-09-29T14:09:46.425779Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Una vez que todo esté configurado y el modelo base esté preparado, podemos utilizar la función auxiliar print_trainable_parameters() para ver cuántos parámetros entrenables hay en el modelo.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:50.364622Z","iopub.execute_input":"2024-09-29T14:09:50.365029Z","iopub.status.idle":"2024-09-29T14:09:50.378792Z","shell.execute_reply.started":"2024-09-29T14:09:50.364999Z","shell.execute_reply":"2024-09-29T14:09:50.377856Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"all model parameters: 3567521792\ntrainable model parameters: 67108864\npercentage of trainable model parameters: 1.88%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Observa cómo se ve diferente el modelo ahora, con los adaptadores LoRA añadidos:\nprint(peft_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:09:52.896077Z","iopub.execute_input":"2024-09-29T14:09:52.897030Z","iopub.status.idle":"2024-09-29T14:09:52.914737Z","shell.execute_reply.started":"2024-09-29T14:09:52.896986Z","shell.execute_reply":"2024-09-29T14:09:52.913660Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32000, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 9. Entrenando el Adaptador PEFT\n\nDefine los argumentos de entrenamiento y crea una instancia de Trainer.","metadata":{}},{"cell_type":"code","source":"tensorboard_callback = TensorBoardCallback()\ntb_process, ngrok_process = launch_tensorboard()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:10:07.164788Z","iopub.execute_input":"2024-09-29T14:10:07.165708Z","iopub.status.idle":"2024-09-29T14:10:07.175277Z","shell.execute_reply.started":"2024-09-29T14:10:07.165671Z","shell.execute_reply":"2024-09-29T14:10:07.174176Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"TensorBoard URL: https://9a69-35-238-225-48.ngrok-free.app\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import amp\n\n'''\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n'''\n'''\npeft_training_args = TrainingArguments(\n    output_dir = LOG_TRAIN_PATH,\n    do_eval=True,\n    eval_strategy=\"steps\",\n    fp16=False,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    learning_rate=2.0e-04,\n    logging_steps=25,\n    log_level=\"info\",\n    logging_strategy=\"steps\",\n    lr_scheduler_type=\"cosine\",\n    max_steps=1000,\n    #num_train_epochs=1,\n    overwrite_output_dir = True,\n    per_device_eval_batch_size=1,\n    per_device_train_batch_size=1,\n    save_strategy=\"steps\",\n    eval_steps=25,\n    group_by_length=True,\n    logging_dir=LOG_PATH,\n    optim=\"paged_adamw_8bit\",\n    save_steps=25,\n    warmup_steps=50,\n    save_total_limit=None,\n    seed=42,\n    report_to=\"tensorboard\",\n)\n\npeft_trainer = transformers.Trainer(model=peft_model,\n                                    train_dataset=train_dataset,\n                                    eval_dataset=eval_dataset,\n                                    args=peft_training_args,\n                                    data_collator=transformers.DataCollatorForLanguageModeling(llm.tokenizer, mlm=False),\n                                    )\n'''\n\nllm.model.train()\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_eval_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    #group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    seed=42,\n    eval_strategy=evaluation_strategy,\n    eval_steps=eval_steps,\n    load_best_model_at_end=True,\n    report_to=\"tensorboard\"\n)\n\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=llm.model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    #dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=llm.tokenizer,\n    args=training_arguments,\n    compute_metrics=compute_metrics,\n    packing=packing,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:10:25.128237Z","iopub.execute_input":"2024-09-29T14:10:25.128652Z","iopub.status.idle":"2024-09-29T14:10:26.342830Z","shell.execute_reply.started":"2024-09-29T14:10:25.128622Z","shell.execute_reply":"2024-09-29T14:10:26.341733Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#training_args.device\ntorch.cuda.empty_cache()\nprint(f\"GPUs disponibles: {torch.cuda.device_count()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:10:38.094199Z","iopub.execute_input":"2024-09-29T14:10:38.095552Z","iopub.status.idle":"2024-09-29T14:10:38.103393Z","shell.execute_reply.started":"2024-09-29T14:10:38.095498Z","shell.execute_reply":"2024-09-29T14:10:38.102317Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"GPUs disponibles: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-29T14:10:43.755641Z","iopub.execute_input":"2024-09-29T14:10:43.756829Z","iopub.status.idle":"2024-09-29T14:15:28.661415Z","shell.execute_reply.started":"2024-09-29T14:10:43.756778Z","shell.execute_reply":"2024-09-29T14:15:28.659860Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 4/39 03:01 < 53:00, 0.01 it/s, Epoch 0.23/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:434\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 434\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:306\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:313\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    315\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 556.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 13829 has 14.26 GiB memory in use. Of the allocated memory 11.44 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 556.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 13829 has 14.26 GiB memory in use. Of the allocated memory 11.44 GiB is allocated by PyTorch, and 2.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Liberar memoria para la fusión de pesos\ndel llm.model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cualitativamente (Evaluación Humana)","metadata":{}},{"cell_type":"code","source":"try:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nft_model = PeftModel.from_pretrained(llm.model, \n                                     \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",\n                                     torch_dtype=torch.float16,\n                                     is_trainable=False\n                                    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 120\nset_seed(seed)\nmax_tokens = 512\n\ntry:\n    prompt = dataset['train'][index]\n\n    # Instrucción: Resume la siguiente conversación\n    formatted_prompt = f'Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\\n Provide a detailed description of the medication {prompt[\"generic_name\"]} using the available data.\\n Output:\\n'\n    res = ft_model.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n    output = res[0].split('Output:\\n')[1]\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Peft Model Generation:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cuantitativamente (con la Metrica ROUGE)","metadata":{}},{"cell_type":"code","source":"def data_process(dataset):\n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n\n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n\n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n\n        dataset = dataset.map(create_prompt_formats_v1,\n                              num_proc=num_proc\n                             )#, batched=True)\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    train_dataset=data_process(dataset['train'])\n    eval_dataset=data_process(dataset['validation'])\n\n    print(f\"Shapes of the datasets:\")\n    print(f\"Training: {train_dataset.shape}\")\n    print(f\"Validation: {eval_dataset.shape}\")\n    \nexcept Exception as ex:\n    print(f'Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_dataset[120])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset[120]","metadata":{},"execution_count":null,"outputs":[]}]}