{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9501867,"sourceType":"datasetVersion","datasetId":5782792}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**En este notebook y tutorial, realizaremos un fine-tune [Llama-8k](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) modelo relativamente pequeño de 7 mil millones de parametros - que ha 'demostrado un rendimiento casi de última generación entre los modelos con menos de 13 mil millones de parámetros' - *en tus propios datos!!***\n\n**Aqui usaremos [QLoRA (Efficient Finetuning of Quantized LLMs)](https://arxiv.org/abs/2305.14314), una técnica de fine-tunning altamente eficiente que consiste en cuantizar un LLM preentrenado a solo 4 bits y agregar pequeños 'Adaptadores de Bajo Rango'. Este enfoque único permite realizar el fine-tunning de LLMs utilizando solo una GPU. Esta técnica está respaldada por el/la... [PEFT library](https://huggingface.co/docs/peft/index).**","metadata":{}},{"cell_type":"markdown","source":"# Tabla de Contenido","metadata":{}},{"cell_type":"markdown","source":"- [1- Instalar librerias requeridas](#1)\n- [ 2 - Cargar dataset](#2)\n- [ 3 - Crear configuración de bitsandbytes](#3)\n- [ 4 - Cargar Modelo Base](#4)\n- [ 5 - Tokenizar](#5)\n- [ 6 - Testear el modelo con Zero Shot Inferencing](#6)\n- [ 7 - Pre-procesando el dataset](#7)\n- [ 8 - Configurar el modelo PEFT/LoRA para realizar Fine-Tuning](#8)\n- [ 9 - Entrenar Adaptador PEFT](#9)\n- [ 10 - Evaluar el Modelo Qualitativamente (Evaluacion Humana)](#10)\n- [ 11 - Evaluar el Modelo Quantitaviamente (con Metrica ROUGE)](#11)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n#### 1. Instalar librerias requeridas","metadata":{}},{"cell_type":"code","source":"%%time\n!pip install -U transformers\n!pip install -U bitsandbytes\n!pip install -U peft\n!pip install -U accelerate\n!pip install -U datasets\n!pip install -U scipy\n!pip install -U einops\n!pip install -U evaluate\n!pip install -U trl\n!pip install -U rouge_score\n!pip install -U torch","metadata":{"_uuid":"c34dd264-0a8e-4fae-8f88-83c906c39c3c","_cell_guid":"59a8cbf9-4057-4756-a512-100f15222529","execution":{"iopub.status.busy":"2024-09-28T17:15:26.334991Z","iopub.execute_input":"2024-09-28T17:15:26.335417Z","iopub.status.idle":"2024-09-28T17:21:19.112561Z","shell.execute_reply.started":"2024-09-28T17:15:26.335383Z","shell.execute_reply":"2024-09-28T17:21:19.110244Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nCollecting tokenizers<0.21,>=0.20 (from transformers)\n  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed tokenizers-0.20.0 transformers-4.45.1\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.0\nCollecting peft\n  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2+cpu)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\nSuccessfully installed accelerate-0.34.2\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting datasets\n  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\nSuccessfully installed datasets-3.0.1\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nCollecting scipy\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\nDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.4\n    Uninstalling scipy-1.11.4:\n      Successfully uninstalled scipy-1.11.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.14.1\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting trl\n  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.2+cpu)\nRequirement already satisfied: transformers>=4.40.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.45.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.34.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (3.0.1)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.5.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.4.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->trl) (4.66.4)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.40.0->trl) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, docstring-parser, tyro, trl\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed docstring-parser-0.16 shtab-1.7.1 trl-0.11.1 tyro-0.8.11\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=3668c15060dccd6247d897d6861edb1705563844ec867bf2048ffdec1e2772f4\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\nCollecting torch\n  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2+cpu\n    Uninstalling torch-2.1.2+cpu:\n      Successfully uninstalled torch-2.1.2+cpu\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.1 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.4.1 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.4.1 which is incompatible.\ntorchvision 0.16.2+cpu requires torch==2.1.2, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0\nCPU times: user 7.95 s, sys: 1.66 s, total: 9.62 s\nWall time: 5min 52s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport os\nimport shutil\nimport zipfile\nimport gc\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport multiprocessing\nimport psutil\nimport requests\nimport tarfile\nimport json\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom transformers.integrations import TensorBoardCallback\nfrom tqdm import tqdm\nfrom huggingface_hub import interpreter_login\nfrom pynvml import *\nfrom functools import partial\nfrom transformers import set_seed\nfrom datasets import load_dataset, DatasetDict\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom IPython.display import FileLink\nfrom urllib.request import urlopen\nfrom io import BytesIO\nfrom subprocess import Popen\nfrom os import chmod\nfrom os.path import isfile\n\n\n#interpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:28:27.172576Z","iopub.execute_input":"2024-09-28T17:28:27.173698Z","iopub.status.idle":"2024-09-28T17:28:27.186717Z","shell.execute_reply.started":"2024-09-28T17:28:27.173644Z","shell.execute_reply":"2024-09-28T17:28:27.184969Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"CPU times: user 485 µs, sys: 85 µs, total: 570 µs\nWall time: 579 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"# Habilitar los permisos necesarios para acceder a google-drive\nimport os\n\nPROJECT_NAME = 'drugs-generative'\n\ntry:\n    from google.colab import drive\n    ROOT = '/content/drive/'\n    drive.mount(ROOT, force_remount=True)\n    IN_COLAB = True\n    BASE_FOLDER = ROOT + 'MyDrive/' + PROJECT_NAME\n    DATASET_FOLDER = BASE_FOLDER\nexcept:\n    #ROOT = '/kaggle/input/drugs-data'\n    ROOT = '/kaggle'\n    IN_COLAB = False\n    BASE_FOLDER = os.path.join(\"/kaggle/working\", PROJECT_NAME)\n    DATASET_FOLDER = os.path.join(\"/kaggle/input\", PROJECT_NAME)\n    TOKENIZER_FOLDER = os.path.join(\"/kaggle/input\", 'drugs-tokenizer')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:28:30.298746Z","iopub.execute_input":"2024-09-28T17:28:30.299214Z","iopub.status.idle":"2024-09-28T17:28:30.308354Z","shell.execute_reply.started":"2024-09-28T17:28:30.299179Z","shell.execute_reply":"2024-09-28T17:28:30.306795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nUSE_ALL_DATASET = False\nNUMBER_ELEMENT = 1000\nSAVE_TOKENIZER = True\nLOAD_TOKENIZER = False\nPROCESS_SAMPLE = True\nNGROK_TOKEN = '2mfZzvcUfXHZqEB2Cc3REgZQ3eG_8a2WJJCc9vp9UpVV3AFVT'\nHUGGING_TOKEN = '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'\n\n#model_name='meta-llama/Meta-Llama-3-8B'\nmodel_name = 'meta-llama/Llama-2-7b-hf'\nname_zip_tokenizer = 'tokenizer.zip'\nlog_name_directory = 'logs'\nngrok_url = 'https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz'\ntrain_dataset = None\neval_dataset = None\nseed = 42\n\nDATASET_PATH = os.path.join(DATASET_FOLDER, \"drugs_data.parquet\")\nTOKENIZER_PATH = os.path.join(TOKENIZER_FOLDER, \"tokenizer.zip\")\nLOG_TRAIN_PATH = os.path.join(BASE_FOLDER, log_name_directory)\n\n# Valida directorio principal del proyecto\nif not(os.path.exists(BASE_FOLDER)):\n    !mkdir -p {BASE_FOLDER}\n    print('Directorio proyecto creado exitosamente!!')\n\n    \n# Valida directorio en donde se almacenan los logs del entrenamiento\nif not(os.path.exists(LOG_TRAIN_PATH)):\n    !mkdir -p {LOG_TRAIN_PATH}\n    print('Directorio para almacenar logs creado exitosamente!!')    \n    \n# Valida descarga dataset del Proyecto\nif not (os.path.exists(DATASET_PATH)):\n    print('Dataset no existe!!')\n\n    \n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = LOG_TRAIN_PATH\n\n# Number of training epochs\nnum_train_epochs = 35\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 100\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:28:38.434646Z","iopub.execute_input":"2024-09-28T17:28:38.436026Z","iopub.status.idle":"2024-09-28T17:28:40.853597Z","shell.execute_reply.started":"2024-09-28T17:28:38.435982Z","shell.execute_reply":"2024-09-28T17:28:40.851760Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Directorio proyecto creado exitosamente!!\nDirectorio para almacenar logs creado exitosamente!!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2'></a>\n#### 2. Definición de Funciones ","metadata":{}},{"cell_type":"code","source":"# Funcion para imprimir la utilización de la memoria de la GPU\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n\n\n# Función para reemplazar NaN con cadena vacía\ndef replace_nan_with_empty_string(example):\n    for key, value in example.items():\n        if value is None or pd.isna(value) or (value == 'nan'):\n            example[key] = ''\n    return example\n\n\ndef create_prompt_formats_llama3(sample):\n    '''\n    \n    '''\n    #===========================================================================================\n    try:\n        # Construir las partes iniciales\n        instruct_key = '### Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available.\"'\n        context_key = '### Context: You are a pharmaceutical chemist specialized in the in-depth understanding of drug descriptions. Your task is to generate a professional and accurate response based on the information provided. If a specific field lacks information, state \"No specific information available\" instead of providing unconfirmed details.'\n        input_key = f\"### Input: Provide a detailed description of the medication {sample.get('generic_name', '')} using the available data.\"\n        end_key = \"### End\"\n\n        # Lista de campos a procesar\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                drugs.append(f'<{field}> {label_name}: {field_value} </{field}>')\n                if question_text:\n                    questions.append(f'<question> {question_text}</question><answer> {field_value}</answer>')\n\n        # Construir las partes finales\n        output_key = f\"### Output: {sample.get('description', '')}\"\n        if drugs:\n            output_key += \"\\n\" + \"\\n\".join(drugs)\n\n        question_key = '### Questions: ' + (\"\\n\".join(questions) if questions else \"\")\n\n        # Construir el texto final\n        parts = [instruct_key, context_key, input_key, output_key, question_key, end_key]\n        sample[\"text\"] = \"\\n\\n\".join(parts)\n\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    return sample\n\ndef create_prompt_formats_llama2(sample):\n    '''\n\n    '''\n    #===========================================================================================\n    try:\n        # Lista de campos a procesar: campo en el dataset, nombre a mostrar, pregunta asociada\n        fields = [\n            (\"brand_name\", \"Brand Name\", \"What is the brand name of the medication?\"),\n            (\"generic_name\", \"Generic Name\", \"What is the generic name of the medication?\"),\n            (\"substance_name\", \"Active Ingredient\", \"What is the active ingredient of the medication?\"),\n            (\"manufacturer_name\", \"Manufacturer Name\", \"Who is the manufacturer of the medication?\"),\n            (\"product_type\", \"Product Type\", None),\n            (\"route\", \"Route of Administration\", None),\n            (\"dosage_and_administration\", \"Dosage and Administration\", \"What is the recommended dosage for this medication?\"),\n            (\"indications_and_usage\", \"Indications and Usage\", \"What is this medication used for?\"),\n            (\"contraindications\", \"Contraindications\", \"What are the contraindications of the medication?\"),\n            (\"warnings\", \"Warnings\", \"What warnings are associated with this medication?\"),\n            (\"precautions\", \"Precautions\", None),\n            (\"adverse_reactions\", \"Adverse Reactions\", \"What adverse reactions are associated with this medication?\"),\n            (\"controlled_substance\", \"Controlled Substance\", None),\n            (\"active_ingredient\", \"Chemical Substance\", None),\n            (\"last_update\", \"Last Update\", None)\n        ]\n\n        drugs = []\n        questions = []\n\n        # Procesar los campos y construir las secciones de descripción y preguntas/respuestas\n        for field, label_name, question_text in fields:\n            field_value = sample.get(field)\n            if field_value:\n                # Añadir al bloque de descripción del medicamento en formato simple\n                drugs.append(f'{label_name}: {field_value}')\n                # Si hay una pregunta asociada al campo, añadirla también\n                if question_text:\n                    questions.append(f'Question: {question_text}\\nAnswer: {field_value}')\n\n        # Mensaje del sistema con el prompt mejorado\n        system_message = f\"\"\"You are a helpful Medical Assistant. Your task is to generate descriptions of medications or respond to questions related to them, depending on the user's request.\n\n        If the user requests a **medication description**, follow this structure:\n        - Brand Name: [Brand Name]\n        - Generic Name: [Generic Name]\n        - Active Ingredient: [Active ingredients]\n        - Indications: [Uses]\n        - Dosage: [Recommended dosage]\n        - Side Effects: [Common side effects]\n        - Contraindications: [When the medication should not be used]\n        - Interactions: [Drugs or substances that interact with this medication]\n\n        If the user asks a **direct question about the medication**, answer based on the provided information and the medication's context.\n\n        Please use only the available information in the provided context.\n\n        Guidelines:\n        - Maintain a professional, precise, and concise tone in all responses.\n        - Do not fabricate information. If a field lacks data, state \"No specific information available.\"\n        - Ensure the information is understandable for both healthcare professionals and patients.\n        \"\"\"\n\n        # Construir la descripción y preguntas de manera directa sin etiquetas adicionales\n        description = \"\\n\".join(drugs)\n\n        # Agregar las preguntas si hay\n        if questions:\n            questions_block = \"\\n\\n\".join(questions)\n        else:\n            questions_block = \"No additional questions provided.\"\n\n        # Crear el prompt completo combinando todo dentro de [INST]\n        sample['text'] = f\"\"\"<s>[INST] <<SYS>>\n        {system_message}\n        <</SYS>>\n        {description}\n\n        {questions_block}\n\n        [/INST]</s>\"\"\"\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al cargar el prompt [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return sample\n    \n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )\n\n    \ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        create_prompt_formats = None\n        if(model_name == 'meta-llama/Meta-Llama-3-8B'):\n            print(\"Create prompt llama3...\")\n            create_prompt_formats = create_prompt_formats_llama3\n        elif(model_name == 'meta-llama/Llama-2-7b-hf'):\n            print(\"Create prompt llama2...\")\n            create_prompt_formats = create_prompt_formats_llama2\n        \n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        _preprocessing_function = partial(preprocess_batch,\n                                          max_length = max_length,\n                                          tokenizer = tokenizer\n                                         )\n\n        dataset = dataset.map(_preprocessing_function, \n                              remove_columns=[col for col in dataset.column_names if col != \"text\"],\n                              #num_proc=num_proc\n                             )\n\n        # Filtrar las muestras que tienen input_ids que exceden la longitud máxima (max_length).\n        dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n\n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef preprocess_dataset_sample(seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n        \n        create_prompt_formats = None\n        if(model_name == 'meta-llama/Meta-Llama-3-8B'):\n            print(\"Create prompt llama3...\")\n            create_prompt_formats = create_prompt_formats_llama3\n        elif(model_name == 'meta-llama/Llama-2-7b-hf'):\n            print(\"Create prompt llama2...\")\n            create_prompt_formats = create_prompt_formats_llama2\n        \n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n        \n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n        \n        dataset = dataset.map(create_prompt_formats\n                              #num_proc=num_proc\n                             )#, batched=True)\n        \n        # Shuffle dataset\n        dataset = dataset.shuffle(seed=seed)\n\n        return dataset\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n        \ndef print_number_of_trainable_model_parameters(model):\n    try:\n        trainable_model_params = 0\n        all_model_params = 0\n        for _, param in model.named_parameters():\n            all_model_params += param.numel()\n            if param.requires_grad:\n                trainable_model_params += param.numel()\n        return f\"all model parameters: {all_model_params}\\ntrainable model parameters: {trainable_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n    except Exception as ex:\n        print(f'Ocurrió un error inesperado al imprimir los parametros del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n\ndef launch_tensorboard():\n    tb_process, ngrok_process = None, None\n    \n    # Launch TensorBoard\n    if not is_process_running('tensorboard'):\n        tb_command = f'tensorboard --logdir {LOG_TRAIN_PATH}/ --host 0.0.0.0 --port 6006'\n        tb_process = run_cmd_async_unsafe(tb_command)\n    \n    # Install ngrok\n    if not isfile('./ngrok'):\n        #ngrok_url = 'https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\n        print('Inicia descarga de ngrok....')\n        download_and_extract(ngrok_url)\n        chmod('./ngrok', 0o755)\n        \n        #Registra token de autorizacion\n        tb_command = f'./ngrok config add-authtoken {NGROK_TOKEN}'\n        tb_process = run_cmd_async_unsafe(tb_command)\n\n    # Create ngrok tunnel and print its public URL\n    if not is_process_running('ngrok'):\n        ngrok_process = run_cmd_async_unsafe('./ngrok http 6006')\n        time.sleep(1) # Waiting for ngrok to start the tunnel\n    \n    ngrok_api_res = urlopen('http://127.0.0.1:4040/api/tunnels', timeout=10)\n    ngrok_api_res = json.load(ngrok_api_res)\n    assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n    tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n    print(f'TensorBoard URL: {tb_public_url}')\n\n    return tb_process, ngrok_process\n\n\ndef download_and_extract(url, extract_to='.'):\n    try:\n        # Descargar el archivo\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Lanza una excepción si la respuesta tiene un error\n\n        # Detectar el tipo de archivo a partir de la URL\n        if url.endswith('.zip'):\n            # Si es un archivo ZIP, utilizar ZipFile\n            with ZipFile(BytesIO(response.content)) as zip_file:\n                zip_file.extractall(path=extract_to)\n                print(f'Archivo ZIP extraído en: {os.path.abspath(extract_to)}')\n\n        elif url.endswith('.tgz') or url.endswith('.tar.gz'):\n            # Si es un archivo .tgz o .tar.gz, utilizar tarfile\n            with tarfile.open(fileobj=BytesIO(response.content), mode='r:gz') as tar_file:\n                tar_file.extractall(path=extract_to)\n                print(f'Archivo TGZ extraído en: {os.path.abspath(extract_to)}')\n\n        else:\n            print(\"Formato de archivo no soportado.\")\n            return\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error en la descarga: {e}\")\n    except Exception as e:\n        print(f\"Ocurrió un error inesperado: {e}, {e.__traceback__.tb_lineno}\")\n\n\ndef run_cmd_async_unsafe(cmd):\n    return Popen(cmd, shell=True)\n\n\ndef is_process_running(process_name):\n    running_process_names = (proc.name() for proc in psutil.process_iter())\n    return process_name in running_process_names\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:29:14.560089Z","iopub.execute_input":"2024-09-28T17:29:14.560552Z","iopub.status.idle":"2024-09-28T17:29:14.613320Z","shell.execute_reply.started":"2024-09-28T17:29:14.560504Z","shell.execute_reply":"2024-09-28T17:29:14.611342Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class ModelAnalizer:\n    '''\n    '''\n    \n    def __init__(self, model_name_or_path):\n        self.model_name_or_path = model_name_or_path\n        self.model = None\n        self.tokenizer = None\n        self._load_qtz_config()\n    \n    \n    def _load_qtz_config(self):\n        try:\n            compute_dtype = getattr(torch, \"float16\")\n            self.bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n                                                 bnb_4bit_quant_type='nf4',\n                                                 bnb_4bit_compute_dtype=compute_dtype,\n                                                 bnb_4bit_use_double_quant=True,\n                                                )\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar quantization-config [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n        \n    def _load_model(self):\n        try:\n            device_map = {\"\": 0}\n            self.model = AutoModelForCausalLM.from_pretrained(self.model_name_or_path, \n                                                              device_map=device_map,\n                                                              quantization_config=self.bnb_config,\n                                                              trust_remote_code=True,\n                                                              token=HUGGING_TOKEN\n                                                              )\n            # Carga el tokenizador\n            self._tokenizer()\n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n            \n    \n    def _tokenizer(self):\n        # https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n        try:\n            print(f'self.model_name_or_path : {self.model_name_or_path}')\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, \n                                                          trust_remote_code=True, \n                                                          add_bos_token=True,\n                                                          use_fast=False, \n                                                          add_eos_token=True, \n                                                          padding_side=\"left\",\n                                                          token=HUGGING_TOKEN\n                                                         )\n            if not(self.tokenizer):\n                raise Exception(f'No se ha definido el atributo self.tokenizer')\n            \n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al cargar el tokenizador [line: {ex.__traceback__.tb_lineno}] - {ex}')\n        \n    \n    \n    def gen(self, prompt, maxlen=512, sample=True):\n        try:\n            '''\n            eval_tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path,\n                                                           trust_remote_code=True,\n                                                           add_bos_token=True,\n                                                           use_fast=False\n                                                          )\n            eval_tokenizer.pad_token = eval_tokenizer.eos_token\n            \n            toks = eval_tokenizer(p, return_tensors=\"pt\")\n            '''\n            \n            toks = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n            res = self.model.generate(**toks.to(\"cuda\"), \n                                      max_new_tokens=maxlen,\n                                      do_sample=sample,\n                                      num_return_sequences=1,\n                                      temperature=0.7,\n                                      num_beams=1,\n                                      top_p=0.95\n                                     ).to('cpu')\n            return self.tokenizer.batch_decode(res, skip_special_tokens=True)\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al procesar la inferencia en el modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n    \n    def get_max_length(self):\n        try:\n            max_length = None\n            for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n                max_length = getattr(self.model.config, length_setting, None)\n                if max_length:\n                    print(f\"Found max length: {max_length}\")\n                    break\n            if not max_length:\n                max_length = 1024\n                print(f\"Using default max length: {max_length}\")\n            return max_length\n        \n        except Exception as ex:\n            raise Exception(f'Ocurrió un error inesperado al obtener tamaño del modelo [line: {ex.__traceback__.tb_lineno}] - {ex}')\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:29:20.807722Z","iopub.execute_input":"2024-09-28T17:29:20.808189Z","iopub.status.idle":"2024-09-28T17:29:20.830171Z","shell.execute_reply.started":"2024-09-28T17:29:20.808146Z","shell.execute_reply":"2024-09-28T17:29:20.828565Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<a name='3'></a>\n#### 3. Cargar el dataset","metadata":{}},{"cell_type":"code","source":"%%time\n#Cargar tu dataset\ndataset = load_dataset('parquet', data_files=DATASET_PATH)\n\n\n# Tomar una muestra aleatoria de x cantidad de registros de forma aleatoria)\nif not(USE_ALL_DATASET):\n    sampled_dataset = dataset['train'].shuffle(seed=42).select(range(NUMBER_ELEMENT))\nelse:\n    sampled_dataset = dataset['train']\n\n# Dividir en 70% train y 30% (test + validation)\ntrain_test_valid = sampled_dataset.train_test_split(test_size=0.3, seed=42)\n\n# Dividir el 30% restante en 15% test y 15% validation\ntest_valid = train_test_valid['test'].train_test_split(test_size=0.5, seed=42)\n\n# Reunir los conjuntos en un DatasetDict\ndataset = DatasetDict({\n    'train': train_test_valid['train'],\n    'test': test_valid['test'],\n    'validation': test_valid['train']\n})\n\ndataset\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:29:24.815604Z","iopub.execute_input":"2024-09-28T17:29:24.816086Z","iopub.status.idle":"2024-09-28T17:29:46.824110Z","shell.execute_reply.started":"2024-09-28T17:29:24.816049Z","shell.execute_reply":"2024-09-28T17:29:46.822959Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af82e76319447be940e4e84648344f7"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 20.7 s, sys: 7.8 s, total: 28.5 s\nWall time: 22 s\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 700\n    })\n    test: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 150\n    })\n    validation: Dataset({\n        features: ['abuse', 'abuse_table', 'active_ingredient', 'active_ingredient_table', 'adverse_reactions', 'adverse_reactions_table', 'alarms', 'ask_doctor_or_pharmacist', 'ask_doctor_or_pharmacist_table', 'ask_doctor_table', 'brand_name', 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility', 'clinical_pharmacology_table', 'clinical_studies_table', 'components_table', 'contraindications', 'controlled_substance', 'dependence', 'dependence_table', 'description', 'description_table', 'do_not_use_table', 'dosage_and_administration', 'dosage_and_administration_table', 'drug_abuse_and_dependence', 'drug_abuse_and_dependence_table', 'drug_and_or_laboratory_test_interactions_table', 'drug_interactions_table', 'effective_time', 'general_precautions', 'general_precautions_table', 'generic_name', 'geriatric_use', 'how_supplied_table', 'id', 'inactive_ingredient', 'indications_and_usage', 'indications_and_usage_table', 'information_for_patients_table', 'instructions_for_use', 'labor_and_delivery', 'labor_and_delivery_table', 'laboratory_tests', 'laboratory_tests_table', 'last_updated', 'mechanism_of_action_table', 'microbiology_table', 'nonteratogenic_effects', 'nursing_mothers', 'nursing_mothers_table', 'overdosage', 'overdosage_table', 'package_label_principal_display_panel', 'patient_medication_information', 'pediatric_use_table', 'pharmacodynamics_table', 'pharmacogenomics_table', 'pharmacokinetics_table', 'precautions', 'precautions_table', 'pregnancy', 'pregnancy_or_breast_feeding', 'pregnancy_or_breast_feeding_table', 'pregnancy_table', 'product_type', 'purpose_table', 'questions_table', 'recent_major_changes', 'recent_major_changes_table', 'risks', 'risks_table', 'spl_medguide', 'spl_patient_package_insert', 'statement_of_identity', 'stop_use_table', 'storage_and_handling', 'storage_and_handling_table', 'substance_name', 'summary_of_safety_and_effectiveness', 'teratogenic_effects', 'use_in_specific_populations', 'use_in_specific_populations_table', 'user_safety_warnings', 'warnings', 'warnings_and_cautions_table', 'when_using', 'when_using_table'],\n        num_rows: 150\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][120]","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:30:04.724723Z","iopub.execute_input":"2024-09-28T17:30:04.729334Z","iopub.status.idle":"2024-09-28T17:30:04.797123Z","shell.execute_reply.started":"2024-09-28T17:30:04.729253Z","shell.execute_reply":"2024-09-28T17:30:04.792738Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'abuse': '',\n 'abuse_table': '',\n 'active_ingredient': '',\n 'active_ingredient_table': '',\n 'adverse_reactions': '6 ADVERSE REACTIONS The following serious adverse reaction is also described elsewhere in the labeling: Hypoglycemia [see Warnings and Precautions ( 5.1 )] Common adverse reactions associated with nateglinide (3% or greater incidence) were upper respiratory tract infection, back pain, flu symptoms, dizziness, arthropathy, diarrhea. ( 6.1 ) To report SUSPECTED ADVERSE REACTIONS, contact Zydus Pharmaceuticals (USA) Inc. at 1-877-993-8779 or FDA at 1-800-FDA-1088 or ww w . f da . g o v / m e dw a t c h. 6.1 Clinical Trials Experience Because clinical trials are conducted under widely varying conditions, adverse reaction rates observed in the clinical trials of a drug cannot be directly compared to rates in the clinical trials of another drug and may not reflect the rates observed in practice. In clinical trials, approximately 2,600 patients with type 2 diabetes mellitus were treated with nateglinide. Of these, approximately 1,335 patients were treated for 6 months or longer and approximately 190 patients for one year or longer. Table 1 shows the most common adverse reactions associated with nateglinide. Table 1 Adverse Reactions other than Hypoglycemia (%) occurring Greater than or Equal to 2% in Nateglinide-Treated Patients from Pool of 12 to 64 week Placebo Controlled Trials Placebo Nateglinide N=458 N=1,441 Preferred Term Upper Respiratory Infection 8.1 10.5 Back Pain 3.7 4 Flu Symptoms 2.6 3.6 Dizziness 2.2 3.6 Arthropathy 2.2 3.3 Diarrhea 3.1 3.2 Accidental Trauma 1.7 2.9 Bronchitis 2.6 2.7 Coughing 2.2 2.4 Hypoglycemia Episodes of severe hypoglycemia (plasma glucose less than 36 mg/dL) were reported in two patients treated with nateglinide. Non-severe hypoglycemia occurred in 2.4 % of nateglinide treated patients and 0.4 % of placebo treated patients [see Warnings and Precautions ( 5.1 )]. Weight Gain Patients treated with nateglinide had statistically significant mean increases in weight compared to placebo. In clinical trials, the mean weight increases with nateglinide 60 mg (3 times daily) and nateglinide 120 mg (3 times daily) compared to placebo were 1kg and 1.6 kg respectively. Laboratory Test Increases in Uric Acid : There were increases in mean uric acid levels for patients treated with nateglinide alone, nateglinide in combination with metformin, metformin alone, and glyburide alone. The respective differences from placebo were 0.29 mg/dL, 0.45 mg/dL, 0.28 mg/dL, and 0.19 mg/dL. 6.2 Postmarketing Experience The following adverse reactions have been identified during post-approval use of nateglinide. Because these reactions are reported voluntarily from a population of uncertain size, it is not always possible to reliably estimate their frequency or establish a causal relationship to drug exposure. Hypersensitivity Reactions: Rash, itching, and urticaria Hepatobiliary Disorders: Jaundice, cholestatic hepatitis, and elevated liver enzymes',\n 'adverse_reactions_table': ' Table 1 Adverse Reactions other than Hypoglycemia (%) occurring Greater than or Equal to 2% in Nateglinide-Treated Patients from Pool of 12 to 64 week Placebo Controlled Trials  Placebo  Nateglinide N=458 N=1,441  Preferred Term  Upper Respiratory Infection  8.1  10.5  Back Pain  3.7  4  Flu Symptoms  2.6  3.6  Dizziness  2.2  3.6  Arthropathy  2.2  3.3  Diarrhea  3.1  3.2  Accidental Trauma  1.7  2.9  Bronchitis  2.6  2.7  Coughing  2.2  2.4 ',\n 'alarms': '',\n 'ask_doctor_or_pharmacist': '',\n 'ask_doctor_or_pharmacist_table': '',\n 'ask_doctor_table': '',\n 'brand_name': 'Nateglinide',\n 'carcinogenesis_and_mutagenesis_and_impairment_of_fertility': '13.1 Carcinogenesis, Mutagenesis, Impairment of Fertility Carcinogenicity: Nateglinide did not increase tumors in two year carcinogenicity studies conducted in mice and rats. Oral doses of Nateglinide up to 900 mg/kg in rats and 400 mg/kg in mice were tested, which produced exposures in rats approximately 30 times to 40 times and in mice 10 times to 30 times the human therapeutic exposure of nateglinide at a dose of 120 mg three times daily, based on AUC. Mutagenesis: Nateglinide was not genotoxic in the in vitro Ames test, mouse lymphoma assay, chromosome aberration assay or in the in vivo mouse micronucleus test. Impairment of Fertility: Fertility was unaffected by administration of nateglinide to rats at doses up to 600 mg/kg (corresponding to 16 times the MRHD of 120 mg three times per day, based on BSA).',\n 'clinical_pharmacology_table': ' Table 3 Effect of Coadministered drugs on Pharmacokinetics of Nateglinide  Coadministered drug  Dosing regimen of  coadministered drug  Dosing regimen of  nateglinide  Change in Cmax  Change in AUC  Glyburide  10 mg once daily for 3 weeks  120 mg three times a day, single dose  8.78% &#x2193;  3.53 % &#x2193;  Metformin  500 mg three times a day for 3 weeks  120 mg three times a day, single dose  AM: 7.14% &#x2191;  PM: 11.4% &#x2193;  AM: 1.51% &#x2191;  PM: 5.97% &#x2191;  Digoxin  1 mg, single dose  120 mg three times a day, single dose  AM: 2.17% &#x2193;  PM: 3.19% &#x2191;  AM: 7.62% &#x2191;  PM: 2.22% &#x2191;  Warfarin  30 mg, single dose  120 mg three times a day for 4 days  2.65% &#x2191;  3.72% &#x2193;  Diclofenac  75 mg, single dose  120 mg twice daily, single dose  AM: 13.23% &#x2193;  *PM: 3.76% &#x2191;  AM: 2.2% &#x2193;  *PM: 7.5% &#x2191;   Table 4 Effect of Nateglinide on Pharmacokinetics of Coadministered Drugs  Coadministered drug  Dosing regimen of  coadministered drug  Dosing regimen of  nateglinide  Change in Cmax  Change in AUC  Glyburide  10 mg once daily for 3  weeks  120 mg three times a day, single dose  3.18% &#x2193;  7.34 % &#x2193;  Metformin  500 mg three times a  day for 3 weeks  120 mg three times a day, single dose  AM: 10.7% &#x2191;  PM: 0.40% &#x2193;  AM: 13.3% &#x2191;  PM: 2.27% &#x2193;  Digoxin  1 mg, single dose  120 mg three times a day, single dose  5.41% &#x2193;  6.58% &#x2191;  Warfarin  30 mg, single dose  120 mg three times a day for 4 days  R-Warfarin: 1.03%&#x2193;  S-Warfarin: 0.85%&#x2193;  R-Warfarin: 0.74%&#x2191;  S-Warfarin: 7.23%&#x2191;  Diclofenac  75 mg, single dose  120 mg twice daily, single dose  2.19% &#x2191;  7.97% &#x2191; ',\n 'clinical_studies_table': ' Table 5 Endpoint results for a 24-week, fixed dose study of Nateglinide monotherapy  Placebo  Nateglinide  Nateglinide  60 mg  120 mg  three times  daily  before meals  three times  daily  before meals  HbA1C (%) N=168 N=167 N=168  Baseline (mean)  8.0  7.9  8.1  Change from baseline (mean)  +0.2  -0.3  -0.5  Difference from placebo (mean)  -0.5a  -0.7a  FPG (mg/dL) N=172 N=171 N=169  Baseline (mean)  167.9  161.0  166.5  Change from baseline (mean)  +9.1  +0.4  -4.5  Difference from placebo (mean)  -8.7a  -13.6a   Table 6 Endpoint results for a 24-week study of Nateglinide monotherapy compared to Glyburide a p-value &lt;0.001 Glyburide  Nateglinide  Nateglinide  10 mg  60 mg  120 mg  Once daily  three times  daily  before meals  three times  daily  before meals  HbA1C (%) N=183 N=178 N=179  Baseline (mean)  7.8  8  7.9  Change from baseline (mean)  0.3  1.3  1.1  Difference from glyburide (mean)  1a  0.9a  FPG (mmol/L) N=184 N=182 N=180  Baseline (mean)  9.44  9.67  9.61  Change from baseline (mean)  0.19  3.06  2.84  Difference from glyburide (mean)  2.87a  2.66a   Table 7 Endpoint results for a 24-week study of Nateglinide monotherapy and combination with Metformin ap-value &#x2264; 0.05 vs. placebobp-value &#x2264; 0.03 vs. metformincp-value &#x2264; 0.05 vs. combination* Metformin was administered three times daily Placebo  Nateglinide  Metformin  Nateglinide  120 mg  500 mg  120 mg  three times  daily before  meals  three times  daily  before  meals plus  Metformin*  HbA1C (%)  All N=160 N=171 N=172 N=162  Baseline (mean)  8.3  8.3  8.4  8.4  Change from baseline (mean)  +0.4  -0.4bc  -0.8c  -1.5  Difference from placebo  -0.8a  -1.2a  -1.9a  Na&#x457;ve N=98 N=99 N=98 N=81  Baseline (mean)  8.2  8.1  8.3  8.2  Change from baseline (mean)  +0.3  -0.7c  -0.8c  -1.6  Difference from placebo  -1a  -1.1a  -1.9a  Non- Na&#x457;ve N=62 N=72 N=74 N=81  Baseline (mean)  8.3  8.5  8.7  8.7  Change from baseline (mean)  +0.6  +0.004bc  -0.8c  -1.4  Difference from placebo  -0.6a  -1.4a  -2a  FPG (mg/dL)  All N=166 N=173 N=174 N=167  Baseline (mean)  194  196.5  196  197.7  Change from baseline (mean)  +8  -13.1bc  -30c  -44.9  Difference from placebo  -21.1a  -38a  -52.9a   Table 8 Endpoint results for a 24-week study of Nateglinide monotherapy as add-on to Metformin a p-value 0.003 vs. metforminb p-value &lt; 0.001 vs. metforminAll nateglinide/placebo taken three times daily before meals; all metformin 1,000 mg twice daily. Placebo  Nateglinide  60 mg  Nateglinide  120 mg  +  +  +  metformin  metformin  metformin  HbA1C (%) N=150 N=152 N=154  Baseline (mean)  8.2  8  8.2  Change from baseline (mean)  0.01  -0.4  -0.6  Difference from metformin  -0.4a  -0.6b   Table 9 Endpoint results for a 24-week study of the effect of adding Nateglinide or placebo to Rosiglitazone  Placebo +  Nateglinide  120 mg  before meals +  rosiglitazone  8 mg  once daily  rosiglitazone  8 mg  once daily  HbA1C (%) N=191 N=194  Baseline (mean)  8.4  8.3  Change from baseline (mean)  0.03  -0.7  Difference from rosiglitazone (mean)  -0.7a ap-value &#x2264; 0.0001   Table 10 Endpoint results for a 12-week study of the effect of adding Nateglinide or placebo to Glyburide Placebo or nateglinide given 10 minutes prior to breakfast, lunch, and dinner; glyburide given with the breakfast dose of nateglinide or placebo.ap-value 0.6959bp-value 0.1246 Placebo +  Nateglinide  60 mg  before meals +  Nateglinide  120 mg  before meals +  glyburide  10 mg  once daily  glyburide  10 mg  once daily  glyburide  10 mg  once daily  HbA1C (%) N=58 N=55 N=54  Baseline (mean)  8.7  8.7  8.7  Change from baseline (mean)  0.3  0.2  -0.02  Difference from glyburide (mean)  -0.1a  -0.3b ',\n 'components_table': '',\n 'contraindications': '4 CONTRAINDICATIONS Nateglinide is contraindicated in patients with a history of hypersensitivity to nateglinide or its inactive ingredients. History of hypersensitivity to nateglinide or its inactive ingredients ( 4 )',\n 'controlled_substance': '',\n 'dependence': '',\n 'dependence_table': '',\n 'description': '11 DESCRIPTION Nateglinide is an oral blood glucose-lowering drug of the glinide class. Nateglinide, (-)-N-[(trans-4- isopropylcyclohexane)carbonyl]-D-phenylalanine, is structurally unrelated to the oral sulfonylurea insulin secretagogues. The structural formula is as shown: Nateglinide, USP is a white powder with a molecular weight of 317.43 g/mol. It is freely soluble in methanol and alcohol, soluble in ether, sparingly soluble in acetonitrile and octanol, practically insoluble in water. Each nateglinide tablet, USP intended for oral administration contains nateglinide, USP 60 mg and 120 mg. In addition, each tablet contains the following inactive ingredients: citric acid anhydrous, colloidal silicon dioxide, corn starch, crospovidone, hypromellose, lactose monohydrate, magnesium stearate, polysorbate 80, polyethylene glycol, povidone, talc and titanium dioxide. Additionally, each 120 mg tablet contains iron oxide red and iron oxide yellow. Image',\n 'description_table': '',\n 'do_not_use_table': '',\n 'dosage_and_administration': '2 DOSAGE AND ADMINISTRATION The recommended dose of nateglinide tablets is 120 mg orally three times daily before meals. The recommended dose of nateglinide tablets is 60 mg orally three times daily before meals in patients who are near glycemic goal when treatment is initiated. Instruct patients to take nateglinide tablets 1 to 30 minutes before meals. In patients who skip meals, instruct patients to skip the scheduled dose of nateglinide tablets to reduce the risk of hypoglycemia [see Warnings and Precautions ( 5.1 )] . Recommended dose is 120 mg three times daily. ( 2 ) In patients who are near glycemic goal when treatment is initiated, 60 mg three times daily may be administered. ( 2 ) Administer 1 to 30 minutes before meals. ( 2 ) If a meal is skipped, skip the scheduled dose to reduce the risk of hypoglycemia. ( 2 , 5.1 )',\n 'dosage_and_administration_table': '',\n 'drug_abuse_and_dependence': '',\n 'drug_abuse_and_dependence_table': '',\n 'drug_and_or_laboratory_test_interactions_table': '',\n 'drug_interactions_table': ' Table 2 Clinically Significant Drug Interactions with Nateglinide  Drugs That May Increase the Blood-Glucose-Lowering Effect of Nateglinide and Susceptibility to Hypoglycemia Drugs:  Nonsteroidal anti-inflammatory drugs (NSAIDs), salicylates,  monoamine oxidase inhibitors, non-selective beta-adrenergic-blocking agents, anabolic hormones (e.g. methandrostenolone), guanethidine, gymnema sylvestre, glucomannan, thioctic acid, and inhibitors of CYP2C9 (e.g. amiodarone, fluconazole, voriconazole, sulfinpyrazone), or in patients known to be poor metabolizers of CYP2C9 substrates,alcohol. Intervention:  Dose reductions and increased frequency of glucose monitoring may be required when nateglinide is coadministered with these drugs.  Drugs and Herbals That May Reduce the Blood-Glucose-Lowering Effect of  Nateglinide and Increase Susceptibility to Hyperglycemia Drugs:  Thiazides, corticosteroids, thyroid products, sympathomimetics, somatropin, somatostatin analogues (e.g. lanreotide, octreotide), and CYP inducers (e.g. rifampin, phenytoin and St John&apos;s Wort). Intervention:  Dose increases and increased frequency of glucose monitoring may be required when nateglinide is coadministered with these drugs.  Drugs That May Blunt Signs and Symptoms of Hypoglycemia Drugs:  beta-blockers, clonidine, guanethidine, and reserpine Intervention:  Increased frequency of glucose monitoring may be required when nateglinide is co-administered with these drugs. ',\n 'effective_time': '20231107',\n 'general_precautions': '',\n 'general_precautions_table': '',\n 'generic_name': 'NATEGLINIDE',\n 'geriatric_use': '8.5 Geriatric Use 436 patients 65 years and older, and 80 patients 75 years and older were exposed to nateglinide in clinical studies. No differences were observed in safety or efficacy of nateglinide between patients age 65 and over, and those under age 65. However, greater sensitivity of some older individuals to nateglinide therapy cannot be ruled out.',\n 'how_supplied_table': '',\n 'id': '061a2260-2804-4753-b443-17943eca74dd',\n 'inactive_ingredient': '',\n 'indications_and_usage': '1 INDICATIONS AND USAGE Nateglinide tablets are indicated as an adjunct to diet and exercise to improve glycemic control in adults with type 2 diabetes mellitus. Limitations of Use: Nateglinide tablets should not be used in patients with type 1 diabetes mellitus or for the treatment of diabetic ketoacidosis. Nateglinide is a glinide indicated as an adjunct to diet and exercise to improve glycemic control in adults with type 2 diabetes mellitus. ( 1 ) Limitations of use : Not for treating type 1 diabetes mellitus or diabetes ketoacidosis ( 1 )',\n 'indications_and_usage_table': '',\n 'information_for_patients_table': '',\n 'instructions_for_use': '',\n 'labor_and_delivery': '',\n 'labor_and_delivery_table': '',\n 'laboratory_tests': '',\n 'laboratory_tests_table': '',\n 'last_updated': '2024-09-21',\n 'mechanism_of_action_table': '',\n 'microbiology_table': '',\n 'nonteratogenic_effects': '',\n 'nursing_mothers': '8.2 Lactation Risk summary There are no data on the presence of nateglinide in human milk, the effects on the breastfeeding infant, or the effects on milk production. The drug is present in animal milk. When a drug is present in animal milk, it is likely that the drug will be present in human milk (see Data). Because the potential for hypoglycemia in breast-fed infants, advise women that use of nateglinide is not recommended while breastfeeding. Data In rat reproduction studies, nateglinide and its metabolite are excreted in the milk following oral dose (300 mg/kg). The overall milk: plasma (M/P) concentration ratio of the total radioactivity was approximately 1.4 based on AUC0-48 values. The M/P ratio of unchanged nateglinide was approximately 2.2.',\n 'nursing_mothers_table': '',\n 'overdosage': '10 OVERDOSAGE There have been no instances of overdose with nateglinide in clinical trials. However, an overdose may result in an exaggerated glucose-lowering effect with the development of hypoglycemic symptoms. Hypoglycemic symptoms without loss of consciousness or neurological findings should be treated with oral glucose and adjustments in dosage and/or meal patterns. Severe hypoglycemic reactions with coma, seizure, or other neurological symptoms should be treated with intravenous glucose. As nateglinide is highly protein bound, dialysis is not an efficient means of removing it from the blood.',\n 'overdosage_table': '',\n 'package_label_principal_display_panel': 'PACKAGE LABEL.PRINCIPAL DISPLAY PANEL NDC 68382-721-16 in bottle of 90 tablets Nateglinide Tablets USP, 60 mg Rx only 90 tablets ZYDUS NDC 68382-722-16 in bottle of 90 tablets Nateglinide Tablets USP, 120 mg Rx only 90 tablets ZYDUS 60 mg 120 mg',\n 'patient_medication_information': '',\n 'pediatric_use_table': '',\n 'pharmacodynamics_table': '',\n 'pharmacogenomics_table': '',\n 'pharmacokinetics_table': ' Table 3 Effect of Coadministered drugs on Pharmacokinetics of Nateglinide  Coadministered drug  Dosing regimen of  coadministered drug  Dosing regimen of  nateglinide  Change in Cmax  Change in AUC  Glyburide  10 mg once daily for 3 weeks  120 mg three times a day, single dose  8.78% &#x2193;  3.53 % &#x2193;  Metformin  500 mg three times a day for 3 weeks  120 mg three times a day, single dose  AM: 7.14% &#x2191;  PM: 11.4% &#x2193;  AM: 1.51% &#x2191;  PM: 5.97% &#x2191;  Digoxin  1 mg, single dose  120 mg three times a day, single dose  AM: 2.17% &#x2193;  PM: 3.19% &#x2191;  AM: 7.62% &#x2191;  PM: 2.22% &#x2191;  Warfarin  30 mg, single dose  120 mg three times a day for 4 days  2.65% &#x2191;  3.72% &#x2193;  Diclofenac  75 mg, single dose  120 mg twice daily, single dose  AM: 13.23% &#x2193;  *PM: 3.76% &#x2191;  AM: 2.2% &#x2193;  *PM: 7.5% &#x2191;   Table 4 Effect of Nateglinide on Pharmacokinetics of Coadministered Drugs  Coadministered drug  Dosing regimen of  coadministered drug  Dosing regimen of  nateglinide  Change in Cmax  Change in AUC  Glyburide  10 mg once daily for 3  weeks  120 mg three times a day, single dose  3.18% &#x2193;  7.34 % &#x2193;  Metformin  500 mg three times a  day for 3 weeks  120 mg three times a day, single dose  AM: 10.7% &#x2191;  PM: 0.40% &#x2193;  AM: 13.3% &#x2191;  PM: 2.27% &#x2193;  Digoxin  1 mg, single dose  120 mg three times a day, single dose  5.41% &#x2193;  6.58% &#x2191;  Warfarin  30 mg, single dose  120 mg three times a day for 4 days  R-Warfarin: 1.03%&#x2193;  S-Warfarin: 0.85%&#x2193;  R-Warfarin: 0.74%&#x2191;  S-Warfarin: 7.23%&#x2191;  Diclofenac  75 mg, single dose  120 mg twice daily, single dose  2.19% &#x2191;  7.97% &#x2191; ',\n 'precautions': '',\n 'precautions_table': '',\n 'pregnancy': \"8.1 Pregnancy Risk Summary The available data from published literature and the applicant's pharmacovigilance with use of nateglinide in pregnant women are insufficient to identify a drug-associated risk of major birth defects, miscarriage or other adverse maternal or fetal outcomes. There are risks to the mother and fetus associated with poorly controlled diabetes in pregnancy (see Clinical Considerations) . Nateglinide should be used during pregnancy only if the potential benefit justifies the potential risk to the fetus. In animal reproduction studies, there was no teratogenicity in rats and rabbits administered oral nateglinide during organogenesis at approximately 27 and 8 times the maximum recommended human dose (MRHD), respectively, based on body surface area (BSA). The estimated background risk of major birth defects is 6% to 10% in women with pre-gestational diabetes with a HbA1c > 7 and has been reported to be as high as 20% to 25% in women with a HbA1c > 10. The estimated background risk of miscarriage for the indicated population is unknown. In the U.S. general population, the estimated background risk of major birth defects and miscarriage in clinically recognized pregnancies is 2% to 4% and 15% to 20%, respectively. Clinical Considerations Disease-Associated Maternal and/or Embryo/Fetal Risk Poorly controlled diabetes in pregnancy increases the maternal risk for diabetic ketoacidosis, pre-eclampsia, spontaneous abortions, preterm delivery, and delivery complications. Poorly controlled diabetes increases the fetal risk for major birth defects, stillbirth, and macrosomia related morbidity. Data Animal data In embryofetal development studies, nateglinide administered orally during the period of organogenesis was not teratogenic in rats at doses up to 1,000 mg/kg (corresponding to 27 times the MRHD of 120 mg three times per day, based on BSA). In rabbits, embryonic development was adversely affected at 500 mg/kg/day and the incidence of gallbladder agenesis or small gallbladder was increased at a dose of 300 and 500 mg/kg (corresponding to 16 and 27 times the MRHD). No such effects were observed at 150 mg/kg/day (corresponding to 8 times the MRHD). In a pre-and postnatal development study in rats, nateglinide administered by oral gavage at doses of 100, 300, and 1000 mg/kg/day from gestation day 17 to lactation day 21 resulted in lower body weight in offspring of rats administered nateglinide at 1,000 mg/kg/day (corresponding to 27 times the MHRD).\",\n 'pregnancy_or_breast_feeding': '',\n 'pregnancy_or_breast_feeding_table': '',\n 'pregnancy_table': '',\n 'product_type': 'HUMAN PRESCRIPTION DRUG',\n 'purpose_table': '',\n 'questions_table': '',\n 'recent_major_changes': '',\n 'recent_major_changes_table': '',\n 'risks': '',\n 'risks_table': '',\n 'spl_medguide': '',\n 'spl_patient_package_insert': '',\n 'statement_of_identity': '',\n 'stop_use_table': '',\n 'storage_and_handling': '',\n 'storage_and_handling_table': '',\n 'substance_name': 'NATEGLINIDE',\n 'summary_of_safety_and_effectiveness': '',\n 'teratogenic_effects': '',\n 'use_in_specific_populations': \"8 USE IN SPECIFIC POPULATIONS Lactation: Nateglinide is not recommended when breastfeeding ( 8.2 ) 8.1 Pregnancy Risk Summary The available data from published literature and the applicant's pharmacovigilance with use of nateglinide in pregnant women are insufficient to identify a drug-associated risk of major birth defects, miscarriage or other adverse maternal or fetal outcomes. There are risks to the mother and fetus associated with poorly controlled diabetes in pregnancy (see Clinical Considerations) . Nateglinide should be used during pregnancy only if the potential benefit justifies the potential risk to the fetus. In animal reproduction studies, there was no teratogenicity in rats and rabbits administered oral nateglinide during organogenesis at approximately 27 and 8 times the maximum recommended human dose (MRHD), respectively, based on body surface area (BSA). The estimated background risk of major birth defects is 6% to 10% in women with pre-gestational diabetes with a HbA1c > 7 and has been reported to be as high as 20% to 25% in women with a HbA1c > 10. The estimated background risk of miscarriage for the indicated population is unknown. In the U.S. general population, the estimated background risk of major birth defects and miscarriage in clinically recognized pregnancies is 2% to 4% and 15% to 20%, respectively. Clinical Considerations Disease-Associated Maternal and/or Embryo/Fetal Risk Poorly controlled diabetes in pregnancy increases the maternal risk for diabetic ketoacidosis, pre-eclampsia, spontaneous abortions, preterm delivery, and delivery complications. Poorly controlled diabetes increases the fetal risk for major birth defects, stillbirth, and macrosomia related morbidity. Data Animal data In embryofetal development studies, nateglinide administered orally during the period of organogenesis was not teratogenic in rats at doses up to 1,000 mg/kg (corresponding to 27 times the MRHD of 120 mg three times per day, based on BSA). In rabbits, embryonic development was adversely affected at 500 mg/kg/day and the incidence of gallbladder agenesis or small gallbladder was increased at a dose of 300 and 500 mg/kg (corresponding to 16 and 27 times the MRHD). No such effects were observed at 150 mg/kg/day (corresponding to 8 times the MRHD). In a pre-and postnatal development study in rats, nateglinide administered by oral gavage at doses of 100, 300, and 1000 mg/kg/day from gestation day 17 to lactation day 21 resulted in lower body weight in offspring of rats administered nateglinide at 1,000 mg/kg/day (corresponding to 27 times the MHRD). 8.2 Lactation Risk summary There are no data on the presence of nateglinide in human milk, the effects on the breastfeeding infant, or the effects on milk production. The drug is present in animal milk. When a drug is present in animal milk, it is likely that the drug will be present in human milk (see Data). Because the potential for hypoglycemia in breast-fed infants, advise women that use of nateglinide is not recommended while breastfeeding. Data In rat reproduction studies, nateglinide and its metabolite are excreted in the milk following oral dose (300 mg/kg). The overall milk: plasma (M/P) concentration ratio of the total radioactivity was approximately 1.4 based on AUC0-48 values. The M/P ratio of unchanged nateglinide was approximately 2.2. 8.4 Pediatric Use The safety and effectiveness of nateglinide have not been established in pediatric patients. 8.5 Geriatric Use 436 patients 65 years and older, and 80 patients 75 years and older were exposed to nateglinide in clinical studies. No differences were observed in safety or efficacy of nateglinide between patients age 65 and over, and those under age 65. However, greater sensitivity of some older individuals to nateglinide therapy cannot be ruled out. 8.6 Renal Impairment No dosage adjustment is recommended in patients with mild to severe renal impairment [see Clinical Pharmacology ( 12.3 )] . 8.7 Hepatic Impairment No dose adjustment is recommended for patients with mild hepatic impairment. Use of nateglinide in patients with moderate-to-severe hepatic impairment has not been studied and therefore, should be used with caution in these patients [see Clinical Pharmacology ( 12.3 )] .\",\n 'use_in_specific_populations_table': '',\n 'user_safety_warnings': '',\n 'warnings': '',\n 'warnings_and_cautions_table': '',\n 'when_using': '',\n 'when_using_table': ''}"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Imprime el consumo de GPU antes de cargar el modelo pre-entrenado","metadata":{}},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Prueba el modelo con inferencia Zero Shot","metadata":{}},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 120\nset_seed(seed)\nmax_tokens = 100\n\ntry:\n    prompt = dataset['train'][index]\n\n    # Instrucción: Resume la siguiente conversación\n    formatted_prompt = f'Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\\n Provide a detailed description of the medication {prompt[\"generic_name\"]} using the available data.\\n Output:\\n'\n    res = llm.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n    output = res[0].split('Output:\\n')[1]\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Model Generation - Zero Shot:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7. Pre-procesando el dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntry:\n    \n    path_tokenizer = os.path.join(BASE_FOLDER, \"dataset\")\n    path_save_tokenizer = os.path.join(path_tokenizer, name_zip_tokenizer)\n    #path_load_tokenizer = os.path.join(DATASET_FOLDER, name_zip_tokenizer)\n    \n    train_dataset_path = os.path.join(path_tokenizer, \"train_dataset\")\n    eval_dataset_path = os.path.join(path_tokenizer, \"eval_dataset\")\n    \n    if not(os.path.exists(path_tokenizer)):\n        #!mkdir -p {tokenizer_path_folder}\n        os.makedirs(path_tokenizer)\n        print('Directorio para almacenar dataset creado exitosamente!')\n    \n     \n    if (LOAD_TOKENIZER):\n        if (os.path.exists(TOKENIZER_PATH)):\n            with zipfile.ZipFile(TOKENIZER_PATH, 'r') as zip_ref:\n                zip_ref.extractall(path_tokenizer)\n            print(f\"Tokenizador cargado desde {TOKENIZER_PATH}\")\n        else:\n            raise FileNotFoundError(f\"El tokenizador no existe en la ruta {TOKENIZER_PATH}\")\n        \n        train_dataset = load_dataset(train_dataset_path)\n        eval_dataset = load_dataset(eval_dataset_path)\n        \n    else:\n        if not(PROCESS_SAMPLE):\n            max_length = llm.get_max_length()\n            train_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                               max_length=max_length,\n                                               seed=seed,\n                                               dataset=dataset['train']\n                                              )\n\n            eval_dataset = preprocess_dataset(tokenizer=llm.tokenizer, \n                                              max_length=max_length,\n                                              seed=seed,\n                                              dataset=dataset['validation']\n                                             )\n        else:\n            print('Process dataset sample..')\n            train_dataset = preprocess_dataset_sample(seed=seed,\n                                                      dataset=dataset['train']\n                                                     )\n\n            eval_dataset = preprocess_dataset_sample(seed=seed,\n                                                     dataset=dataset['validation']\n                                                     )\n            \n        \n        \n        if(SAVE_TOKENIZER):\n            # save in disk\n            train_dataset.save_to_disk(train_dataset_path)\n            eval_dataset.save_to_disk(eval_dataset_path)\n            \n            file_path = None\n            \n            if (os.path.exists(path_save_tokenizer)):\n                os.remove(path_save_tokenizer)  # Eliminar el archivo existente\n                print(f'Archivo zip existente, eliminado desde la ruta : {path_tokenizer}')\n                \n            with zipfile.ZipFile(path_save_tokenizer, 'w') as zipf:\n                for folder, subfolders, files in os.walk(path_tokenizer):\n                    for file in files:\n                        file_path = os.path.join(folder, file)\n                        zipf.write(file_path, os.path.relpath(file_path, path_tokenizer))\n            \n            if(os.path.exists(path_save_tokenizer)):\n                print('Process file zip tokenizer...')\nexcept Exception as ex:\n    print(f\"Error [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n\nFileLink(f'./{PROJECT_NAME}/dataset/{name_zip_tokenizer}')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:30:22.019723Z","iopub.execute_input":"2024-09-28T17:30:22.020443Z","iopub.status.idle":"2024-09-28T17:30:25.700454Z","shell.execute_reply.started":"2024-09-28T17:30:22.020403Z","shell.execute_reply":"2024-09-28T17:30:25.699196Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Directorio para almacenar dataset creado exitosamente!\nProcess dataset sample..\nPreprocessing dataset...\nCreate prompt llama2...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf50d50238f47a9ac10d541803ebcdc"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\nCreate prompt llama2...\nNúmero de núcleos de la CPU disponibles: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb28c04587bd45018a9ca48fdc904792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99c56e866e6c43fc8cddfb6dd53e07cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a911771dea754e13aeabbf1c37b71d77"}},"metadata":{}},{"name":"stdout","text":"Process file zip tokenizer...\nCPU times: user 3.07 s, sys: 547 ms, total: 3.62 s\nWall time: 3.66 s\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/drugs-generative/dataset/tokenizer.zip","text/html":"<a href='./drugs-generative/dataset/tokenizer.zip' target='_blank'>./drugs-generative/dataset/tokenizer.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8. Configura el modelo PEFT/LoRA para el Fine-Tuning\nAhora, vamos a realizar un ajuste fino eficiente en parámetros (PEFT). PEFT es una forma de ajuste fino por instrucciones que es mucho más eficiente que el ajuste fino completo. PEFT es un término genérico que incluye Adaptación de Bajo Rango (LoRA) y ajuste por indicaciones (¡que NO ES LO MISMO que la ingeniería de prompts!). En la mayoría de los casos, cuando alguien menciona PEFT, generalmente se refieren a LoRA. LoRA, en esencia, permite un ajuste fino eficiente del modelo utilizando menos recursos computacionales, a menudo realizable con solo una GPU. Después del ajuste fino con LoRA para una tarea o caso de uso específico, el resultado es un LLM original sin cambios y la aparición de un \"adaptador LoRA\" considerablemente más pequeño, que a menudo representa un porcentaje de un solo dígito del tamaño del LLM original (en MBs en lugar de GBs).\n\nDurante la inferencia, el adaptador LoRA debe combinarse con su LLM original. La ventaja radica en la capacidad de muchos adaptadores LoRA para reutilizar el LLM original, reduciendo así los requisitos generales de memoria cuando se manejan múltiples tareas y casos de uso.\n\nNota el hiperparámetro de rango (r), que define el rango/dimensión del adaptador a ser entrenado. r es el rango de la matriz de bajo rango utilizada en los adaptadores, lo que controla el número de parámetros entrenados. Un rango mayor permitirá mayor expresividad, pero hay una compensación en términos de cómputo.\n\nalpha es el factor de escalado para los pesos aprendidos. La matriz de pesos se escala por alpha/r, y por lo tanto, un valor más alto de alpha asigna más peso a las activaciones de LoRA.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(llm.model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(llm.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(r=64, #32\n                         lora_alpha=16, #32,\n                         target_modules=['q_proj','k_proj','v_proj','o_proj'], #dense\n                         bias=\"none\",\n                         lora_dropout=0.1, #0.05,  # Conventional\n                         task_type=\"CAUSAL_LM\",\n                        )\n\n\n# 2 - Utilizando el método prepare_model_for_kbit_training de PEFT.\nllm.model = prepare_model_for_kbit_training(llm.model)\n\npeft_model = get_peft_model(llm.model, peft_config)\npeft_model.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez que todo esté configurado y el modelo base esté preparado, podemos utilizar la función auxiliar print_trainable_parameters() para ver cuántos parámetros entrenables hay en el modelo.","metadata":{}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observa cómo se ve diferente el modelo ahora, con los adaptadores LoRA añadidos:\nprint(peft_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 9. Entrenando el Adaptador PEFT\n\nDefine los argumentos de entrenamiento y crea una instancia de Trainer.","metadata":{}},{"cell_type":"code","source":"tensorboard_callback = TensorBoardCallback()\ntb_process, ngrok_process = launch_tensorboard()","metadata":{"execution":{"iopub.status.busy":"2024-09-28T17:36:47.288381Z","iopub.execute_input":"2024-09-28T17:36:47.288932Z","iopub.status.idle":"2024-09-28T17:36:47.302475Z","shell.execute_reply.started":"2024-09-28T17:36:47.288895Z","shell.execute_reply":"2024-09-28T17:36:47.300656Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"TensorBoard URL: https://df4a-34-105-35-24.ngrok-free.app\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import amp\n\noutput_dir = './drugs-final-checkpoint'\n\n'''\npeft_training_args = TrainingArguments(\n    output_dir = output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    max_steps=1000,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    logging_dir=\"./logs\",\n    save_strategy=\"steps\",\n    save_steps=25,\n    evaluation_strategy=\"steps\",\n    eval_steps=25,\n    do_eval=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    overwrite_output_dir = 'True',\n    group_by_length=True,\n)\n'''\n'''\npeft_training_args = TrainingArguments(\n    output_dir = LOG_TRAIN_PATH,\n    do_eval=True,\n    eval_strategy=\"steps\",\n    fp16=False,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    learning_rate=2.0e-04,\n    logging_steps=25,\n    log_level=\"info\",\n    logging_strategy=\"steps\",\n    lr_scheduler_type=\"cosine\",\n    max_steps=1000,\n    #num_train_epochs=1,\n    overwrite_output_dir = True,\n    per_device_eval_batch_size=1,\n    per_device_train_batch_size=1,\n    save_strategy=\"steps\",\n    eval_steps=25,\n    group_by_length=True,\n    logging_dir=LOG_PATH,\n    optim=\"paged_adamw_8bit\",\n    save_steps=25,\n    warmup_steps=50,\n    save_total_limit=None,\n    seed=42,\n    report_to=\"tensorboard\",\n)\n\npeft_trainer = transformers.Trainer(model=peft_model,\n                                    train_dataset=train_dataset,\n                                    eval_dataset=eval_dataset,\n                                    args=peft_training_args,\n                                    data_collator=transformers.DataCollatorForLanguageModeling(llm.tokenizer, mlm=False),\n                                    )\n'''\n\nself.model.train()\n# Set training parameters\ntraining_arguments = TrainingArguments(\n            output_dir=path_folder,\n            num_train_epochs=num_train_epochs,\n            per_device_train_batch_size=per_device_train_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            optim=optim,\n            save_steps=save_steps,\n            logging_steps=logging_steps,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            fp16=fp16,\n            bf16=bf16,\n            max_grad_norm=max_grad_norm,\n            max_steps=max_steps,\n            warmup_ratio=warmup_ratio,\n            #group_by_length=group_by_length,\n            lr_scheduler_type=lr_scheduler_type,\n            seed=42,\n            report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n            model=self.model,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            peft_config=peft_config,\n            dataset_text_field=\"text\",\n            max_seq_length=max_seq_length,\n            tokenizer=self.tokenizer,\n            args=training_arguments,\n            packing=packing,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_training_args.device\n#print(f\"GPUs disponibles: {torch.cuda.device_count()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Liberar memoria para la fusión de pesos\ndel llm.model\ndel peft_trainer\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_gpu_utilization()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cualitativamente (Evaluación Humana)","metadata":{}},{"cell_type":"code","source":"try:\n    llm = ModelAnalizer(model_name)\n    llm._load_model()\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nft_model = PeftModel.from_pretrained(llm.model, \n                                     \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-1000\",\n                                     torch_dtype=torch.float16,\n                                     is_trainable=False\n                                    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nseed = 42\nindex = 120\nset_seed(seed)\nmax_tokens = 512\n\ntry:\n    prompt = dataset['train'][index]\n\n    # Instrucción: Resume la siguiente conversación\n    formatted_prompt = f'Instruct: Generate a detailed description of the medication for healthcare professionals and patients. Maintain a professional and concise tone throughout all responses. Do not fabricate information, and if a specific field regarding the safety in sensitive groups (pregnant women, children, elderly) is not present, simply state \"No specific information available\".\\n Provide a detailed description of the medication {prompt[\"generic_name\"]} using the available data.\\n Output:\\n'\n    res = ft_model.gen(formatted_prompt, max_tokens)\n    #print(res[0])\n    output = res[0].split('Output:\\n')[1]\n\n    dash_line = '-'.join('' for x in range(100))\n    print(dash_line)\n    print(f'Input Prompt:\\n{formatted_prompt}')\n    print(dash_line)\n    print(f'Peft Model Generation:\\n{output}')\n\nexcept Exception as ex:\n    print(f\"Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Evaluar el modelo cuantitativamente (con la Metrica ROUGE)","metadata":{}},{"cell_type":"code","source":"def data_process(dataset):\n    try:\n        # Añadir un prompt a cada muestra\n        print(\"Preprocessing dataset...\")\n\n        num_cores = multiprocessing.cpu_count()\n        print(f\"Número de núcleos de la CPU disponibles: {num_cores}\")\n\n        # Usar todos menos uno o dos núcleos para no sobrecargar el sistema\n        num_proc = max(1, num_cores - 1)\n\n        dataset = dataset.map(create_prompt_formats_v1,\n                              num_proc=num_proc\n                             )#, batched=True)\n    except Exception as ex:\n        raise Exception(f'Ocurrió un error inesperado al pre-procesar el dataset [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntry:\n    train_dataset=data_process(dataset['train'])\n    eval_dataset=data_process(dataset['validation'])\n\n    print(f\"Shapes of the datasets:\")\n    print(f\"Training: {train_dataset.shape}\")\n    print(f\"Validation: {eval_dataset.shape}\")\n    \nexcept Exception as ex:\n    print(f'Ocurrió un error inesperado [line: {ex.__traceback__.tb_lineno}] - {ex}')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train_dataset[120])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset[120]","metadata":{},"execution_count":null,"outputs":[]}]}